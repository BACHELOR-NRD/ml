{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9fc93bc",
   "metadata": {},
   "source": [
    "# Analyis Rerun on K-fold Cross Validation\n",
    "\n",
    "In this notebook, we rerun the analysis that are to be included in the final report, but this time using K-fold cross validation instead of a simple train-test split. This allows us to better assess the performance and robustness of our models across different subsets of the data.\n",
    "\n",
    "The following will be covered:\n",
    "1. dataset manipulations\n",
    "2. augumentations comparisons\n",
    "3. model score distributions analysis\n",
    "    - score distributions (during ensemble opt)\n",
    "4. ensemble strategies analysis\n",
    "    - wbf vs nms (during ensemble opt)\n",
    "5. one-class-only datasets analysis\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd55746",
   "metadata": {},
   "source": [
    "## Helper functions\n",
    "\n",
    "This is just a setup for all experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4736b7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ml_carbucks.utils.conversions import convert_yolo_to_coco\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "def create_counter(images_dir_root: str | Path, splits: list = [\"train\", \"val\"], normalize: bool = False):\n",
    "    counter = {\"all\": {}, \"img_counts\": {}}\n",
    "    for split in splits:\n",
    "        # load each file and count how many classes there are in each split and what is their distribution\n",
    "        split_dir = os.path.join(images_dir_root, split)\n",
    "        counter[split] = {}\n",
    "        for root, dirs, files in os.walk(split_dir):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".jpg\"):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(root, file)\n",
    "                label_path = file_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "\n",
    "                counter[\"img_counts\"][split] = counter[\"img_counts\"].get(split, 0) + 1\n",
    "                counter[\"img_counts\"]['all'] = counter[\"img_counts\"].get('all', 0) + 1\n",
    "                if not os.path.exists(label_path):\n",
    "                    counter['all']['no_label'] = counter['all'].get(\"no_label\", 0) + 1\n",
    "                    counter[split][\"no_label\"] = counter[split].get(\"no_label\", 0) + 1\n",
    "                    continue\n",
    "\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    if len(f.read().strip()) == 0:\n",
    "                        counter['all']['no_label'] = counter['all'].get(\"no_label\", 0) + 1\n",
    "                        counter[split][\"no_label\"] = counter[split].get(\"no_label\", 0) + 1\n",
    "                        continue\n",
    "                    f.seek(0)\n",
    "                    for line in f:\n",
    "                        class_id = line.strip().split()[0]\n",
    "                        counter[split][class_id] = counter[split].get(class_id, 0) + 1\n",
    "                        counter['all'][class_id] = counter['all'].get(class_id, 0) + 1\n",
    "\n",
    "    if normalize:\n",
    "        counter_normalized = deepcopy(counter)\n",
    "        for split in splits + ['all']:\n",
    "            total = sum(counter_normalized[split].values())\n",
    "            for class_id in counter_normalized[split]:\n",
    "                counter_normalized[split][class_id] = round(counter_normalized[split][class_id] / total * 100, 4) \n",
    "\n",
    "            counter_normalized[split] = dict(sorted(counter_normalized[split].items(), key=lambda item: (item[0] != \"no_label\", int(item[0]) if item[0] != \"no_label\" else -1)))\n",
    "        return counter_normalized\n",
    "    else:\n",
    "        for split in splits + ['all']:\n",
    "            counter[split] = dict(sorted(counter[split].items(), key=lambda item: (item[0] != \"no_label\", int(item[0]) if item[0] != \"no_label\" else -1)))\n",
    "    return counter\n",
    "\n",
    "def visualize_counter(counter:dict, split: str = 'all', counter_name: str = \"\"):\n",
    "    plt.bar(\n",
    "        x=list(counter[split].keys()),\n",
    "        height=[v for v in counter[split].values()]\n",
    "    )\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    # write actual numbers on top of bars\n",
    "    for i, v in enumerate(counter[split].values()):\n",
    "        plt.text(i, v + 0.5, str(v), ha='center')\n",
    "    plt.title(f\"Class Distribution in '{split}' Split for {counter_name} Dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_analysis(images_dir_root: str | Path, splits: list = [\"train\", \"val\"], counter_name: str = \"\", normalize: bool = False, visualize_splits: list = ['all']):\n",
    "    counter = create_counter(images_dir_root, splits, normalize)\n",
    "    print(f\"Dataset Analysis for {counter_name} Dataset:\")\n",
    "    print(counter)\n",
    "    for split in visualize_splits:\n",
    "        visualize_counter(counter, split, counter_name)\n",
    "\n",
    "def clean_up_empty_labels(dataset_dir: str | Path, splits: list):\n",
    "    print(f\"Cleaning up empty labels in dataset at: {dataset_dir}\")\n",
    "    for split in splits:\n",
    "        for root, dirs, files in os.walk(Path(dataset_dir) / \"images\" / split):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".jpg\"):\n",
    "                    continue\n",
    "                \n",
    "                img_file_path = os.path.join(root, file)\n",
    "                label_file_path = img_file_path.replace(\".jpg\", \".txt\").replace(\"images\", \"labels\")\n",
    "                img_name = file\n",
    "                label_name = img_name.replace(\".jpg\", \".txt\")\n",
    "\n",
    "                if not os.path.exists(label_file_path):\n",
    "                    print(f\"Found image with no corresponding label file: {img_file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"empty\", split), exist_ok=True)\n",
    "                    # move image file\n",
    "                    new_img_path = os.path.join(dataset_dir, \"images\", \"empty\", split, file)\n",
    "                    os.rename(img_file_path, new_img_path)\n",
    "                    continue\n",
    "\n",
    "                with open(label_file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                if len(lines) == 0:\n",
    "                    print(f\"Found empty label file: {label_file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"empty\", split), exist_ok=True)\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"labels\", \"empty\", split), exist_ok=True)\n",
    "                    # move label file\n",
    "                    new_label_path = os.path.join(dataset_dir, \"labels\", \"empty\", split, label_name)\n",
    "                    os.rename(label_file_path, new_label_path)\n",
    "                    # move image file\n",
    "                    new_img_path = os.path.join(dataset_dir, \"images\", \"empty\", split, img_name)\n",
    "                    os.rename(img_file_path, new_img_path)\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=dataset_dir,\n",
    "        splits=splits,\n",
    "    )\n",
    "                    \n",
    "def balance_dataset(dataset_dir: str | Path, splits: list, remove_class_probabilities: dict[str, float] | None = None):\n",
    "    for split in splits:\n",
    "        files_moved_cnt = {class_id: 0 for class_id in remove_class_probabilities.keys()} if remove_class_probabilities else {}\n",
    "        for root, dirs, files in os.walk(Path(dataset_dir) / \"labels\" / split):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".txt\"):\n",
    "                    continue\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                class_labels = set()\n",
    "                for line in lines:\n",
    "                    class_id = line.strip().split()[0]\n",
    "                    class_labels.add(class_id)\n",
    "\n",
    "                # we want to move the files only that have pure one class labels, not mixed\n",
    "                if len(class_labels) != 1:\n",
    "                    continue\n",
    "\n",
    "                class_id = class_labels.pop()\n",
    "                move_file = False\n",
    "                if remove_class_probabilities and class_id in remove_class_probabilities:\n",
    "                    prob = remove_class_probabilities[class_id]\n",
    "                    if random.random() <= prob:\n",
    "                        move_file = True\n",
    "\n",
    "                if move_file:\n",
    "                    files_moved_cnt[class_id] += 1\n",
    "                    print(f\"Moving pure class {class_id} label file: {file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"balancing\", split), exist_ok=True)\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"labels\", \"balancing\", split), exist_ok=True)\n",
    "                    # move label file\n",
    "                    new_label_path = os.path.join(dataset_dir, \"labels\", \"balancing\", split, file)\n",
    "                    os.rename(file_path, new_label_path)\n",
    "                    # move image file\n",
    "                    img_file = file.replace(\".txt\", \".jpg\")\n",
    "                    img_path = os.path.join(dataset_dir, \"images\", split, img_file)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_path = os.path.join(dataset_dir, \"images\", \"balancing\", split, img_file)\n",
    "                        os.rename(img_path, new_img_path)\n",
    "                    else:\n",
    "                        print(f\"Corresponding image file not found for label: {file_path}\")\n",
    "        if files_moved_cnt:\n",
    "            print(f\"Moved files for {split}: {files_moved_cnt}\")\n",
    "\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=dataset_dir,\n",
    "        splits=splits,\n",
    "    )\n",
    "       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba17b632",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Please make sure that the functions above were executed as the analysis rely that the datasets were correclty prepared. You may comment this out once done.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50831a95",
   "metadata": {},
   "source": [
    "### 1. Dataset manipulations\n",
    "We will analyze how different dataset manipulations affect model performance across the K folds.\n",
    "- raw data\n",
    "- cleaned data (without empty annotation images)\n",
    "- balanced class\n",
    "- combined datasets\n",
    "\n",
    "Thos manipulations will be evaluated using cross-validation on the baseline datasets meaning the manipulations will only be applied to the training folds while the validation fold remains unchanged. This ensures that we can accurately assess the impact of each manipulation on model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d474dc07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.utils.conversions import convert_coco_to_yolo\n",
    "\n",
    "manipulated_datasets_dir = []\n",
    "for train_fold in DatasetsPathManager.CARBUCKS_TRAIN_CV:\n",
    "    original_train_img_dir = train_fold[0][0]\n",
    "    fold_dir = original_train_img_dir.parent.parent # type: ignore\n",
    "    print(f\"Processing fold directory: {fold_dir}\")\n",
    "\n",
    "\n",
    "    cleaned_dir = fold_dir.parent / f\"{fold_dir.name}_cleaned\"\n",
    "    balanced_dir = fold_dir.parent / f\"{fold_dir.name}_balanced\"\n",
    "\n",
    "    os.system(f\"rm -rf {cleaned_dir}\")\n",
    "    os.system(f\"rm -rf {balanced_dir}\")\n",
    " \n",
    "    os.makedirs(cleaned_dir / \"images\" / \"train\", exist_ok=True)\n",
    "    os.makedirs(balanced_dir / \"images\" / \"train\", exist_ok=True)\n",
    "    \n",
    "    # #copy original fold to new location\n",
    "    os.system(f\"cp -r {fold_dir}/images/train/ {cleaned_dir}/images/\")\n",
    "    os.system(f\"cp -r {fold_dir}/instances_train_curated.json {cleaned_dir}\")\n",
    "    os.system(f\"cp -r {fold_dir}/images/train/ {balanced_dir}/images/\")\n",
    "    os.system(f\"cp -r {fold_dir}/instances_train_curated.json {balanced_dir}\")\n",
    "    \n",
    "    convert_coco_to_yolo(\n",
    "        img_dir=cleaned_dir / \"images\" / \"train\",\n",
    "        ann_file=cleaned_dir / \"instances_train_curated.json\",\n",
    "    )\n",
    "    \n",
    "    convert_coco_to_yolo(\n",
    "        img_dir=balanced_dir / \"images\" / \"train\",\n",
    "        ann_file=balanced_dir / \"instances_train_curated.json\",\n",
    "    )\n",
    "\n",
    "    clean_up_empty_labels(dataset_dir=cleaned_dir, splits=[\"train\"])\n",
    "    clean_up_empty_labels(dataset_dir=balanced_dir, splits=[\"train\"])\n",
    "\n",
    "    balance_dataset(\n",
    "        dataset_dir=balanced_dir,\n",
    "        splits=[\"train\"],\n",
    "        remove_class_probabilities={\n",
    "            \"0\": 0.9, # <- this can be adjusted to remove some, it removes the % of pure class images\n",
    "            \"1\": 0.0, # <-\n",
    "            \"2\": 0.0, # <-\n",
    "        }\n",
    "    )\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=cleaned_dir,\n",
    "        splits=[\"train\"],\n",
    "    )\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=balanced_dir,\n",
    "        splits=[\"train\"],\n",
    "    )\n",
    "\n",
    "    display_dataset_analysis(\n",
    "        images_dir_root=cleaned_dir / \"images\",\n",
    "        splits=[\"train\"],\n",
    "        counter_name=f\"Cleaned Dataset Fold {fold_dir.name}\",\n",
    "        normalize=True,\n",
    "        visualize_splits=[\"train\"]\n",
    "    )\n",
    "    display_dataset_analysis(\n",
    "        images_dir_root=balanced_dir / \"images\",\n",
    "        splits=[\"train\"],\n",
    "        counter_name=f\"Balanced Dataset Fold {fold_dir.name}\",\n",
    "        normalize=True,\n",
    "        visualize_splits=[\"train\"]\n",
    "    )\n",
    "\n",
    "    \n",
    "    manipulated_datasets_dir.append(cleaned_dir)\n",
    "    manipulated_datasets_dir.append(balanced_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c5fc2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from ml_carbucks import RESULTS_DIR\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "from ml_carbucks.adapters import YoloUltralyticsAdapter, RtdetrUltralyticsAdapter, FasterRcnnAdapter, EfficientDetAdapter\n",
    "\n",
    "\n",
    "METRIC = \"map_50\"\n",
    "BASE_PARAMS :dict[str, Any]= {\n",
    "    \"epochs\": 20,\n",
    "    \"batch_size\": 16,\n",
    "    \"accumulation_steps\": 2,\n",
    "    \"img_size\": 320,\n",
    "    \"verbose\": True\n",
    "}\n",
    "\n",
    "saver1 = ResultSaver(\n",
    "    path=RESULTS_DIR,\n",
    "    name=\"saver1_crossval_data_manipulations\",\n",
    "    metadata=BASE_PARAMS,\n",
    "    append=True\n",
    ")\n",
    "\n",
    "logger.info(f\"Starting cross-validation for models on singular datasets manipulations\")\n",
    "singular_models: list[BaseDetectionAdapter] = [\n",
    "    YoloUltralyticsAdapter(**BASE_PARAMS),\n",
    "    RtdetrUltralyticsAdapter(**BASE_PARAMS),\n",
    "    FasterRcnnAdapter(**BASE_PARAMS),\n",
    "    EfficientDetAdapter(**BASE_PARAMS),\n",
    "]\n",
    "for train_idx, (train, val) in enumerate(zip(DatasetsPathManager.CARBUCKS_TRAIN_CV[:3], DatasetsPathManager.CARBUCKS_VAL_CV[:3], strict=True)):\n",
    "    for model in singular_models:\n",
    "        \n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"carbucks_standard\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit(train).evaluate(val)[METRIC]\n",
    "        )\n",
    "\n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"cardd\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit(DatasetsPathManager.CARDD_TRAIN).evaluate(val)[METRIC]\n",
    "        )\n",
    "\n",
    "        cleaned_train = [\n",
    "            (str(train[0][0]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_cleaned\"), \n",
    "             str(train[0][1]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_cleaned\"))]\n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"carbucks_cleaned\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit(cleaned_train).evaluate(val)[METRIC] # type: ignore\n",
    "        )\n",
    "        \n",
    "        balanced_train = [\n",
    "            (str(train[0][0]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_balanced\"), \n",
    "             str(train[0][1]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_balanced\"))]\n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"carbucks_balanced\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit(balanced_train).evaluate(val)[METRIC] # type: ignore\n",
    "        )\n",
    "\n",
    "logger.info(f\"Starting cross-validation for models on combined datasets manipulations\")\n",
    "combined_models: list[BaseDetectionAdapter] = [\n",
    "    FasterRcnnAdapter(**BASE_PARAMS),\n",
    "    EfficientDetAdapter(**BASE_PARAMS),\n",
    "]\n",
    "for train_idx, (train, val) in enumerate(zip(DatasetsPathManager.CARBUCKS_TRAIN_CV[:3], DatasetsPathManager.CARBUCKS_VAL_CV[:3], strict=True)):\n",
    "    for model in combined_models:\n",
    "        \n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"cardd+carbucks_standard\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit([DatasetsPathManager.CARDD_TRAIN[0], DatasetsPathManager.CARBUCKS_TRAIN_CV[train_idx][0]]).evaluate(val)[METRIC] # type: ignore\n",
    "        )\n",
    "\n",
    "        cleaned_train = [\n",
    "            (str(train[0][0]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_cleaned\"), \n",
    "             str(train[0][1]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_cleaned\"))]\n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"cardd+carbucks_cleaned\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit([DatasetsPathManager.CARDD_TRAIN[0], cleaned_train[0]]).evaluate(val)[METRIC] # type: ignore\n",
    "        )\n",
    "\n",
    "        balanced_train = [\n",
    "            (str(train[0][0]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_balanced\"), \n",
    "             str(train[0][1]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_balanced\"))]\n",
    "        saver1.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"cardd+carbucks_balanced\",\n",
    "            fold=train_idx,\n",
    "            metric_name=METRIC,\n",
    "            metric_value=model.clone().fit([DatasetsPathManager.CARDD_TRAIN[0], balanced_train[0]]).evaluate(val)[METRIC] # type: ignore\n",
    "        )\n",
    "    \n",
    "logger.info(\"Cross-validation for models on dataset manipulations completed.\")\n",
    "logger.info(saver1.data)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4bb1521",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d86d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for man_dir in manipulated_datasets_dir:\n",
    "    os.system(f\"rm -rf {man_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4615c8e",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf618cfa",
   "metadata": {},
   "source": [
    "## 2. Augmentations comparisons\n",
    "We will compare different augmentation strengths and techniques using cross-validation to determine their effect on model robustness and accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91f62a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any\n",
    "\n",
    "from ml_carbucks import RESULTS_DIR\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "from ml_carbucks.adapters import YoloUltralyticsAdapter, RtdetrUltralyticsAdapter, FasterRcnnAdapter, EfficientDetAdapter\n",
    "\n",
    "\n",
    "AUG_METRIC = \"map_50\"\n",
    "AUG_BASE_PARAMS :dict[str, Any]= {\n",
    "    \"batch_size\": 8,\n",
    "    \"accumulation_steps\": 4,\n",
    "    \"img_size\": 320,\n",
    "}\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Literal\n",
    "import torch\n",
    "import gc\n",
    "@dataclass\n",
    "class EfficientDetAdapterCustomLoader(EfficientDetAdapter):\n",
    "    loader: Literal[\"inbuilt\", \"custom\"] = \"custom\"\n",
    "\n",
    "model_clsasses: list[type[BaseDetectionAdapter]] = [\n",
    "    YoloUltralyticsAdapter,\n",
    "    RtdetrUltralyticsAdapter,\n",
    "    FasterRcnnAdapter,\n",
    "    EfficientDetAdapter,\n",
    "    EfficientDetAdapterCustomLoader,\n",
    "]\n",
    "\n",
    "saver2 = ResultSaver(\n",
    "    path=RESULTS_DIR,\n",
    "    name=\"saver2_crossval_augmentations_comparisons\",\n",
    "    metadata=AUG_BASE_PARAMS,\n",
    ")\n",
    "\n",
    "EPOCHS_TO_TEST = [30]\n",
    "\n",
    "logger.info(\"Starting cross-validation for models augmentations comparisons\")\n",
    "for epoch_count in EPOCHS_TO_TEST:\n",
    "    for train_idx, (train, val) in enumerate(\n",
    "        zip(\n",
    "            DatasetsPathManager.CARBUCKS_TRAIN_CV[:3],\n",
    "            DatasetsPathManager.CARBUCKS_VAL_CV[:3],\n",
    "            strict=True,\n",
    "        )\n",
    "    ):\n",
    "        for model_cls in model_clsasses:\n",
    "\n",
    "            model_aug = model_cls(**AUG_BASE_PARAMS, training_augmentations=True, epochs=epoch_count)  # type: ignore\n",
    "            \n",
    "            res_aug = model_aug.debug(train, val, results_path=RESULTS_DIR / \"augmentation_comparison\", results_name=f\"fold_{train_idx}_{model_cls.__name__}_aug_\")\n",
    "            saver2.save(\n",
    "                model_name=model_aug.__class__.__name__,\n",
    "                augmentation=True,\n",
    "                fold=train_idx,\n",
    "                metric_name=AUG_METRIC,\n",
    "                epochs=epoch_count,\n",
    "                metric_value=res_aug[AUG_METRIC],\n",
    "            )\n",
    "            del model_aug\n",
    "            del res_aug\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "            model_noaug = model_cls(**AUG_BASE_PARAMS, training_augmentations=False, epochs=epoch_count)  # type: ignore\n",
    "            res_noaug = model_noaug.debug(train, val, results_path=RESULTS_DIR / \"augmentation_comparison\", results_name=f\"fold_{train_idx}_{model_cls.__name__}_noaug_\")\n",
    "\n",
    "            saver2.save(\n",
    "                model_name=model_noaug.__class__.__name__,\n",
    "                augmentation=False,\n",
    "                fold=train_idx,\n",
    "                metric_name=AUG_METRIC,\n",
    "                epochs=epoch_count,\n",
    "                metric_value=res_noaug[AUG_METRIC],\n",
    "            )\n",
    "            del model_noaug\n",
    "            del res_noaug\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "            torch.cuda.ipc_collect()\n",
    "\n",
    "logger.info(\"Cross-validation for models augmentations comparisons completed.\")\n",
    "logger.info(saver2.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7055438e",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f02965",
   "metadata": {},
   "source": [
    "## 3. Model score distributions analysis\n",
    "We will analyze the score distributions of different models using cross-validation to understand how they affect ensemble performance\n",
    "\n",
    "For that we will analyse the distributions of the final ensemble that were already obtained during the `Ensemble Optimization` step using cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc1def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise ValueError(\"Please make sure that the prestep_hash variable is set to the correct hash of the ensemble optimization prestep that was run with cross-validation. You may comment this out once done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a3be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_histograms(scores_dict, xlim=None, info_title=''):\n",
    "    # make seeded color from title\n",
    "    np.random.seed(abs(hash(info_title)) % (2**32))\n",
    "    colors = np.random.rand(len(scores_dict), 3)\n",
    "\n",
    "\n",
    "    fig, axs = plt.subplots(2, 2, figsize=(10, 5))\n",
    "    axs = axs.flatten()\n",
    "    for i, scores in enumerate(scores_dict.values()):\n",
    "        axs[i].hist(scores, bins=50, color=colors[0])\n",
    "        if xlim is not None:\n",
    "            axs[i].set_xlim(xlim)\n",
    "        axs[i].set_title(f'Adapter {list(scores_dict.keys())[i]} prediction scores distribution {info_title}')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdf9e911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "from ml_carbucks import OPTUNA_DIR\n",
    "\n",
    "PRESTEP_HASH = \"your_prestep_hash_here\"  # the hash of the ensemble optimization prestep\n",
    "\n",
    "PRESTEP_PATH = OPTUNA_DIR / \"ensemble\" / f\"prestep_{PRESTEP_HASH}\"\n",
    "\n",
    "\n",
    "(\n",
    "    adapters_predictions,\n",
    "    ground_truths,\n",
    "    distributions,\n",
    "    adapters_crossval_metrics,\n",
    "    adapters_dataset_metrics,\n",
    "    adapter_hashes,\n",
    "    adapter_names,\n",
    ")  = pkl.load(open(PRESTEP_PATH, \"rb\"))\n",
    "\n",
    "\n",
    "all_scores = [\n",
    "    [] for _ in range(len(adapters_predictions))\n",
    "]\n",
    "\n",
    "for i, preds in enumerate(adapters_predictions):\n",
    "    all_scores[i] = []\n",
    "    for p in preds:\n",
    "        for scores in p['scores']:\n",
    "            all_scores[i].append(scores.item())\n",
    "\n",
    "from torch import erf\n",
    "import torch\n",
    "\n",
    "\n",
    "minmax_normalized_scores = []\n",
    "for scores in all_scores:\n",
    "    min_score = min(scores)\n",
    "    max_score = max(scores)\n",
    "    norm_scores = [(s - min_score) / (max_score - min_score) for s in scores]\n",
    "    minmax_normalized_scores.append(norm_scores)\n",
    "\n",
    "zscore_normalized_scores = []\n",
    "for scores in all_scores:\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    std_score = (sum((s - mean_score) ** 2 for s in scores) / len(scores)) ** 0.5\n",
    "    norm_scores = [(s - mean_score) / std_score for s in scores]\n",
    "    zscore_normalized_scores.append(norm_scores)\n",
    "\n",
    "zscore_probability_normalized_scores = []\n",
    "for scores in all_scores:\n",
    "    mean_score = sum(scores) / len(scores)\n",
    "    std_score = (sum((s - mean_score) ** 2 for s in scores) / len(scores)) ** 0.5\n",
    "    zscore_scores = [(s - mean_score) / std_score for s in scores]\n",
    "    prob_scores = [0.5 * (1 + erf(torch.tensor(s) / (2 ** 0.5))) for s in zscore_scores]\n",
    "    zscore_probability_normalized_scores.append(prob_scores)\n",
    "\n",
    "quantile_normalized_scores = []\n",
    "for scores in all_scores:\n",
    "    sorted_scores = sorted(scores)\n",
    "    norm_scores = [sorted_scores.index(s) / len(scores) for s in scores]\n",
    "    quantile_normalized_scores.append(norm_scores)\n",
    "\n",
    "plot_histograms(all_scores, info_title='raw')\n",
    "plot_histograms(minmax_normalized_scores, xlim=(0, 1), info_title='min-max normalized')\n",
    "plot_histograms(zscore_normalized_scores, xlim=(-6, 6), info_title='z-score normalized')\n",
    "plot_histograms(zscore_probability_normalized_scores, xlim=(0, 1), info_title='z-score probability normalized')\n",
    "plot_histograms(quantile_normalized_scores, xlim=(0, 1), info_title='quantile normalized')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7338303",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bb96d",
   "metadata": {},
   "source": [
    "## 4. Ensemble strategies analysis\n",
    "We will evaluate different ensemble strategies using cross-validation to understand their impact on detection performance.\n",
    "    - wbf vs nms (during ensemble opt)\n",
    "\n",
    "For that we will analyse the results of the `Ensemble Optimization` step that was already performed using cross-validation.\n",
    "\n",
    "Topics:\n",
    "- strategies (nms, wbf)\n",
    "- score normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ceca884",
   "metadata": {},
   "source": [
    "### Optuna pictures here and results I guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3627d2",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3869701e",
   "metadata": {},
   "source": [
    "## 5. One-class-only datasets analysis\n",
    "We will investigate the performance of models trained on datasets containing only one class using cross-validation. \n",
    "\n",
    "We will focus only on `Crack` class as it is the most `under-represented` class in the dataset and thus the most challenging one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d17894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from ml_carbucks.utils.conversions import convert_yolo_to_coco\n",
    "\n",
    "\n",
    "def filter_dataset(data_dir: str | Path, splits:list[str], class_to_keep:str=\"2\", negative_sample_ratio:float=-1):\n",
    "\n",
    "    dataset_yaml_path = os.path.join(data_dir, \"dataset_single.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml_path):\n",
    "        logger.info(f\"Dataset yaml file does not exist. Proceeding...\")\n",
    "    else:\n",
    "        loaded_yaml = None\n",
    "        with open(dataset_yaml_path, \"r\") as f:\n",
    "            loaded_yaml = yaml.safe_load(f)\n",
    "        if loaded_yaml['nc'] == 1 and loaded_yaml['names'] == [f'class_to_keep_{class_to_keep}']:\n",
    "            logger.info(f\"Dataset already filtered to class {class_to_keep}. Exiting...\")\n",
    "            return\n",
    "\n",
    "\n",
    "    for split in splits:\n",
    "        obj = {}\n",
    "        for file in os.listdir(os.path.join(data_dir, \"labels\", split)):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(data_dir, \"labels\", split, file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    file_classes = set()\n",
    "                    for line in lines:\n",
    "                        class_id = line.split()[0]\n",
    "                        file_classes.add(class_id)\n",
    "                    obj[file.split(\".\")[0]] = list(file_classes)\n",
    "\n",
    "        files_to_keep = []\n",
    "        for key, value in obj.items():\n",
    "            if class_to_keep in value:\n",
    "                files_to_keep.append(key)\n",
    "\n",
    "        files_neg_to_keep = []\n",
    "        for key, value in obj.items():\n",
    "            if class_to_keep not in value:\n",
    "                if negative_sample_ratio < 0:\n",
    "                    files_neg_to_keep.append(key)\n",
    "                elif len(files_neg_to_keep) < len(files_to_keep) and random.random() < negative_sample_ratio:\n",
    "                    files_neg_to_keep.append(key)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        \n",
    "        print(f\"{len(files_to_keep)}, {len(files_neg_to_keep)}\")\n",
    "\n",
    "        for key, value in obj.items():\n",
    "            if key in files_to_keep or key in files_neg_to_keep:\n",
    "                # clean up other classes from the labels\n",
    "                with open(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                with open(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"), \"w\") as f:\n",
    "                    for line in lines:\n",
    "                        class_id = line.split()[0]\n",
    "                        if class_id == class_to_keep:\n",
    "                            f.write(line.replace(f\"{class_to_keep} \", \"0 \"))\n",
    "            else:\n",
    "                os.remove(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"))\n",
    "                os.remove(os.path.join(data_dir, \"images\", split, f\"{key}.jpg\"))   \n",
    "    \n",
    "    # write dataset.yaml file\n",
    "    dataset_yaml = {\n",
    "        'nc': 1,\n",
    "        'names': [f'class_to_keep_{class_to_keep}']\n",
    "    }\n",
    "\n",
    "    dataset_yaml[\"train\"] = f\"images/train\"\n",
    "    dataset_yaml[\"val\"] = f\"images/val\"\n",
    "    for split in splits:\n",
    "        dataset_yaml[split] = f\"images/{split}\"\n",
    "\n",
    "    with open(dataset_yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_yaml, f)\n",
    "\n",
    "    convert_yolo_to_coco(data_dir, splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4fadc97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing fold directory: /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_1\n",
      "INFO ml_carbucks.utils.conversions 14:58:21 | COCO to YOLO conversion completed in 0.08 seconds\n",
      "INFO ml_carbucks.utils.conversions 14:58:21 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO __main__ 14:58:21 | Dataset yaml file does not exist. Proceeding...\n",
      "424, 2870\n",
      "INFO ml_carbucks.utils.conversions 14:58:21 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_1_crack/instances_train_curated.json with 3258 images and 424 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:21 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_1_crack/instances_train_curated.json with 3258 images and 424 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:21 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_1_crack/instances_val_curated.json with 814 images and 0 annotations.\n",
      "Processing fold directory: /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_2\n",
      "INFO ml_carbucks.utils.conversions 14:58:32 | COCO to YOLO conversion completed in 0.08 seconds\n",
      "INFO ml_carbucks.utils.conversions 14:58:32 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO __main__ 14:58:32 | Dataset yaml file does not exist. Proceeding...\n",
      "424, 2870\n",
      "INFO ml_carbucks.utils.conversions 14:58:33 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_2_crack/instances_train_curated.json with 3258 images and 436 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:33 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_2_crack/instances_train_curated.json with 3258 images and 436 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:33 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_2_crack/instances_val_curated.json with 814 images and 0 annotations.\n",
      "Processing fold directory: /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_3\n",
      "INFO ml_carbucks.utils.conversions 14:58:48 | COCO to YOLO conversion completed in 0.07 seconds\n",
      "INFO ml_carbucks.utils.conversions 14:58:49 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO __main__ 14:58:49 | Dataset yaml file does not exist. Proceeding...\n",
      "424, 2870\n",
      "INFO ml_carbucks.utils.conversions 14:58:49 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_3_crack/instances_train_curated.json with 3257 images and 426 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:49 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_3_crack/instances_train_curated.json with 3257 images and 426 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:58:49 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_3_crack/instances_val_curated.json with 815 images and 0 annotations.\n",
      "Processing fold directory: /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_4\n",
      "INFO ml_carbucks.utils.conversions 14:59:03 | COCO to YOLO conversion completed in 0.07 seconds\n",
      "INFO ml_carbucks.utils.conversions 14:59:03 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO __main__ 14:59:03 | Dataset yaml file does not exist. Proceeding...\n",
      "424, 2870\n",
      "INFO ml_carbucks.utils.conversions 14:59:04 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_4_crack/instances_train_curated.json with 3258 images and 432 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:59:04 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_4_crack/instances_train_curated.json with 3258 images and 432 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:59:04 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_4_crack/instances_val_curated.json with 814 images and 0 annotations.\n",
      "Processing fold directory: /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_5\n",
      "INFO ml_carbucks.utils.conversions 14:59:13 | COCO to YOLO conversion completed in 0.07 seconds\n",
      "INFO ml_carbucks.utils.conversions 14:59:13 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO __main__ 14:59:13 | Dataset yaml file does not exist. Proceeding...\n",
      "424, 2870\n",
      "INFO ml_carbucks.utils.conversions 14:59:13 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_5_crack/instances_train_curated.json with 3257 images and 430 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:59:14 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_5_crack/instances_train_curated.json with 3257 images and 430 annotations.\n",
      "INFO ml_carbucks.utils.conversions 14:59:14 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crossval/fold_5_crack/instances_val_curated.json with 815 images and 0 annotations.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.utils.conversions import convert_coco_to_yolo, convert_yolo_to_coco\n",
    "\n",
    "single_class_created_dirs = []\n",
    "for train_fold in DatasetsPathManager.CARBUCKS_TRAIN_CV:\n",
    "    original_train_img_dir = train_fold[0][0]\n",
    "    fold_dir = original_train_img_dir.parent.parent # type: ignore\n",
    "    print(f\"Processing fold directory: {fold_dir}\")\n",
    "    filtered_dir = fold_dir.parent / f\"{fold_dir.name}_crack\"\n",
    "    \n",
    "    os.system(f\"rm -rf {filtered_dir}\")\n",
    "    #copy original fold to new location\n",
    "    os.makedirs(filtered_dir / \"images\" / \"train\", exist_ok=True)\n",
    "    os.system(f\"cp -r {fold_dir}/images/train/ {filtered_dir}/images/\")\n",
    "    os.system(f\"cp -r {fold_dir}/images/val/ {filtered_dir}/images/\")\n",
    "    os.system(f\"cp -r {fold_dir}/instances_train_curated.json {filtered_dir}\")\n",
    "    os.system(f\"cp -r {fold_dir}/instances_val_curated.json {filtered_dir}\")\n",
    "    \n",
    "    convert_coco_to_yolo(\n",
    "        img_dir=filtered_dir / \"images\" / \"train\",\n",
    "        ann_file=filtered_dir / \"instances_train_curated.json\",\n",
    "    )\n",
    "    convert_coco_to_yolo(\n",
    "        img_dir=filtered_dir / \"images\" / \"train\",\n",
    "        ann_file=filtered_dir / \"instances_val_curated.json\",\n",
    "    )\n",
    "    filter_dataset(\n",
    "        data_dir=filtered_dir,\n",
    "        splits=[\"train\"],\n",
    "        class_to_keep=\"2\",  # assuming '2' is the class ID for 'Crack'\n",
    "        negative_sample_ratio=-1\n",
    "    )\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=filtered_dir,\n",
    "        splits=[\"train\", \"val\"],\n",
    "    )\n",
    "\n",
    "    single_class_created_dirs.append(filtered_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b3af4fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks import RESULTS_DIR\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "from ml_carbucks.adapters import YoloUltralyticsAdapter, RtdetrUltralyticsAdapter, FasterRcnnAdapter, EfficientDetAdapter\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "\n",
    "ONE_METRIC = \"map_50\"\n",
    "ONE_BASE_PARAMS :dict[str, Any]= {\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 8,\n",
    "    \"accumulation_steps\": 4,\n",
    "    \"img_size\": 320,\n",
    "}\n",
    "\n",
    "saver5 = ResultSaver(\n",
    "    path=RESULTS_DIR,\n",
    "    name=\"saver5_crossval_one_class_datasets\",\n",
    "    metadata=ONE_BASE_PARAMS\n",
    ")\n",
    "\n",
    "one_models: list[BaseDetectionAdapter] = [\n",
    "    YoloUltralyticsAdapter(**ONE_BASE_PARAMS),\n",
    "    RtdetrUltralyticsAdapter(**ONE_BASE_PARAMS),\n",
    "    FasterRcnnAdapter(**ONE_BASE_PARAMS),\n",
    "    EfficientDetAdapter(**ONE_BASE_PARAMS),\n",
    "]\n",
    "\n",
    "for train_idx, (train, val) in enumerate(zip(DatasetsPathManager.CARBUCKS_TRAIN_CV, DatasetsPathManager.CARBUCKS_VAL_CV, strict=True)):\n",
    "    for model in one_models:\n",
    "        \n",
    "        # NOTE: this will produce BOX PR with each class MAP@50, and we will need to manually look it up later\n",
    "        model.debug(\n",
    "            train, val, \n",
    "            results_path=RESULTS_DIR / \"debug_one_class_datasets\" ,\n",
    "            results_name = f\"fold_{train_idx}_{model.__class__.__name__}\"\n",
    "        )\n",
    "\n",
    "        one_class_crack = [\n",
    "            (str(train[0][0]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_crack\"), \n",
    "                str(train[0][1]).replace(f\"fold_{train_idx}\", f\"fold_{train_idx}_crack\"))\n",
    "        ]\n",
    "        saver5.save(\n",
    "            model_name=model.__class__.__name__,\n",
    "            dataset_name=\"carbucks_one_class_crack\",\n",
    "            fold=train_idx,\n",
    "            metric_name=ONE_METRIC,\n",
    "            metric_value=model.clone().fit(one_class_crack).evaluate(val)[ONE_METRIC] # type: ignore\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198f4ff6",
   "metadata": {},
   "source": [
    "### Cleaning up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f7cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "for single_class_dir in single_class_created_dirs:\n",
    "    os.system(f\"rm -rf {single_class_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9625ad1c",
   "metadata": {},
   "source": [
    "### Results and Discussion"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
