{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a04233e1",
   "metadata": {},
   "source": [
    "## This verifies that there is the same amount of labels and images and they match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9da81d2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from ml_carbucks import DATA_DIR\n",
    "\n",
    "std_dir = DATA_DIR / \"final_carbucks\" / \"standard\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "for split in splits:\n",
    "    for file in os.listdir(os.path.join(std_dir, \"labels\", split)):\n",
    "        key = file.split(\".txt\")[0]\n",
    "        \n",
    "        image_exists = os.path.exists(os.path.join(std_dir, \"images\", split, f\"{key}.jpg\"))\n",
    "\n",
    "        if not image_exists:\n",
    "            print(f\"Image {split}/{key} does not exist: path {os.path.join(std_dir, 'labels', split, f'{key}.txt')}\")\n",
    "\n",
    "for split in splits:\n",
    "    for file in os.listdir(os.path.join(std_dir, \"images\", split)):\n",
    "        key = file.split(\".jpg\")[0]\n",
    "        \n",
    "        label_exists = os.path.exists(os.path.join(std_dir, \"labels\", split, f\"{key}.txt\"))\n",
    "\n",
    "        if not label_exists:\n",
    "            print(f\"Label {split}/{key} does not exist: path {os.path.join(std_dir, 'images', split, f'{key}.jpg')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c799ce",
   "metadata": {},
   "source": [
    "## This filters the dataset by category only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d88c0ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from pprint import pprint as print\n",
    "\n",
    "import yaml\n",
    "\n",
    "from ml_carbucks.utils.conversions import convert_yolo_to_coco\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "def filter_dataset(data_dir: str | Path, splits:list[str], class_to_keep:str=\"2\", negative_sample_ratio:float=-1):\n",
    "\n",
    "    dataset_yaml_path = os.path.join(data_dir, \"dataset_single.yaml\")\n",
    "\n",
    "    if not os.path.exists(dataset_yaml_path):\n",
    "        logger.info(f\"Dataset yaml file does not exist. Proceeding...\")\n",
    "    else:\n",
    "        loaded_yaml = None\n",
    "        with open(dataset_yaml_path, \"r\") as f:\n",
    "            loaded_yaml = yaml.safe_load(f)\n",
    "        if loaded_yaml['nc'] == 1 and loaded_yaml['names'] == [f'class_to_keep_{class_to_keep}']:\n",
    "            logger.info(f\"Dataset already filtered to class {class_to_keep}. Exiting...\")\n",
    "            return\n",
    "\n",
    "\n",
    "    for split in splits:\n",
    "        obj = {}\n",
    "        for file in os.listdir(os.path.join(data_dir, \"labels\", split)):\n",
    "            if file.endswith(\".txt\"):\n",
    "                with open(os.path.join(data_dir, \"labels\", split, file), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                    file_classes = set()\n",
    "                    for line in lines:\n",
    "                        class_id = line.split()[0]\n",
    "                        file_classes.add(class_id)\n",
    "                    obj[file.split(\".\")[0]] = list(file_classes)\n",
    "\n",
    "        files_to_keep = []\n",
    "        for key, value in obj.items():\n",
    "            if class_to_keep in value:\n",
    "                files_to_keep.append(key)\n",
    "\n",
    "        files_neg_to_keep = []\n",
    "        for key, value in obj.items():\n",
    "            if class_to_keep not in value:\n",
    "                if negative_sample_ratio < 0:\n",
    "                    files_neg_to_keep.append(key)\n",
    "                elif len(files_neg_to_keep) < len(files_to_keep) and random.random() < negative_sample_ratio:\n",
    "                    files_neg_to_keep.append(key)\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        \n",
    "        print(f\"{len(files_to_keep)}, {len(files_neg_to_keep)}\")\n",
    "\n",
    "        for key, value in obj.items():\n",
    "            if key in files_to_keep or key in files_neg_to_keep:\n",
    "                # clean up other classes from the labels\n",
    "                with open(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"), \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                with open(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"), \"w\") as f:\n",
    "                    for line in lines:\n",
    "                        class_id = line.split()[0]\n",
    "                        if class_id == class_to_keep:\n",
    "                            f.write(line.replace(f\"{class_to_keep} \", \"0 \"))\n",
    "            else:\n",
    "                os.remove(os.path.join(data_dir, \"labels\", split, f\"{key}.txt\"))\n",
    "                os.remove(os.path.join(data_dir, \"images\", split, f\"{key}.jpg\"))   \n",
    "    \n",
    "    # write dataset.yaml file\n",
    "    dataset_yaml = {\n",
    "        'nc': 1,\n",
    "        'names': [f'class_to_keep_{class_to_keep}']\n",
    "    }\n",
    "\n",
    "    for split in splits:\n",
    "        dataset_yaml[split] = f\"images/{split}\"\n",
    "\n",
    "    with open(dataset_yaml_path, \"w\") as f:\n",
    "        yaml.dump(dataset_yaml, f)\n",
    "\n",
    "    convert_yolo_to_coco(data_dir, splits)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f892c49d",
   "metadata": {},
   "source": [
    "## Test it out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59b4324c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO __main__ 12:11:24 | Dataset yaml file does not exist. Proceeding...\n",
      "'332, 2940'\n",
      "'92, 708'\n",
      "'27, 273'\n",
      "INFO ml_carbucks.utils.conversions 12:11:25 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crack/instances_train_curated.json with 3272 images and 430 annotations.\n",
      "INFO ml_carbucks.utils.conversions 12:11:25 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crack/instances_val_curated.json with 800 images and 107 annotations.\n",
      "INFO ml_carbucks.utils.conversions 12:11:25 | Saved /home/bachelor/ml-carbucks/data/final_carbucks/crack/instances_test_curated.json with 300 images and 37 annotations.\n"
     ]
    }
   ],
   "source": [
    "from ml_carbucks import DATA_DIR, RESULTS_DIR\n",
    "\n",
    "data_dir = DATA_DIR / \"final_carbucks\" / \"crack\"\n",
    "splits = [\"train\", \"val\", \"test\"]\n",
    "class_to_keep = \"2\"  # crack class\n",
    "filter_dataset(data_dir, splits, class_to_keep=class_to_keep, negative_sample_ratio=-1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5531a18f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.233 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.229 ðŸš€ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/final_carbucks/crack/dataset_single.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=50, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=/home/bachelor/ml-carbucks/results/experiments/yolo11l_crack, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/results/experiments/yolo11l_crack/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1411795  ultralytics.nn.modules.head.Detect           [1, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,311,251 parameters, 25,311,235 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 5355.9Â±2721.0 MB/s, size: 2597.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/crack/labels/train... 3272 images, 2940 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 3272/3272 2.9Kit/s 1.1s0.0s\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/bachelor/ml-carbucks/data/final_carbucks/crack/labels/train.cache\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3184.9Â±2201.0 MB/s, size: 3008.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/crack/labels/val... 800 images, 708 backgrounds, 0 corrupt: 100% â”â”â”â”â”â”â”â”â”â”â”â” 800/800 5.6Kit/s 0.1s0.1s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/bachelor/ml-carbucks/data/final_carbucks/crack/labels/val.cache\n",
      "Plotting labels to /home/bachelor/ml-carbucks/results/experiments/yolo11l_crack/train/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/results/experiments/yolo11l_crack/train\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/50      10.3G      2.668      17.83       2.11          0        640: 25% â”â”â•¸â”€â”€â”€â”€â”€â”€â”€â”€â”€ 51/205 2.7it/s 33.5s<57.0s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01multralytics\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmodels\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01myolo\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m YOLO\n\u001b[32m      3\u001b[39m model = YOLO(\u001b[33m\"\u001b[39m\u001b[33myolo11l.pt\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m/home/bachelor/ml-carbucks/data/final_carbucks/crack/dataset_single.yaml\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m16\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mimgsz\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m640\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mRESULTS_DIR\u001b[49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexperiments\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m/\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43myolo11l_crack\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/ultralytics/engine/model.py:778\u001b[39m, in \u001b[36mModel.train\u001b[39m\u001b[34m(self, trainer, **kwargs)\u001b[39m\n\u001b[32m    775\u001b[39m     \u001b[38;5;28mself\u001b[39m.trainer.model = \u001b[38;5;28mself\u001b[39m.trainer.get_model(weights=\u001b[38;5;28mself\u001b[39m.model \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ckpt \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, cfg=\u001b[38;5;28mself\u001b[39m.model.yaml)\n\u001b[32m    776\u001b[39m     \u001b[38;5;28mself\u001b[39m.model = \u001b[38;5;28mself\u001b[39m.trainer.model\n\u001b[32m--> \u001b[39m\u001b[32m778\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    779\u001b[39m \u001b[38;5;66;03m# Update model and cfg after training\u001b[39;00m\n\u001b[32m    780\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m RANK \u001b[38;5;129;01min\u001b[39;00m {-\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m}:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:243\u001b[39m, in \u001b[36mBaseTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    240\u001b[39m         ddp_cleanup(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28mstr\u001b[39m(file))\n\u001b[32m    242\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_do_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:436\u001b[39m, in \u001b[36mBaseTrainer._do_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    434\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.scale(\u001b[38;5;28mself\u001b[39m.loss).backward()\n\u001b[32m    435\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ni - last_opt_step >= \u001b[38;5;28mself\u001b[39m.accumulate:\n\u001b[32m--> \u001b[39m\u001b[32m436\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    437\u001b[39m     last_opt_step = ni\n\u001b[32m    439\u001b[39m     \u001b[38;5;66;03m# Timed stopping\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/ultralytics/engine/trainer.py:682\u001b[39m, in \u001b[36mBaseTrainer.optimizer_step\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    680\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Perform a single step of the training optimizer with gradient clipping and EMA update.\"\"\"\u001b[39;00m\n\u001b[32m    681\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.unscale_(\u001b[38;5;28mself\u001b[39m.optimizer)  \u001b[38;5;66;03m# unscale gradients\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m682\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mutils\u001b[49m\u001b[43m.\u001b[49m\u001b[43mclip_grad_norm_\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10.0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    683\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.step(\u001b[38;5;28mself\u001b[39m.optimizer)\n\u001b[32m    684\u001b[39m \u001b[38;5;28mself\u001b[39m.scaler.update()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:221\u001b[39m, in \u001b[36mclip_grad_norm_\u001b[39m\u001b[34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m    216\u001b[39m         warnings.warn(\n\u001b[32m    217\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33m`parameters` is an empty generator, no gradient clipping will occur.\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    218\u001b[39m             stacklevel=\u001b[32m3\u001b[39m,\n\u001b[32m    219\u001b[39m         )\n\u001b[32m    220\u001b[39m grads = [p.grad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p.grad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m total_norm = \u001b[43m_get_total_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merror_if_nonfinite\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mforeach\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:36\u001b[39m, in \u001b[36m_no_grad.<locals>._no_grad_wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m     34\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_no_grad_wrapper\u001b[39m(*args, **kwargs):\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/nn/utils/clip_grad.py:89\u001b[39m, in \u001b[36m_get_total_norm\u001b[39m\u001b[34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[39m\n\u001b[32m     85\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors.items():\n\u001b[32m     86\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m     87\u001b[39m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[32m     88\u001b[39m     ):\n\u001b[32m---> \u001b[39m\u001b[32m89\u001b[39m         norms.extend(\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_foreach_norm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice_tensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m     90\u001b[39m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[32m     91\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m     92\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mforeach=True was passed, but can\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice.type\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m tensors\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     93\u001b[39m         )\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from ultralytics.models.yolo import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11l.pt\")\n",
    "\n",
    "model.train(\n",
    "    data=\"/home/bachelor/ml-carbucks/data/final_carbucks/crack/dataset_single.yaml\",\n",
    "    epochs=50,\n",
    "    batch=16,\n",
    "    imgsz=640,\n",
    "    project=RESULTS_DIR / \"experiments\" / \"yolo11l_crack\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c63907",
   "metadata": {},
   "source": [
    "## Results\n",
    "\n",
    "After running it by python files, it seems that the best result was at around 0.8 which is pretty good but not much better than with all categories\n",
    "\n",
    "It could be tested again with more epochs or different parameters"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
