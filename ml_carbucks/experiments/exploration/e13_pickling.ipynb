{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a986b3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Optional\n",
    "\n",
    "from ml_carbucks.adapters.UltralyticsAdapter import YoloUltralyticsAdapter, RtdetrUltralyticsAdapter\n",
    "from ml_carbucks.adapters.FasterRcnnAdapter import FasterRcnnAdapter\n",
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "\n",
    "\n",
    "v, dec = \"9_redone_hyper\", \"checkpoint_cleanup\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69540a77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:12:07 | Starting training...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:12:07 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:12:07 | COCO to YOLO conversion completed in 0.10 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:12:07 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_train_curated.yaml\n",
      "New https://pypi.org/project/ultralytics/8.3.230 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_train_curated.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=384, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.00012520173287645337, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.7751745737454412, mosaic=1.0, multi_scale=False, name=train64, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/train64, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=1.2339139608460587e-05, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,312,793 parameters, 25,312,777 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 3883.2¬±1836.0 MB/s, size: 3375.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/train.cache... 3272 images, 614 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3272/3272 14.1Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2026.3¬±1352.2 MB/s, size: 3742.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/train.cache... 3272 images, 614 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3272/3272 3.6Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/train64/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.00012520173287645337, momentum=0.7751745737454412) with parameter groups 167 weight(decay=0.0), 174 weight(decay=1.2339139608460587e-05), 173 bias(decay=0.0)\n",
      "Image sizes 384 train, 384 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train64\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/5      2.34G      2.521      3.344      2.085         25        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 409/409 9.6it/s 42.8ss<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        2/5      2.34G      2.354      2.967      1.972         30        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 409/409 11.9it/s 34.3s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        3/5      2.34G      2.241      2.766      1.874         22        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 409/409 12.1it/s 33.9s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        4/5      2.34G      2.166      2.669      1.817         13        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 409/409 12.0it/s 34.2s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        5/5      2.34G      2.104      2.536       1.78         22        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 409/409 11.9it/s 34.3s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 6.1it/s 33.4s0.3ss\n",
      "                   all       3272       4568      0.295      0.185      0.154     0.0636\n",
      "\n",
      "5 epochs completed in 0.060 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train64/weights/last.pt, 51.1MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train64/weights/best.pt, 51.1MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/train64/weights/best.pt...\n",
      "Ultralytics 8.3.229 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 7.5it/s 27.5ss<0.1s\n",
      "                   all       3272       4568      0.295      0.185      0.154     0.0637\n",
      "Speed: 0.1ms preprocess, 2.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train64\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:22 | Starting evaluation...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:22 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:16:22 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:22 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_val_curated.yaml\n",
      "Ultralytics 8.3.229 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 6300.2¬±276.2 MB/s, size: 2860.9 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/val.cache... 800 images, 164 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 3.5Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 2.6it/s 18.9s0.1s\n",
      "                   all        800       1094       0.24      0.148     0.0974     0.0392\n",
      "Speed: 0.6ms preprocess, 6.8ms inference, 0.0ms loss, 2.0ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val56\u001b[0m\n",
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/rtdetr-l.pt to 'rtdetr-l.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63.4MB 96.0MB/s 0.7s 0.6s<0.1s\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:46 | Starting training...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:46 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:16:46 | COCO to YOLO conversion completed in 0.10 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:16:46 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_train_curated.yaml\n",
      "New https://pypi.org/project/ultralytics/8.3.230 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_train_curated.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=384, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.0006706879778698832, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=rtdetr-l.pt, momentum=0.5574030731076479, mosaic=1.0, multi_scale=False, name=train65, nbs=64, nms=False, opset=None, optimize=False, optimizer=Adam, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/train65, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.00029530220750148133, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4298.3¬±1737.3 MB/s, size: 1506.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/train.cache... 3272 images, 614 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3272/3272 11.9Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 1689.0¬±84.5 MB/s, size: 4748.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/train.cache... 3272 images, 614 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 3272/3272 4.9Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/train65/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m Adam(lr=0.0006706879778698832, momentum=0.5574030731076479) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.00029530220750148133), 226 bias(decay=0.0)\n",
      "Image sizes 384 train, 384 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train65\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        1/5      6.95G       1.68      17.94      0.968         42        384: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/205  10.5s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        1/5      7.23G      1.398      1.374     0.8481         13        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 2.4it/s 1:25<0.5ss\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        2/5      7.23G      1.318     0.8104     0.6141         26        384: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/205  0.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        2/5      7.23G      1.053     0.9609     0.5333         21        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 2.9it/s 1:11<0.3ss\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        3/5      7.23G     0.8905      1.064     0.4634         40        384: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/205  0.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        3/5      7.23G     0.8941       1.07      0.443         18        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 2.9it/s 1:10<0.3ss\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        4/5      7.23G     0.7748      1.227     0.5017         36        384: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/205  0.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        4/5      7.23G     0.8408      1.071     0.4024         17        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 2.9it/s 1:10<0.3ss\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        5/5      7.23G     0.8632       1.06      0.417         36        384: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/205  0.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        5/5      7.23G     0.7877      1.067     0.3682         18        384: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 205/205 2.9it/s 1:11<0.3ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 103/103 4.5it/s 22.9s0.2s\n",
      "                   all       3272       4568     0.0695      0.153     0.0379     0.0181\n",
      "\n",
      "5 epochs completed in 0.109 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train65/weights/last.pt, 66.2MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train65/weights/best.pt, 66.2MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/train65/weights/best.pt...\n",
      "Ultralytics 8.3.229 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 103/103 5.0it/s 20.4s0.2s\n",
      "                   all       3272       4568     0.0693      0.153      0.038     0.0181\n",
      "Speed: 0.1ms preprocess, 4.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train65\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:23:49 | Starting evaluation...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:23:49 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:23:49 | COCO to YOLO conversion completed in 0.03 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:23:49 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_val_curated.yaml\n",
      "Ultralytics 8.3.229 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 2836.5¬±3096.3 MB/s, size: 2336.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/val.cache... 800 images, 164 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 2.3Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 5.2it/s 9.7s0.1s\n",
      "                   all        800       1094     0.0633      0.155     0.0266     0.0108\n",
      "Speed: 0.3ms preprocess, 7.4ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val57\u001b[0m\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:24:02 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:24:02 | Epoch 1/5\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:25:45 | Epoch 2/5\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:27:27 | Epoch 3/5\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:29:10 | Epoch 4/5\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:30:53 | Epoch 5/5\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:32:35 | Starting evaluation...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:32:54 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:32:54 | Epoch 1/5\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:34:16 | Epoch 2/5\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:35:37 | Epoch 3/5\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:36:58 | Epoch 4/5\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 15:38:21 | Epoch 5/5\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluation results:\n",
      "{'map_50': 0.0973865522109537, 'map_50_95': 0.039158445750999915, 'map_75': -inf, 'classes': []}\n",
      "{'map_50': 0.026583786966850587, 'map_50_95': 0.010797734906550505, 'map_75': -inf, 'classes': []}\n",
      "{'map_50': 0.046577658504247665, 'map_50_95': 0.014303876087069511, 'map_75': 0.0056637865491211414, 'classes': [1, 2, 3]}\n",
      "{'map_50': 0.05875212699174881, 'map_50_95': 0.02138049341738224, 'map_75': 0.011335215531289577, 'classes': [1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "override_epochs: Optional[int] = 5\n",
    "\n",
    "adapters: List[BaseDetectionAdapter] = [\n",
    "    YoloUltralyticsAdapter().set_params({\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": override_epochs or 18,\n",
    "        \"lr\": 0.00012520173287645337,\n",
    "        \"momentum\": 0.7751745737454412,\n",
    "        \"weight_decay\": 1.2339139608460587e-05,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }),\n",
    "    RtdetrUltralyticsAdapter().set_params( {\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": override_epochs or 28,\n",
    "        \"lr\": 0.0006706879778698832,\n",
    "        \"momentum\": 0.5574030731076479,\n",
    "        \"weight_decay\": 0.00029530220750148133,\n",
    "        \"optimizer\": \"Adam\"\n",
    "    }),\n",
    "    FasterRcnnAdapter().set_params({\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": override_epochs or 22,\n",
    "        \"lr_head\": 0.0001760393933421673,\n",
    "        \"weight_decay_head\": 1.6330563888408336e-05,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "\n",
    "    }),\n",
    "    EfficientDetAdapter().set_params({\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 32,\n",
    "        \"epochs\": override_epochs or 15,\n",
    "        \"optimizer\": \"adam\",\n",
    "        \"lr\": 0.0002740546809702111,\n",
    "        \"weight_decay\": 8.588886298464197e-05,\n",
    "        \"loader\": \"inbuild\"\n",
    "    }),\n",
    "]\n",
    "results = []\n",
    "\n",
    "for adapter in adapters:\n",
    "    adapter.setup()\n",
    "    adapter.fit(DatasetsPathManager.CARBUCKS_TRAIN_STANDARD)\n",
    "    res = adapter.evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "\n",
    "    results.append(res)\n",
    "\n",
    "print(\"Evaluation results:\")\n",
    "for res in results:\n",
    "    print(res)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5766bfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for iii in range(len(adapters)):\n",
    "    adapters[iii].save(f\"/home/bachelor/ml-carbucks/results/pickle{v}\", prefix=f\"{adapters[iii].__class__.__name__}_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b91b3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "\n",
    "\n",
    "loaded_adapters: List[BaseDetectionAdapter] = [\n",
    "    YoloUltralyticsAdapter(weights=f\"/home/bachelor/ml-carbucks/results/pickle{v}/YoloUltralyticsAdapter_model.pkl\"),\n",
    "    RtdetrUltralyticsAdapter(weights=f\"/home/bachelor/ml-carbucks/results/pickle{v}/RtdetrUltralyticsAdapter_model.pkl\"),\n",
    "    FasterRcnnAdapter(weights=f\"/home/bachelor/ml-carbucks/results/pickle{v}/FasterRcnnAdapter_model.pkl\"),\n",
    "    EfficientDetAdapter(weights=f\"/home/bachelor/ml-carbucks/results/pickle{v}/EfficientDetAdapter_model.pkl\"),\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "df8b6b88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING ml_carbucks.adapters.UltralyticsAdapter 15:40:06 | Overwriting adapter parameters with loaded pickled parameters.\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:06 | Starting evaluation...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:06 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:40:06 | COCO to YOLO conversion completed in 0.03 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:06 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_val_curated.yaml\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5860.7¬±882.6 MB/s, size: 3512.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/val.cache... 800 images, 164 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 4.6Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 5.7it/s 8.7s<0.1s\n",
      "                   all        800       1094       0.24      0.148     0.0972     0.0391\n",
      "Speed: 0.2ms preprocess, 3.3ms inference, 0.0ms loss, 0.7ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val58\u001b[0m\n",
      "WARNING ml_carbucks.adapters.UltralyticsAdapter 15:40:17 | Overwriting adapter parameters with loaded pickled parameters.\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:17 | Starting evaluation...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:17 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.utils.conversions 15:40:17 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 15:40:17 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/final_carbucks/standard/instances_val_curated.yaml\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4577.2¬±1771.8 MB/s, size: 2608.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/final_carbucks/standard/labels/val.cache... 800 images, 164 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 800/800 1.9Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 50/50 5.4it/s 9.2s0.1s\n",
      "                   all        800       1094     0.0632      0.155     0.0266     0.0108\n",
      "Speed: 0.3ms preprocess, 7.5ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val59\u001b[0m\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 15:40:29 | Overwriting adapter parameters with loaded pickled parameters.\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 15:40:29 | Starting evaluation...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING ml_carbucks.adapters.EfficientDetAdapter 15:40:48 | Overwriting adapter parameters with loaded pickled parameters.\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Evaluation results of loaded adapters:\n",
      "{'map_50': 0.09715922335280462, 'map_50_95': 0.03913957510290556, 'map_75': -inf, 'classes': []}\n",
      "{'map_50': 0.026566410010862393, 'map_50_95': 0.010791814082945411, 'map_75': -inf, 'classes': []}\n",
      "{'map_50': 0.04838128760457039, 'map_50_95': 0.014445419423282146, 'map_75': 0.005470282398164272, 'classes': [1, 2, 3]}\n",
      "{'map_50': 0.05875212699174881, 'map_50_95': 0.02138049341738224, 'map_75': 0.011335215531289577, 'classes': [1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "lesults = []\n",
    "for jjj in range(len(loaded_adapters)):\n",
    "    try:\n",
    "        loaded_adapters[jjj].setup()\n",
    "        les = loaded_adapters[jjj].evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "        lesults.append(les)\n",
    "    except Exception:\n",
    "        lesults.append(\"Evaluation failed\")\n",
    "\n",
    "print(\"Evaluation results of loaded adapters:\")\n",
    "for les in lesults:\n",
    "    print(les)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6bca5668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing results\n",
      "Original: 0.0973865522109537\n",
      "Loaded:   0.09715922335280462\n",
      "Original: 0.026583786966850587\n",
      "Loaded:   0.026566410010862393\n",
      "Original: 0.046577658504247665\n",
      "Loaded:   0.04838128760457039\n",
      "Original: 0.05875212699174881\n",
      "Loaded:   0.05875212699174881\n"
     ]
    }
   ],
   "source": [
    "print(\"Comparing results\")\n",
    "metric = \"map_50\"\n",
    "\n",
    "for original, loaded in zip(results, lesults):\n",
    "    print(\"Original:\", original[metric])\n",
    "    print(\"Loaded:  \", loaded[metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2ace7a",
   "metadata": {},
   "source": [
    "NOTES:\n",
    "- X yolo works fine \n",
    "- X rtdetr works fine\n",
    "- X fasterRCNN works fine\n",
    "- X efficientDet works fine"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
