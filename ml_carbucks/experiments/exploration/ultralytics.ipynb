{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad1ab050",
   "metadata": {},
   "source": [
    "# THIS was initial Ultralytics exploration with optuna setup and hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50c9b53b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime as dt  # noqa: F401\n",
    "from pathlib import Path\n",
    "from typing import Any, Callable, Dict, Union\n",
    "\n",
    "from ultralytics.models.yolo.model import YOLO\n",
    "from ultralytics.models.rtdetr.model import RTDETR\n",
    "from optuna import Trial\n",
    "import optuna\n",
    "import cloudpickle as cpkl  # noqa: F401\n",
    "\n",
    "from ml_carbucks import (  # noqa: F401\n",
    "    RTDETR_PRETRAINED_L,\n",
    "    YOLO_PRETRAINED_11L,\n",
    "    YOLO_PRETRAINED_11N,\n",
    "    DATA_CAR_DD_YAML,\n",
    "    RESULTS_DIR,\n",
    ")\n",
    "\n",
    "\n",
    "def get_trial_params(trial: Trial, version: int) -> Dict[str, Any]:\n",
    "    if version == 1:\n",
    "        return get_params_hyper(trial)\n",
    "    elif version == 2:\n",
    "        return get_params_augmentation(trial)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported version: {version}\")\n",
    "\n",
    "\n",
    "def get_params_hyper(trial: Trial) -> Dict[str, Any]:\n",
    "    epochs = trial.suggest_int(\"epochs\", 30, 150)\n",
    "    batch = trial.suggest_categorical(\"batch\", [8, 16, 32])\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-1, log=True)\n",
    "    momentum = trial.suggest_float(\"momentum\", 0.3, 0.99)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-5, 1e-2, log=True)\n",
    "    patience = trial.suggest_int(\"patience\", 25, 75)\n",
    "\n",
    "    # imgsz = trial.suggest_categorical(\"imgsz\", [320, 640, 960])\n",
    "    opt = trial.suggest_categorical(\"optimizer\", [\"AdamW\", \"NAdam\"])\n",
    "\n",
    "    return {\n",
    "        \"optimizer\": opt,\n",
    "        \"epochs\": epochs,\n",
    "        \"batch\": batch,\n",
    "        \"lr0\": lr,\n",
    "        \"momentum\": momentum,\n",
    "        \"weight_decay\": weight_decay,\n",
    "        \"patience\": patience,\n",
    "    }\n",
    "\n",
    "\n",
    "def get_params_augmentation(trial: Trial) -> Dict[str, Any]:\n",
    "    hsv_h = trial.suggest_float(\"hsv_h\", 0.0, 0.1)\n",
    "    hsv_s = trial.suggest_float(\"hsv_s\", 0.0, 1.0)\n",
    "    hsv_v = trial.suggest_float(\"hsv_v\", 0.0, 1.0)\n",
    "    degrees = trial.suggest_float(\"degrees\", 0.0, 45.0)\n",
    "    translate = trial.suggest_float(\"translate\", 0.0, 0.5)\n",
    "    scale = trial.suggest_float(\"scale\", 0.0, 1.0)\n",
    "    shear = trial.suggest_float(\"shear\", -20.0, 20.0)\n",
    "    fliplr = trial.suggest_float(\"fliplr\", 0.0, 1.0)\n",
    "    mosaic = trial.suggest_float(\"mosaic\", 0.0, 1.0)\n",
    "    mixup = trial.suggest_float(\"mixup\", 0.0, 1.0)\n",
    "\n",
    "    return {\n",
    "        \"hsv_h\": hsv_h,\n",
    "        \"hsv_s\": hsv_s,\n",
    "        \"hsv_v\": hsv_v,\n",
    "        \"degrees\": degrees,\n",
    "        \"translate\": translate,\n",
    "        \"scale\": scale,\n",
    "        \"shear\": shear,\n",
    "        \"fliplr\": fliplr,\n",
    "        \"mosaic\": mosaic,\n",
    "        \"mixup\": mixup,\n",
    "    }\n",
    "\n",
    "\n",
    "def create_objective(\n",
    "    version: str,\n",
    "    data: Path,\n",
    "    name: str,\n",
    "    device: str,\n",
    "    results_dir: Path,\n",
    "    params_version: int,\n",
    "    override_params: Dict[str, Any] = {},\n",
    ") -> Callable:\n",
    "\n",
    "    def objective(trial: Trial) -> float:\n",
    "        if \"rtdetr\" in version:\n",
    "            model = RTDETR(version)\n",
    "        else:\n",
    "            model = YOLO(version)\n",
    "        try:\n",
    "            params = get_trial_params(trial, version=params_version)\n",
    "\n",
    "            if any(param in params for param in override_params):\n",
    "                raise ValueError(\"Override params conflict with trial params\")\n",
    "\n",
    "            params.update(override_params)\n",
    "\n",
    "            results = model.train(\n",
    "                pretrained=True,\n",
    "                seed=42,\n",
    "                data=data,\n",
    "                name=name,\n",
    "                device=device,\n",
    "                verbose=False,\n",
    "                project=str(results_dir),\n",
    "                **params,\n",
    "            )\n",
    "\n",
    "            trial.set_user_attr(\"params\", params)\n",
    "            trial.set_user_attr(\"results\", results.results_dict)  # type: ignore\n",
    "            score = results.fitness  # type: ignore\n",
    "\n",
    "            del results\n",
    "\n",
    "            return score\n",
    "\n",
    "        except optuna.exceptions.TrialPruned as e:\n",
    "            print(\"Trial pruned\")  # NOTE: this should be replace to logger\n",
    "            trial.set_user_attr(\"error\", str(e))\n",
    "            raise e\n",
    "        except Exception as e:\n",
    "            print(f\"Error in objective: {e}\")\n",
    "            trial.set_user_attr(\"error\", str(e))\n",
    "            raise e\n",
    "        finally:\n",
    "            del model\n",
    "\n",
    "    return objective\n",
    "\n",
    "\n",
    "def execute_study(\n",
    "    name: str,\n",
    "    n_trials: int = 25,\n",
    "    results_dir: Path = RESULTS_DIR,\n",
    "    version: Union[Path, str] = YOLO_PRETRAINED_11N,\n",
    "    data: Path = DATA_CAR_DD_YAML,\n",
    "    direction: str = \"maximize\",\n",
    "    device: str = \"0\",\n",
    "    params_version: int = 1,\n",
    "    override_params: Dict[str, Any] = {},\n",
    "    enqueue_trials: list = [],\n",
    "):\n",
    "    \"\"\"Execute an Optuna study to optimize YOLO training parameters\n",
    "\n",
    "    Args:\n",
    "        name (str): Name of the study\n",
    "        n_trials (int, optional): Number of trials to run. Defaults to 25.\n",
    "        results_dir (Path, optional): Directory to store results. Defaults to RESULTS_DIR.\n",
    "        version (Union[Path, str], optional): YOLO model version or path. Defaults to YOLO_PRETRAINED_11N.\n",
    "        data (Path, optional): Path to data YAML file. Defaults to DATA_CAR_D\n",
    "        direction (str, optional): Direction of optimization ('minimize' or 'maximize'). Defaults to 'maximize'.\n",
    "        device (str, optional): Device to use (e.g., '0' for GPU\n",
    "        params_version (int, optional): Version of parameter set to optimize. Defaults to 1.\n",
    "        override_params (Dict[str, Any], optional): Parameters to override in each trial. Defaults to {}.\n",
    "        enqueue_trials (list, optional): List of parameter dicts to enqueue as trials. It is used to add specific (manually defined) trials into the study. Useful if you want to test specific configurations or add default values. Defaults to [].\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If an unsupported version is provided.\n",
    "        TypeError: If an invalid type is provided for any parameter.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    sql_path = results_dir / f\"{name}.db\"\n",
    "\n",
    "    study = optuna.create_study(\n",
    "        direction=direction,\n",
    "        study_name=name,\n",
    "        load_if_exists=True,\n",
    "    )\n",
    "\n",
    "    for trial_params in enqueue_trials:\n",
    "        # NOTE: Enqueue trials with specific parameters\n",
    "        # check if trial with same params exists\n",
    "        existing_trials = [t for t in study.trials if t.params == trial_params]\n",
    "        if not existing_trials:\n",
    "            study.enqueue_trial(trial_params)\n",
    "\n",
    "    study.optimize(\n",
    "        func=create_objective(\n",
    "            version=str(version),\n",
    "            data=data,\n",
    "            name=name,\n",
    "            device=device,\n",
    "            results_dir=results_dir,\n",
    "            params_version=params_version,\n",
    "            override_params=override_params,\n",
    "        ),\n",
    "        n_trials=n_trials,\n",
    "        gc_after_trial=True,\n",
    "    )\n",
    "\n",
    "\n",
    "# RUN_NAME = dt.datetime.now().strftime(\"%Y%m%d_%H%M%S\") + \"_v2\"\n",
    "# RUN_NAME = \"20251011_rtdetr_initial_v1\"\n",
    "\n",
    "# execute_study(\n",
    "#     name=f\"{RUN_NAME}_optuna\",\n",
    "#     version=RTDETR_PRETRAINED_L,\n",
    "#     n_trials=100,\n",
    "#     params_version=1,\n",
    "#     override_params={\n",
    "#         \"imgsz\": 320,\n",
    "#     },\n",
    "# )\n",
    "\n",
    "\n",
    "def train_model(\n",
    "    name: str,\n",
    "    params: Dict[str, Any],\n",
    "    version: Union[Path, str],\n",
    "    data: Union[Path, str] = DATA_CAR_DD_YAML,\n",
    "    results_dir: Path = RESULTS_DIR,\n",
    "    device: str = \"0\",\n",
    "    resume: bool = False,\n",
    ") -> Any:\n",
    "    if \"rtdetr\" in str(version):\n",
    "        model = RTDETR(str(version))\n",
    "    else:\n",
    "        model = YOLO(version)\n",
    "    results = model.train(\n",
    "        pretrained=True,\n",
    "        seed=42,\n",
    "        data=data,\n",
    "        resume=resume,\n",
    "        name=name,\n",
    "        device=device,\n",
    "        verbose=False,\n",
    "        save=True,\n",
    "        project=str(results_dir),\n",
    "        **params,\n",
    "    )\n",
    "    return results\n",
    "\n",
    "\n",
    "# TRAIN_NAME = \"large_1024_hyper&augm_v1\"\n",
    "# final_results = train_model(\n",
    "#     name=TRAIN_NAME,\n",
    "#     params={\n",
    "#         \"imgsz\": 1024,\n",
    "#         \"optimizer\": \"AdamW\",\n",
    "#         \"epochs\": 131,\n",
    "#         \"batch\": 8,\n",
    "#         \"lr0\": 0.00029631881419241645,\n",
    "#         \"momentum\": 0.38243835004885135,\n",
    "#         \"weight_decay\": 9.16499123351809e-05,\n",
    "#         \"patience\": 31,\n",
    "#         \"hsv_h\": 0.015,\n",
    "#         \"hsv_s\": 0.7,\n",
    "#         \"hsv_v\": 0.4,\n",
    "#         \"degrees\": 0.0,\n",
    "#         \"translate\": 0.1,\n",
    "#         \"scale\": 0.5,\n",
    "#         \"shear\": 0.0,\n",
    "#         \"fliplr\": 0.5,\n",
    "#         \"mosaic\": 1.0,\n",
    "#         \"mixup\": 0.0,\n",
    "#     },\n",
    "#     version=str(\n",
    "#         \"/home/bachelor/ml-carbucks/results/large_1024_hyper&augm_v1/weights/last.pt\"\n",
    "#     ),\n",
    "#     resume=True,\n",
    "# )\n",
    "\n",
    "# cpkl_path = RESULTS_DIR / f\"{TRAIN_NAME}_results.pkl\"\n",
    "# with open(cpkl_path, \"wb\") as f:\n",
    "#     cpkl.dump(final_results, f)\n",
    "\n",
    "# print(f\"Train results {final_results} saved to {cpkl_path}\")\n",
    "# NOTE: to view optuna dashboard in terminal: optuna dashboard sqlite:///{sql_path}\n",
    "\n",
    "# ff_results = train_model(\n",
    "#     name=\"rtdetr_l_320_lr-vsmall_v1\",\n",
    "#     params={\n",
    "#         \"imgsz\": 320,\n",
    "#         \"batch\": 16,\n",
    "#         \"epochs\": 500,\n",
    "#         \"optimizer\": \"AdamW\",\n",
    "#         \"lr0\": 0.0001,\n",
    "#     },\n",
    "#     version=RTDETR_PRETRAINED_L,\n",
    "#     device=\"0\",\n",
    "# )\n",
    "# cpkl_path = RESULTS_DIR / \"rtdetr_l_640_final_v1_results.pkl\"\n",
    "# with open(cpkl_path, \"wb\") as f:\n",
    "#     cpkl.dump(ff_results, f)\n",
    "# print(f\"Train results {ff_results} saved to {cpkl_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
