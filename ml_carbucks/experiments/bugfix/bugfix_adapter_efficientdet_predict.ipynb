{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a49cb3",
   "metadata": {},
   "source": [
    "# THIS WAS to debug why predictions were off during predict phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a14a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.ops import nms\n",
    "from effdet import create_model, create_loader\n",
    "from effdet.data import resolve_input_config, resolve_fill_color\n",
    "from effdet.bench import DetBenchPredict  # noqa F401\n",
    "from effdet.anchors import Anchors, AnchorLabeler\n",
    "from effdet.data.transforms import ResizePad, ImageToNumpy, Compose\n",
    "from timm.optim._optim_factory import create_optimizer_v2\n",
    "\n",
    "\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import (\n",
    "    BaseDetectionAdapter,\n",
    "    ADAPTER_PREDICTION,\n",
    ")\n",
    "from ml_carbucks.patches.effdet import (\n",
    "    CocoStatsEvaluator,\n",
    "    ConcatDetectionDataset,\n",
    "    create_dataset_custom,\n",
    ")\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision  # noqa: F401\n",
    "\n",
    "from ml_carbucks import DATA_DIR, RESULTS_DIR\n",
    "from ml_carbucks.utils.inference import plot_img_pred as ppp  # noqa: F401\n",
    "from ml_carbucks.utils.preprocessing import create_clean_loader  # noqa: F401\n",
    "\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EfficientDetAdapter(BaseDetectionAdapter):\n",
    "\n",
    "\n",
    "    weights: str | Path = \"\"\n",
    "    backbone: str = \"tf_efficientdet_d0\"\n",
    "    bench_labeler: bool = True\n",
    "\n",
    "    optimizer: str = \"momentum\"\n",
    "    lr: float = 8e-3\n",
    "    weight_decay: float = 5e-5\n",
    "    confidence_threshold: float = 0.2\n",
    "\n",
    "    def save(self, dir: Path | str, prefix: str = \"\", suffix: str = \"\") -> Path:\n",
    "        save_path = Path(dir) / f\"{prefix}model{suffix}.pth\"\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.model.model.state_dict(), save_path)\n",
    "        return save_path\n",
    "\n",
    "    def clone(self) -> \"EfficientDetAdapter\":\n",
    "        return EfficientDetAdapter(\n",
    "            classes=deepcopy(self.classes),\n",
    "            weights=self.weights,\n",
    "            img_size=self.img_size,\n",
    "            batch_size=self.batch_size,\n",
    "            epochs=self.epochs,\n",
    "            backbone=self.backbone,\n",
    "            bench_labeler=self.bench_labeler,\n",
    "            optimizer=self.optimizer,\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "\n",
    "    def _predict_preprocess_images_v2(self, images: List[torch.Tensor]):\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        fill_color = resolve_fill_color(\n",
    "            input_config[\"fill_color\"], input_config[\"mean\"]\n",
    "        )\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ResizePad(\n",
    "                    target_size=self.img_size,\n",
    "                    interpolation=input_config[\"interpolation\"],\n",
    "                    fill_color=fill_color,\n",
    "                ),\n",
    "                ImageToNumpy(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        batch_list = []\n",
    "        img_scaled = []\n",
    "\n",
    "        for img in images:\n",
    "            if isinstance(img, torch.Tensor):\n",
    "                img = img.permute(1, 2, 0).cpu().numpy()  # [H,W,C]\n",
    "            img_pil = Image.fromarray(img.astype(np.uint8))\n",
    "            img_proc, anno = transform(img_pil, dict())  # no annotations\n",
    "            batch_list.append(img_proc)\n",
    "            img_scaled.append(anno.get(\"img_scale\", 1.0))\n",
    "\n",
    "        batch_np = np.stack(batch_list, axis=0)  # [B,C,H,W]\n",
    "        batch_tensor = torch.from_numpy(batch_np).float().to(self.device)\n",
    "\n",
    "        return batch_tensor, img_scaled\n",
    "\n",
    "    # def predict(\n",
    "    #     self,\n",
    "    #     images: List[torch.Tensor],\n",
    "    #     conf_threshold: float = 0.25,\n",
    "    #     iou_threshold: float = 0.45,\n",
    "    #     max_detections: int = 100,\n",
    "    # ) -> List[ADAPTER_PREDICTION]:\n",
    "\n",
    "    #     # NOTE: Something is wrong PROBABLY HERE, it needs to be verified more\n",
    "    #     \"\"\"\n",
    "    #     The issue is that predicitons are weird but the results of the evaluation\n",
    "    #     are good. So either the evaluation is wrong or the prediction extraction is wrong.\n",
    "    #     \"\"\"\n",
    "    #     predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "    #     predictor.to(self.device)\n",
    "    #     predictor.eval()\n",
    "    #     predictions: List[ADAPTER_PREDICTION] = []\n",
    "\n",
    "    #     with torch.no_grad():\n",
    "    #         batch_tensor, batch_scales = self._predict_preprocess_images_v2(images)\n",
    "    #         outputs = predictor(batch_tensor)\n",
    "\n",
    "    #         for i, pred in enumerate(outputs):\n",
    "    #             out = pred.cpu()  # move to CPU\n",
    "    #             boxes = out[:, :4]\n",
    "    #             scores = out[:, 4]\n",
    "    #             labels_idx = out[:, 5].long()\n",
    "\n",
    "    #             # filter by confidence\n",
    "    #             mask = scores >= conf_threshold\n",
    "    #             boxes, scores, labels_idx = boxes[mask], scores[mask], labels_idx[mask]\n",
    "\n",
    "    #             # apply NMS per image\n",
    "    #             keep = nms(boxes, scores, iou_threshold)\n",
    "    #             keep = keep[:max_detections]  # take top-k\n",
    "\n",
    "    #             boxes = boxes[keep].numpy().copy()\n",
    "    #             boxes *= batch_scales[i]\n",
    "\n",
    "    #             scores = scores[keep].numpy()\n",
    "    #             labels = [self.classes[idx - 1] for idx in labels_idx[keep]]\n",
    "\n",
    "    #             predictions.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "\n",
    "    #     return predictions\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        images: List[torch.Tensor],\n",
    "        conf_threshold: float = 0.25,\n",
    "        iou_threshold: float = 0.45,\n",
    "        max_detections: int = 100,\n",
    "    ) -> List[ADAPTER_PREDICTION]:\n",
    "        \"\"\"\n",
    "        The issue is that predicitons are weird but the results of the evaluation\n",
    "        are good. So either the evaluation is wrong or the prediction extraction is wrong.\n",
    "        \"\"\"\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()\n",
    "        predictions: List[ADAPTER_PREDICTION] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            img_sizes = torch.tensor(\n",
    "                [[img.shape[1], img.shape[2]] for img in images], dtype=torch.float32\n",
    "            ).to(self.device)\n",
    "\n",
    "            batch_tensor, batch_scales = self._predict_preprocess_images_v2(images)\n",
    "\n",
    "            img_scales = torch.tensor(batch_scales, dtype=torch.float32).to(self.device)\n",
    "            img_info_dict = {\n",
    "                \"img_scale\": img_scales,\n",
    "                \"img_size\": img_sizes,\n",
    "            }\n",
    "\n",
    "            outputs = predictor(batch_tensor, img_info=img_info_dict)\n",
    "\n",
    "            for i, pred in enumerate(outputs):\n",
    "                mask = pred[:, 4] >= conf_threshold\n",
    "                if mask.sum() == 0:\n",
    "                    boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "                    scores = np.zeros((0,), dtype=np.float32)\n",
    "                    labels = []\n",
    "                else:\n",
    "                    boxes = pred[mask, :4]\n",
    "                    scores = pred[mask, 4]\n",
    "                    labels_idx = pred[mask, 5].long()\n",
    "\n",
    "                    # apply NMS per image\n",
    "                    keep = nms(boxes, scores, iou_threshold)\n",
    "                    keep = keep[:max_detections]  # take top-k\n",
    "\n",
    "                    boxes = boxes[keep].cpu().numpy().copy()\n",
    "\n",
    "                    scores = scores[keep].cpu().numpy()\n",
    "                    labels = [self.classes[idx - 1] for idx in labels_idx[keep]]\n",
    "                predictions.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def setup(self) -> \"EfficientDetAdapter\":\n",
    "        img_size = self.img_size\n",
    "\n",
    "        backbone = self.backbone\n",
    "        weights = self.weights\n",
    "        bench_labeler = self.bench_labeler\n",
    "\n",
    "        # NOTE: img size would need to be updated here if we want to change it\n",
    "        # I dont think it is possible to change it after model creation\n",
    "        extra_args = dict(image_size=(img_size, img_size))\n",
    "        self.model = create_model(\n",
    "            model_name=backbone,\n",
    "            bench_task=\"train\",\n",
    "            num_classes=len(self.classes),\n",
    "            pretrained=weights == \"\",\n",
    "            checkpoint_path=str(weights),\n",
    "            bench_labeler=bench_labeler,\n",
    "            checkpoint_ema=False,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        self.labeler = None\n",
    "        if bench_labeler is False:\n",
    "            self.labeler = AnchorLabeler(\n",
    "                Anchors.from_config(self.model.config),\n",
    "                self.model.config.num_classes,\n",
    "                match_threshold=0.5,\n",
    "            )\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> \"EfficientDetAdapter\":\n",
    "        logger.info(\"Starting training...\")\n",
    "        self.model.train()\n",
    "\n",
    "        epochs = self.epochs\n",
    "        opt = self.optimizer\n",
    "        lr = self.lr\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        train_loader = self._create_loader(datasets, is_training=True)\n",
    "\n",
    "        parser_max_label = train_loader.dataset.parsers[0].max_label  # type: ignore\n",
    "        config_num_classes = self.model.config.num_classes\n",
    "\n",
    "        if parser_max_label != config_num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Number of classes in dataset ({parser_max_label}) does not match \"\n",
    "                f\"model config ({config_num_classes}).\"\n",
    "                f\"Please verify that the dataset is curated (classes IDs start from 1)\"\n",
    "            )\n",
    "\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=opt,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            _ = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _create_loader(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]], is_training: bool\n",
    "    ):\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        all_datasets = []\n",
    "        for img_dir, ann_file in datasets:\n",
    "            dataset = create_dataset_custom(\n",
    "                img_dir=img_dir,\n",
    "                ann_file=ann_file,\n",
    "                has_labels=True,\n",
    "            )\n",
    "            all_datasets.append(dataset)\n",
    "\n",
    "        concat_dataset = ConcatDetectionDataset(all_datasets)\n",
    "\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        loader = create_loader(\n",
    "            concat_dataset,\n",
    "            input_size=input_config[\"input_size\"],\n",
    "            batch_size=batch_size,\n",
    "            is_training=is_training,\n",
    "            use_prefetcher=True,\n",
    "            interpolation=input_config[\"interpolation\"],\n",
    "            fill_color=input_config[\"fill_color\"],\n",
    "            mean=input_config[\"mean\"],\n",
    "            std=input_config[\"std\"],\n",
    "            num_workers=4,\n",
    "            distributed=False,\n",
    "            pin_mem=False,\n",
    "            anchor_labeler=self.labeler,\n",
    "            transform_fn=None,\n",
    "            collate_fn=None,\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def train_epoch(\n",
    "        self, optimizer: torch.optim.Optimizer, loader: DataLoader\n",
    "    ) -> float:\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            output = self.model(imgs, targets)\n",
    "            loss = output[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def debug(\n",
    "        self,\n",
    "        train_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        val_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        results_path: str | Path,\n",
    "        results_name: str,\n",
    "    ) -> Dict[str, float]:\n",
    "        logger.info(\"Debugging training and evaluation loops...\")\n",
    "\n",
    "        epochs = self.epochs\n",
    "        train_loader = self._create_loader(train_datasets, is_training=True)\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=self.optimizer,\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        saver = ResultSaver(\n",
    "            path=results_path,\n",
    "            name=results_name,\n",
    "        )\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            total_loss = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "            val_metrics = self.evaluate(val_datasets)\n",
    "            saver.save(\n",
    "                epoch=epoch,\n",
    "                loss=total_loss,\n",
    "                val_map=val_metrics[\"map_50_95\"],\n",
    "                val_map_50=val_metrics[\"map_50\"],\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Debug Epoch {epoch}/{epochs} - Loss: {total_loss}, Val MAP: {val_metrics['map_50_95']}\"\n",
    "            )\n",
    "            saver.plot(show=False)\n",
    "\n",
    "        return val_metrics  # type: ignore\n",
    "\n",
    "            \n",
    "    def evaluate(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loader = self._create_loader(datasets, is_training=False)\n",
    "\n",
    "        evaluator = CocoStatsEvaluator(val_loader.dataset)\n",
    "        total_loss = 0.0\n",
    "        cnt = 10\n",
    "        metric = MeanAveragePrecision(extended_summary=False, class_metrics=False)\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                output = self.model(imgs, targets)\n",
    "                pred_output = predictor(imgs)\n",
    "\n",
    "\n",
    "                loss = output[\"loss\"]\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                evaluator.add_predictions(output[\"detections\"], targets)\n",
    "                figsize = (4,4)\n",
    "\n",
    "                for i in range(len(imgs)):\n",
    "                    cnt -= 1\n",
    "                    # print(targets)\n",
    "\n",
    "                    scale = targets[\"img_scale\"][i] if \"img_scale\" in targets else 1.0\n",
    "                    print()\n",
    "\n",
    "                    pred_mask = pred_output[i][:, 4] > self.confidence_threshold\n",
    "                    if pred_mask.sum() == 0:\n",
    "                        pred_bbox = torch.zeros((0, 4), device=self.device)\n",
    "                        pred_scores = torch.zeros((0,), device=self.device)\n",
    "                    else:\n",
    "                        pred_bbox = pred_output[i][pred_mask, :4]\n",
    "                        pred_scores = pred_output[i][pred_mask, 4]\n",
    "\n",
    "                    mask = output[\"detections\"][i][:, 4] > self.confidence_threshold\n",
    "                    if mask.sum() == 0:\n",
    "                        bbox = torch.zeros((0, 4), device=self.device)\n",
    "                        scores = torch.zeros((0,), device=self.device)\n",
    "                        labels_idx = torch.zeros((0,), dtype=torch.long, device=self.device)\n",
    "                    else:\n",
    "                        bbox = output[\"detections\"][i][mask, :4] \n",
    "                        scores = output[\"detections\"][i][mask, 4]\n",
    "                        labels_idx = output[\"detections\"][i][mask, 5].long()\n",
    "\n",
    "                    gt_mask = targets[\"cls\"][i] > 0\n",
    "                    if gt_mask.sum() == 0:\n",
    "                        gt_bbox = torch.zeros((0, 4), device=self.device)\n",
    "                        gt_labels_idx = torch.zeros((0,), dtype=torch.long, device=self.device)\n",
    "                    else:\n",
    "                        gt_bbox_yxyx = targets[\"bbox\"][i][gt_mask] * scale\n",
    "                        gt_bbox_xyxy = torch.zeros_like(gt_bbox_yxyx)\n",
    "                        gt_bbox_xyxy[:, 0] = gt_bbox_yxyx[:, 1]\n",
    "                        gt_bbox_xyxy[:, 1] = gt_bbox_yxyx[:, 0]\n",
    "                        gt_bbox_xyxy[:, 2] = gt_bbox_yxyx[:, 3]\n",
    "                        gt_bbox_xyxy[:, 3] = gt_bbox_yxyx[:, 2]\n",
    "                        gt_bbox = gt_bbox_xyxy\n",
    "                        gt_labels_idx = targets[\"cls\"][i][gt_mask].long()\n",
    "\n",
    "                    if cnt >= 0:\n",
    "                        mask = output[\"detections\"][i][:, 4] > 0.3\n",
    "                        if mask.sum() == 0:\n",
    "                            continue\n",
    "                        \n",
    "                        ppp(imgs[i], gt_bbox/scale, coords=\"xyxy\", color=\"green\", figsize=figsize)\n",
    "                        ppp(imgs[i], bbox/scale, coords=\"xyxy\", color=\"cyan\", figsize=figsize)\n",
    "                        ppp(imgs[i], pred_bbox, coords=\"xyxy\", color=\"magenta\", figsize=figsize)\n",
    "\n",
    "                    metric.update(\n",
    "                        [\n",
    "                            {\n",
    "                                \"boxes\": bbox,\n",
    "                                \"scores\": scores,\n",
    "                                \"labels\": labels_idx,\n",
    "                            }\n",
    "                        ],\n",
    "                        [\n",
    "                            {\n",
    "                                \"boxes\": gt_bbox,\n",
    "                                \"labels\": gt_labels_idx,\n",
    "                            }\n",
    "                        ],\n",
    "                    )\n",
    "                raise ValueError(\"stop\")\n",
    "                    # h, w = imgs.shape[2:]\n",
    "                    # print(\"img size:\", (w, h))\n",
    "                    # print(\"bbox range:\", bbox.min().item() if bbox.numel() else 0, bbox.max().item() if bbox.numel() else 0)\n",
    "                    # print(\"gt range:\", gt_bbox.min().item() if gt_bbox.numel() else 0, gt_bbox.max().item() if gt_bbox.numel() else 0)\n",
    "\n",
    "                #     print(f\"bbox: {bbox}, scores: {scores}, labels: {labels_idx}, gt_bbox: {gt_bbox}, gt_labels: {gt_labels_idx}\")\n",
    "                # print(evaluator.evaluate())\n",
    "                # print(\"----\")\n",
    "                # print(\"Intermediate MAP:\", metric.compute())\n",
    "                # raise ValueError(\"stop\")\n",
    "\n",
    "        print(targets)\n",
    "        mresults = metric.compute()\n",
    "        print(\"Metric results:\", mresults)\n",
    "        print(\"Metric keys:\", mresults.keys())\n",
    "        results = evaluator.evaluate()\n",
    "        metrics = {\n",
    "            \"map_50\": results[1],\n",
    "            \"map_50_95\": results[0],\n",
    "        }\n",
    "        return metrics\n",
    "\n",
    "train_datasets = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"train\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_train_curated.json\",\n",
    "    )\n",
    "]\n",
    "val_datasets = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"val\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_val_curated.json\",\n",
    "    )\n",
    "]\n",
    "demo_datasets = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"val\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_demo_curated.json\",\n",
    "    )\n",
    "]\n",
    "emodel = EfficientDetAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    **{\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 6,\n",
    "        \"optimizer\": \"momentum\",\n",
    "        \"lr\": 0.003459928723120903,\n",
    "        \"weight_decay\": 0.0001302610542371722,\n",
    "    },\n",
    "    weights=\"/home/bachelor/ml-carbucks/results/debug/efficientdet/model.pth\"\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "emodel.setup()\n",
    "\n",
    "# --- TRAINING ---\n",
    "# emodel.fit(train_datasets) # type: ignore\n",
    "# save_path = emodel.save(RESULTS_DIR / \"debug\" / \"efficientdet\")\n",
    "\n",
    "# --- EVALUATION ---\n",
    "eres = emodel.evaluate(demo_datasets) # type: ignore\n",
    "# print(eres)\n",
    "\n",
    "\n",
    "\n",
    "# loader = create_clean_loader(val_datasets, shuffle=False, transforms=None, batch_size=8)  # type: ignore\n",
    "# evaluator = MeanAveragePrecision()\n",
    "# cnt2 = 5\n",
    "# for batch in loader:\n",
    "#     images, targets = batch\n",
    "#     outputs = emodel.predict(images)\n",
    "\n",
    "#     for i in range(len(images)):\n",
    "#         cnt2 -= 1\n",
    "#         if cnt2 >= 0:\n",
    "#             ppp(images[i], targets[i]['boxes'], coords=\"xyxy\", color=\"green\", figsize=(4,4))\n",
    "#             ppp(images[i], outputs[i]['boxes'], coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "#             # print(targets)\n",
    "#         else:\n",
    "#             break\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfaf7f72",
   "metadata": {},
   "source": [
    "# THIS was to verify how the images should be preprocessed for EfficientDet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf1c67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ppi1(images: List[torch.Tensor], img_size, config, hparams, device=\"cuda\"): \n",
    "    input_config = resolve_input_config(hparams, config) \n",
    "    fill_color = resolve_fill_color( input_config[\"fill_color\"], input_config[\"mean\"] ) \n",
    "    mean = torch.tensor([x * 255 for x in input_config['mean']]).to(device).view( 3, 1, 1) \n",
    "    std = torch.tensor([x * 255 for x in input_config['std']]).to(device).view( 3, 1, 1) \n",
    "    transform = Compose( [ ResizePad( target_size=img_size, interpolation=input_config[\"interpolation\"], fill_color=fill_color, ), ImageToNumpy(), ] ) \n",
    "    batch_list = [] \n",
    "    img_scaled = [] \n",
    "    for img in images: \n",
    "        img_pil = Image.fromarray(img.permute(1, 2, 0).cpu().numpy().astype(np.uint8)) \n",
    "        img_proc, anno = transform(img_pil, dict()) # no annotations \n",
    "        img_norm = torch.from_numpy(img_proc).float().to(device).sub_(mean).div_(std) \n",
    "        batch_list.append(img_norm) \n",
    "        img_scaled.append(anno.get(\"img_scale\", 1.0)) \n",
    "    batch_tensor = torch.stack(batch_list, dim=0).to(device) \n",
    "    img_scaled_tensor = torch.tensor(img_scaled, dtype=torch.float32).to(device) \n",
    "    \n",
    "    return batch_tensor, img_scaled_tensor\n",
    "\n",
    "def ppi2(images: List[torch.Tensor], img_size, config, hparams, device=\"cuda\"):\n",
    "        input_config = resolve_input_config(hparams,  config)\n",
    "        fill_color = resolve_fill_color(\n",
    "            input_config[\"fill_color\"], input_config[\"mean\"]\n",
    "        )\n",
    "\n",
    "        mean = (\n",
    "            torch.tensor(input_config[\"mean\"], device= device).view(3, 1, 1) * 255\n",
    "        )\n",
    "        std = torch.tensor(input_config[\"std\"], device= device).view(3, 1, 1) * 255\n",
    "\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ResizePad(\n",
    "                    target_size=img_size,\n",
    "                    interpolation=input_config[\"interpolation\"],\n",
    "                    fill_color=fill_color,\n",
    "                ),\n",
    "                ImageToNumpy(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        batch, scales = [], []\n",
    "\n",
    "        for img in images:\n",
    "\n",
    "            img_np, anno = transform(\n",
    "                Image.fromarray(img.permute(1, 2, 0).cpu().numpy().astype(np.uint8)), {}\n",
    "            )\n",
    "            scales.append(anno.get(\"img_scale\", 1.0))\n",
    "\n",
    "            img_norm = (\n",
    "                torch.from_numpy(img_np)\n",
    "                .to( device, non_blocking=True)\n",
    "                .float()\n",
    "                .sub_(mean)\n",
    "                .div_(std)\n",
    "            )\n",
    "            batch.append(img_norm)\n",
    "\n",
    "        batch_tensor = torch.stack(batch, dim=0)  # B,C,H,W\n",
    "        scales_tensor = torch.tensor(scales, dtype=torch.float32, device= device)\n",
    "\n",
    "        return batch_tensor, scales_tensor\n",
    "\n",
    "import time\n",
    "import torch.nn.functional as F\n",
    "from typing import Tuple\n",
    "\n",
    "# NOTE: This function is fast but does not inherently follow exactly the same logic as effdet's ResizePad\n",
    "# So it could be nice to use but in case of any further development it will need to be rewritten\n",
    "def ppi_fast_torch(images: List[torch.Tensor], img_size: Tuple[int,int], config, hparams, device=\"cuda\"):\n",
    "    \"\"\"\n",
    "    Fast preprocessing when `images` are torch.Tensor (C,H,W) or (H,W,C).\n",
    "    - images: list of torch.Tensor, expected [C,H,W] or [H,W,C] (will detect)\n",
    "    - img_size: (H_target, W_target) or int -> square\n",
    "    Returns: (batch_tensor [B,C,Ht,Wt], img_scales tensor [B])\n",
    "    \"\"\"\n",
    "    # ensure img_size is (H,W)\n",
    "    if isinstance(img_size, int):\n",
    "        target_h = target_w = img_size\n",
    "    else:\n",
    "        target_h, target_w = img_size\n",
    "\n",
    "    input_config = resolve_input_config(hparams, config)\n",
    "    mean = torch.tensor([x * 255 for x in input_config['mean']], dtype=torch.float32, device=device).view(3,1,1)\n",
    "    std  = torch.tensor([x * 255 for x in input_config['std']], dtype=torch.float32, device=device).view(3,1,1)\n",
    "\n",
    "    # Normalize and pad/resize in-batch\n",
    "    batch = []\n",
    "    scales = []\n",
    "    for img in images:\n",
    "        # If image is H,W,C -> convert to C,H,W\n",
    "        if img.ndim == 3 and img.shape[0] not in (1,3):\n",
    "            img = img.permute(2,0,1)\n",
    "        # Convert ints to float tensor in [0,255] if needed\n",
    "        if img.dtype != torch.float32:\n",
    "            img = img.to(dtype=torch.float32)\n",
    "        # compute scale to fit while preserving aspect ratio (ResizePad behaviour)\n",
    "        _, h, w = img.shape\n",
    "        scale = min(target_w / w, target_h / h)\n",
    "        new_w, new_h = int(w * scale), int(h * scale)\n",
    "        # resize using interpolate: needs NCHW, so unsqueeze\n",
    "        img_resized = F.interpolate(img.unsqueeze(0), size=(new_h, new_w), mode='bilinear', align_corners=False)[0]\n",
    "        # pad to target size: anchor at top-left (no centering)\n",
    "        pad_h = target_h - new_h\n",
    "        pad_w = target_w - new_w\n",
    "        # Top-left anchor: pad on right and bottom only\n",
    "        pad_left = 0\n",
    "        pad_right = pad_w\n",
    "        pad_top = 0\n",
    "        pad_bottom = pad_h\n",
    "\n",
    "        # Fill/pad handling: ResizePad in effdet may accept RGB tuples or strings\n",
    "        fill_val = input_config.get('fill_color', 0)\n",
    "        # Normalize fill_val to either a scalar or a 3-tuple of ints\n",
    "        if isinstance(fill_val, str):\n",
    "            fv = fill_val.lstrip('#')\n",
    "            if len(fv) == 6:\n",
    "                try:\n",
    "                    r = int(fv[0:2], 16)\n",
    "                    g = int(fv[2:4], 16)\n",
    "                    b = int(fv[4:6], 16)\n",
    "                    fill_tuple = (r, g, b)\n",
    "                except Exception:\n",
    "                    fill_tuple = (0, 0, 0)\n",
    "            else:\n",
    "                # try comma-separated numbers\n",
    "                try:\n",
    "                    parts = [int(x) for x in fill_val.replace('(', '').replace(')', '').split(',')]\n",
    "                    fill_tuple = tuple(parts)\n",
    "                except Exception:\n",
    "                    fill_tuple = (0, 0, 0)\n",
    "        elif isinstance(fill_val, (list, tuple)):\n",
    "            fill_tuple = tuple(fill_val)\n",
    "        else:\n",
    "            fill_tuple = float(fill_val)\n",
    "\n",
    "        # If fill_tuple is per-channel, create a canvas and paste the resized image into top-left corner\n",
    "        if isinstance(fill_tuple, tuple) and len(fill_tuple) >= img_resized.shape[0]:\n",
    "            # create canvas with same dtype/device as img_resized\n",
    "            canvas = img_resized.new_full((img_resized.shape[0], target_h, target_w), 0)\n",
    "            for c in range(img_resized.shape[0]):\n",
    "                canvas[c].fill_(float(fill_tuple[c]))\n",
    "            # place at top-left\n",
    "            canvas[:, 0 : new_h, 0 : new_w] = img_resized\n",
    "            img_padded = canvas\n",
    "        else:\n",
    "            # scalar fill: use F.pad (top-left anchor achieved by padding right/bottom only)\n",
    "            scalar_fill = float(fill_tuple)\n",
    "            img_padded = F.pad(\n",
    "                img_resized, (pad_left, pad_right, pad_top, pad_bottom), mode='constant', value=scalar_fill\n",
    "            )\n",
    "\n",
    "        batch.append(img_padded)\n",
    "        scales.append(scale)\n",
    "\n",
    "    batch_tensor = torch.stack(batch, dim=0).to(device, non_blocking=True)  # B,C,H,W on device\n",
    "    # normalize in-place\n",
    "    batch_tensor = batch_tensor.sub_(mean).div_(std)\n",
    "\n",
    "    img_scales = torch.tensor(scales, dtype=torch.float32, device=device)\n",
    "    return batch_tensor, 1/img_scales\n",
    "\n",
    "\n",
    "eloader = create_clean_loader(demo_datasets, shuffle=False, transforms=None, batch_size=2)  # type: ignore\n",
    "images, targets = next(iter(eloader))\n",
    "\n",
    "s = time.time()\n",
    "batch_images0, batch_scales0 = ppi1(images, emodel.img_size, emodel.model.config, emodel.get_params())\n",
    "e = time.time()\n",
    "print(\"PPI1 time:\", e - s)\n",
    "s = time.time()\n",
    "batch_images, batch_scales = ppi2(images, emodel.img_size, emodel.model.config, emodel.get_params())\n",
    "e = time.time()\n",
    "print(\"PPI2 time:\", e - s)\n",
    "\n",
    "s = time.time()\n",
    "batch_images_fast, batch_scales_fast = ppi_fast_torch(images, (emodel.img_size,emodel.img_size), emodel.model.config, emodel.get_params())\n",
    "e = time.time()\n",
    "print(\"PPI Fast time:\", e - s)\n",
    "print(batch_images_fast.shape, batch_scales_fast)\n",
    "iii = 0\n",
    "ppp(images[iii], targets[iii]['boxes'], coords=\"xyxy\", color=\"green\", figsize=(4,4))\n",
    "ppp(batch_images[iii], targets[iii]['boxes']/batch_scales[iii].cpu(), coords=\"xyxy\", color=\"green\", figsize=(4,4))\n",
    "ppp(batch_images_fast[iii], targets[iii]['boxes']/batch_scales_fast[iii].cpu(), coords=\"xyxy\", color=\"green\", figsize=(4,4))\n",
    "\n",
    "\"\"\"\n",
    "PPI2 time: 0.0068073272705078125\n",
    "PPI Fast time: 0.0015950202941894531\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f107b8f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter as EfficientDetAdapterOrg\n",
    "from ml_carbucks.utils.preprocessing import create_clean_loader  # noqa: F401\n",
    "from ml_carbucks import DATA_DIR\n",
    "from ml_carbucks.utils.inference import plot_img_pred as ppp  # noqa: F401\n",
    "\n",
    "demo_datasets = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"val\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_demo_curated.json\",\n",
    "    )\n",
    "]\n",
    "eloader = create_clean_loader(demo_datasets, shuffle=False, transforms=None, batch_size=2)  # type: ignore\n",
    "images, targets = next(iter(eloader))\n",
    "\n",
    "eee = EfficientDetAdapterOrg(\n",
    "classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    **{\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 6,\n",
    "        \"optimizer\": \"momentum\",\n",
    "        \"lr\": 0.003459928723120903,\n",
    "        \"weight_decay\": 0.0001302610542371722,\n",
    "    },\n",
    "    weights=\"/home/bachelor/ml-carbucks/results/debug/efficientdet/model.pth\"\n",
    ")\n",
    "eee.setup()\n",
    "\n",
    "# model_train = deepcopy(eee.model).eval().to(\"cuda\")\n",
    "# model_predict = DetBenchPredict(deepcopy(eee.model.model)).eval().to(\"cuda\")\n",
    "\n",
    "\n",
    "preds = eee.predict(images, conf_threshold=0.2, iou_threshold=0.4)\n",
    "\n",
    "ppp(images[0], preds[0]['boxes'], coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "\n",
    "# img_batch, img_scales = ppi2(images, eee.img_size, eee.model.config, eee.get_params())\n",
    "\n",
    "\n",
    "# img_sizes = torch.tensor(\n",
    "#     [[img.shape[1], img.shape[2]] for img in images], dtype=torch.float32\n",
    "# ).to(\"cuda\")\n",
    "\n",
    "# img_info_dict = {\n",
    "#     \"img_scale\": img_scales,\n",
    "#     \"img_size\": img_sizes,\n",
    "# }\n",
    "# print(eee.img_size)\n",
    "# print(img_info_dict)\n",
    "# print(targets)\n",
    "# targets_empty = {\n",
    "#     **{\n",
    "#         \"img_size\": img_sizes,\n",
    "#         \"img_scale\": img_scales,\n",
    "#     },\n",
    "#     \"bbox\": torch.zeros((0, 4), dtype=torch.float32).to(\"cuda\"),\n",
    "#     \"cls\": torch.zeros((0,), dtype=torch.int64).to(\"cuda\"),\n",
    "# }\n",
    "# with torch.no_grad():\n",
    "#     out_train = model_train(img_batch, targets)  # DetBenchTrain; produces output[\"detections\"]\n",
    "#     # out_pred = model_predict(img_batch, img_info=img_info_dict)  # DetBenchPredict -> list[tensor]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54977cf3",
   "metadata": {},
   "source": [
    "# THIS HERE IS THE FIXXING OF PREDICTION DATA LOADING AND PRE AND POST PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c68919",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO __main__ 21:13:04 | THIS IS PRINTING HOW THE IMAGE AND LABELS LOOK DURING EVALUAITON\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO __main__ 21:13:10 | Post-processed Prediction Evaluation Results: {'map': tensor(0.1673), 'map_50': tensor(0.3597), 'map_75': tensor(0.1367), 'map_small': tensor(0.), 'map_medium': tensor(0.0194), 'map_large': tensor(0.2006), 'mar_1': tensor(0.1773), 'mar_10': tensor(0.2747), 'mar_100': tensor(0.2775), 'mar_small': tensor(0.), 'mar_medium': tensor(0.0547), 'mar_large': tensor(0.3492), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([1, 2, 3], dtype=torch.int32)}\n",
      "Post-processed Prediction Evaluation Results: {'map': tensor(0.1673), 'map_50': tensor(0.3597), 'map_75': tensor(0.1367), 'map_small': tensor(0.), 'map_medium': tensor(0.0194), 'map_large': tensor(0.2006), 'mar_1': tensor(0.1773), 'mar_10': tensor(0.2747), 'mar_100': tensor(0.2775), 'mar_small': tensor(0.), 'mar_medium': tensor(0.0547), 'mar_large': tensor(0.3492), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([1, 2, 3], dtype=torch.int32)}\n",
      "{'map_50': 0.3597193956375122, 'map_50_95': 0.16730839014053345}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchvision.ops import nms\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from effdet import create_model, create_loader\n",
    "from effdet.data import resolve_input_config, resolve_fill_color\n",
    "from effdet.bench import DetBenchPredict  # noqa F401\n",
    "from effdet.data.transforms import ResizePad, ImageToNumpy, Compose\n",
    "from timm.optim._optim_factory import create_optimizer_v2\n",
    "\n",
    "from ml_carbucks.utils.postprocessing import postprocess_prediction\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import (\n",
    "    BaseDetectionAdapter,\n",
    "    ADAPTER_PREDICTION,\n",
    ")\n",
    "from ml_carbucks.patches.effdet import (\n",
    "    CocoStatsEvaluator,\n",
    "    ConcatDetectionDataset,\n",
    "    create_dataset_custom,\n",
    ")\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "from ml_carbucks import DATA_DIR\n",
    "from ml_carbucks.utils.preprocessing import create_clean_loader, simple_transform  # noqa: F401\n",
    "from ml_carbucks.utils.inference import plot_img_pred as ppp  # noqa: F401\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EfficientDetAdapter(BaseDetectionAdapter):\n",
    "\n",
    "    weights: str | Path = \"\"\n",
    "    backbone: str = \"tf_efficientdet_d0\"\n",
    "\n",
    "    optimizer: str = \"momentum\"\n",
    "    lr: float = 8e-3\n",
    "    weight_decay: float = 9e-6\n",
    "    confidence_threshold: float = 0.15\n",
    "    training_augmentations: bool = True\n",
    "\n",
    "    def save(self, dir: Path | str, prefix: str = \"\", suffix: str = \"\") -> Path:\n",
    "        save_path = Path(dir) / f\"{prefix}model{suffix}.pth\"\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.model.model.state_dict(), save_path)\n",
    "        return save_path\n",
    "\n",
    "    def _preprocess_images(\n",
    "        self, images: List[np.ndarray]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        fill_color = resolve_fill_color(\n",
    "            input_config[\"fill_color\"], input_config[\"mean\"]\n",
    "        )\n",
    "\n",
    "        mean = (\n",
    "            torch.tensor(input_config[\"mean\"], device=self.device).view(3, 1, 1) * 255\n",
    "        )\n",
    "        std = torch.tensor(input_config[\"std\"], device=self.device).view(3, 1, 1) * 255\n",
    "\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ResizePad(\n",
    "                    target_size=self.img_size,\n",
    "                    interpolation=input_config[\"interpolation\"],\n",
    "                    fill_color=fill_color,\n",
    "                ),\n",
    "                ImageToNumpy(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        batch, scales = [], []\n",
    "\n",
    "        for img in images:\n",
    "\n",
    "            img_np, anno = transform(\n",
    "                Image.fromarray(img), {}\n",
    "            )\n",
    "            scales.append(anno.get(\"img_scale\", 1.0))\n",
    "\n",
    "            img_norm = (\n",
    "                torch.from_numpy(img_np)\n",
    "                .to(self.device, non_blocking=True)\n",
    "                .float()\n",
    "                .sub_(mean)\n",
    "                .div_(std)\n",
    "            )\n",
    "            batch.append(img_norm)\n",
    "\n",
    "        print(f\"PREPROCESSING: image shape: {images[0].shape} -> batch shape: {batch[0].shape}\")\n",
    "        batch_tensor = torch.stack(batch, dim=0)  # B,C,H,W\n",
    "        scales_tensor = torch.tensor(scales, dtype=torch.float32, device=self.device)\n",
    "        batch_original_sizes = torch.tensor(\n",
    "            [[img.shape[1], img.shape[0]] for img in images], \n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        return batch_tensor, scales_tensor, batch_original_sizes\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        images: List[np.ndarray],\n",
    "        conf_threshold: float = 0.15,\n",
    "        iou_threshold: float = 0.4,\n",
    "        max_detections: int = 10,\n",
    "    ) -> List[ADAPTER_PREDICTION]:\n",
    "        \"\"\"\n",
    "        The issue is that predicitons are weird but the results of the evaluation\n",
    "        are good. So either the evaluation is wrong or the prediction extraction is wrong.\n",
    "        \"\"\"\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()\n",
    "        predictions: List[ADAPTER_PREDICTION] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_tensor, batch_scales, batch_original_sizes = self._preprocess_images(\n",
    "                images\n",
    "            )\n",
    "\n",
    "            img_info_dict = {\n",
    "                \"img_scale\": batch_scales,\n",
    "                \"img_size\": batch_original_sizes,\n",
    "            }\n",
    "\n",
    "            # outputs = predictor(batch_tensor, img_info=img_info_dict)\n",
    "            outputs = predictor(batch_tensor, img_info=img_info_dict)\n",
    "\n",
    "            for i, pred in enumerate(outputs):\n",
    "                pred_mask = pred[:, 4] >= conf_threshold\n",
    "                scale = batch_scales[i]\n",
    "                # ppp(batch_tensor[i], pred[pred_mask, :4] / scale, coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "                # return []\n",
    "\n",
    "                # scale = batch_scales[i]\n",
    "                # print(f\"Prediction {i} - scale: {scale}\")\n",
    "                # print(batch_tensor[i])\n",
    "                # pred_box = pred[:10, :4] / batch_scales[i]\n",
    "                # pred_box = pred[:10, :4] / scale\n",
    "\n",
    "                # print(pred)\n",
    "                # ppp(batch_tensor[i], pred_box, coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "\n",
    "                boxes = pred[pred_mask, :4]\n",
    "                scores = pred[pred_mask, 4]\n",
    "                labels_idx = pred[pred_mask, 5]\n",
    "\n",
    "                # prediction = postprocess_prediction(\n",
    "                #     boxes,\n",
    "                #     scores,\n",
    "                #     labels_idx,\n",
    "                #     conf_threshold,\n",
    "                #     iou_threshold,\n",
    "                #     max_detections,\n",
    "                # )\n",
    "\n",
    "                prediction: ADAPTER_PREDICTION = {\n",
    "                    \"boxes\": boxes.cpu(),\n",
    "                    \"scores\": scores.cpu(),\n",
    "                    \"labels\": labels_idx.cpu().long(),\n",
    "                }\n",
    "\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions        \n",
    "\n",
    "        prev_func = \"\"\"\n",
    "        # cnt = 1\n",
    "        # with torch.no_grad():\n",
    "\n",
    "        #     batch_tensor, batch_scales, batch_original_sizes = self._preprocess_images(\n",
    "        #         images\n",
    "        #     )\n",
    "\n",
    "        #     print(\"Batch scales:\", batch_scales)\n",
    "        #     print(\"Batch original sizes:\", batch_original_sizes)\n",
    "        #     img_info_dict = {\n",
    "        #         \"img_scale\": batch_scales,\n",
    "        #         \"img_size\": batch_original_sizes,\n",
    "        #     }\n",
    "        #     outputs = predictor(batch_tensor, img_info=img_info_dict)\n",
    "\n",
    "        #     # outputs = predictor(batch_tensor)\n",
    "\n",
    "        #     for i, pred in enumerate(outputs):\n",
    "        #         scale = batch_scales[i]\n",
    "        #         print(f\"Prediction {i} - scale: {scale}\")\n",
    "        #         # print(batch_tensor[i])\n",
    "        #         # pred_box = pred[:10, :4] / batch_scales[i]\n",
    "        #         pred_box = pred[:10, :4] / scale\n",
    "\n",
    "        #         # print(pred)\n",
    "        #         ppp(batch_tensor[i], pred_box, coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "        #         cnt -= 1\n",
    "        #         if cnt <= 0:\n",
    "        #             break\n",
    "\n",
    "            # for i, pred in enumerate(outputs):\n",
    "            #     mask = pred[:, 4] >= conf_threshold\n",
    "            #     if mask.sum() == 0:\n",
    "            #         boxes = np.zeros((0, 4), dtype=np.float32)\n",
    "            #         scores = np.zeros((0,), dtype=np.float32)\n",
    "            #         labels = []\n",
    "            #     else:\n",
    "            #         boxes = pred[mask, :4]\n",
    "            #         scores = pred[mask, 4]\n",
    "            #         labels_idx = pred[mask, 5].long()\n",
    "\n",
    "            #         # apply NMS per image\n",
    "            #         keep = nms(boxes, scores, iou_threshold)\n",
    "            #         keep = keep[:max_detections]  # take top-k\n",
    "\n",
    "            #         boxes = boxes[keep].cpu().numpy().copy()\n",
    "\n",
    "            #         scores = scores[keep].cpu().numpy()\n",
    "            #         labels = [\n",
    "            #             idx for idx in labels_idx[keep]\n",
    "            #         ]  # Predictions are 1-indexed\n",
    "            #     predictions.append({\"boxes\": boxes, \"scores\": scores, \"labels\": labels})\n",
    "\n",
    "        # return predictions\n",
    "        \"\"\"\n",
    "\n",
    "    def setup(self) -> \"EfficientDetAdapter\":\n",
    "        img_size = self.img_size\n",
    "\n",
    "        backbone = self.backbone\n",
    "        weights = self.weights\n",
    "\n",
    "        extra_args = dict(image_size=(img_size, img_size))\n",
    "        self.model = create_model(\n",
    "            model_name=backbone,\n",
    "            bench_task=\"train\",\n",
    "            num_classes=len(self.classes),\n",
    "            pretrained=weights == \"\",\n",
    "            checkpoint_path=str(weights),\n",
    "            # NOTE: we set it to True because we are using custom Mean Average Precision and it is easier that way\n",
    "            # custom anchor labeler would be good idea if the boxes had unusual sizes and aspect ratios -> worth remembering for future\n",
    "            bench_labeler=True,\n",
    "            checkpoint_ema=False,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> \"EfficientDetAdapter\":\n",
    "        logger.info(\"Starting training...\")\n",
    "        self.model.train()\n",
    "\n",
    "        epochs = self.epochs\n",
    "        opt = self.optimizer\n",
    "        lr = self.lr\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        train_loader = self._create_loader(datasets, is_training=True)\n",
    "\n",
    "        parser_max_label = train_loader.dataset.parsers[0].max_label  # type: ignore\n",
    "        config_num_classes = self.model.config.num_classes\n",
    "\n",
    "        if parser_max_label != config_num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Number of classes in dataset ({parser_max_label}) does not match \"\n",
    "                f\"model config ({config_num_classes}).\"\n",
    "                f\"Please verify that the dataset is curated (classes IDs start from 1)\"\n",
    "            )\n",
    "\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=opt,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            _ = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _create_loader(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]], is_training: bool\n",
    "    ):\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        all_datasets = []\n",
    "        for img_dir, ann_file in datasets:\n",
    "            dataset = create_dataset_custom(\n",
    "                img_dir=img_dir,\n",
    "                ann_file=ann_file,\n",
    "                has_labels=True,\n",
    "            )\n",
    "            all_datasets.append(dataset)\n",
    "\n",
    "        concat_dataset = ConcatDetectionDataset(all_datasets)\n",
    "\n",
    "        if is_training and (not self.training_augmentations):\n",
    "            logger.warning(\n",
    "                \"Data augmentations are disabled. This may worsen model performance. It should only be used for debugging purposes.\"\n",
    "            )\n",
    "\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        loader = create_loader(\n",
    "            concat_dataset,\n",
    "            input_size=input_config[\"input_size\"],\n",
    "            batch_size=batch_size,\n",
    "            is_training=is_training and self.training_augmentations,\n",
    "            use_prefetcher=True,\n",
    "            interpolation=input_config[\"interpolation\"],\n",
    "            fill_color=input_config[\"fill_color\"],\n",
    "            mean=input_config[\"mean\"],\n",
    "            std=input_config[\"std\"],\n",
    "            num_workers=4,\n",
    "            distributed=False,\n",
    "            pin_mem=False,\n",
    "            anchor_labeler=None,\n",
    "            transform_fn=None,\n",
    "            collate_fn=None,\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def train_epoch(\n",
    "        self, optimizer: torch.optim.Optimizer, loader: DataLoader\n",
    "    ) -> float:\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            output = self.model(imgs, targets)\n",
    "            loss = output[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def debug(\n",
    "        self,\n",
    "        train_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        val_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        results_path: str | Path,\n",
    "        results_name: str,\n",
    "    ) -> Dict[str, float]:\n",
    "        logger.info(\"Debugging training and evaluation loops...\")\n",
    "\n",
    "        epochs = self.epochs\n",
    "        train_loader = self._create_loader(train_datasets, is_training=True)\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=self.optimizer,\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        saver = ResultSaver(\n",
    "            path=results_path,\n",
    "            name=results_name,\n",
    "        )\n",
    "        val_metrics = dict()\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            total_loss = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "            val_metrics = self.evaluate(val_datasets)\n",
    "            saver.save(\n",
    "                epoch=epoch,\n",
    "                loss=total_loss,\n",
    "                val_map=val_metrics[\"map_50_95\"],\n",
    "                val_map_50=val_metrics[\"map_50\"],\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Debug Epoch {epoch}/{epochs} - Loss: {total_loss}, Val MAP: {val_metrics['map_50_95']}\"\n",
    "            )\n",
    "            saver.plot(show=False)\n",
    "\n",
    "        return val_metrics\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        datasets: List[Tuple[str | Path, str | Path]],\n",
    "        include_default: bool = False,\n",
    "    ) -> Dict[str, float]:\n",
    "        self.model.eval()\n",
    "\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()        \n",
    "\n",
    "        val_loader = self._create_loader(datasets, is_training=False)\n",
    "\n",
    "        default_evaluator = None\n",
    "        if include_default:\n",
    "            default_evaluator = CocoStatsEvaluator(dataset=val_loader.dataset)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(extended_summary=False, class_metrics=False)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        pred_evaluator = MeanAveragePrecision(extended_summary=False, class_metrics=False)\n",
    "        cnt = 1\n",
    "        skip_cnt = 1\n",
    "        skip = False\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            for imgs, targets in val_loader:\n",
    "                # if skip_cnt >0:\n",
    "                #     skip_cnt -=1\n",
    "                #     continue\n",
    "                output = self.model(imgs, targets)\n",
    "                loss = output[\"loss\"]\n",
    "                total_loss += loss.item()\n",
    "                pred_output = predictor(imgs, targets)\n",
    "\n",
    "                # kkk = 0\n",
    "                # predictor_pred_mask = (\n",
    "                #     pred_output[kkk][:, 4] >= self.confidence_threshold\n",
    "                # )\n",
    "                # ppp(imgs[kkk], targets['bbox'][kkk], coords=\"yxyx\", color=\"green\", figsize=(4,4))\n",
    "                # ppp(imgs[kkk], pred_output[kkk][predictor_pred_mask, :4] / targets[\"img_scale\"][kkk], coords=\"xyxy\", color=\"magenta\", figsize=(4,4))\n",
    "                # # ppp(imgs[0], output[\"detections\"][0][:2,:4] / targets[\"img_scale\"][0], coords=\"yxyx\", color=\"green\", figsize=(4,4))\n",
    "                # return {}\n",
    "                # for i in range(len(output[\"detections\"])):\n",
    "                #     scale = targets[\"img_scale\"][i]\n",
    "                #     if cnt >0:\n",
    "                #         cnt -=1\n",
    "                #         print(imgs[i].shape)\n",
    "                #         gt_boxes_yxyx = targets['bbox'][i][:1]\n",
    "                #         gt_boxes_xyxy = torch.zeros_like(gt_boxes_yxyx)\n",
    "                #         gt_boxes_xyxy[:,0] = gt_boxes_yxyx[:,1]\n",
    "                #         gt_boxes_xyxy[:,1] = gt_boxes_yxyx[:,0]\n",
    "                #         gt_boxes_xyxy[:,2] = gt_boxes_yxyx[:,3]\n",
    "                #         gt_boxes_xyxy[:,3] = gt_boxes_yxyx[:,2]\n",
    "                #         gt_boxes_xyxy = gt_boxes_xyxy\n",
    "\n",
    "                #         # print(imgs[i])\n",
    "                #         ppp(imgs[i], gt_boxes_xyxy, coords=\"xyxy\", color=\"green\", figsize=(4,4))\n",
    "\n",
    "                #         demo_boxes_xyxy = output[\"detections\"][i][:10,:4] / scale\n",
    "\n",
    "                #         ppp(imgs[i], demo_boxes_xyxy, coords=\"xyxy\", color=\"red\", figsize=(4,4))\n",
    "\n",
    "                #         pred_boxes_xyxy = pred_output[i][:10,:4] / scale\n",
    "                #         ppp(imgs[i], pred_boxes_xyxy, coords=\"xyxy\", color=\"magenta\", figsize=(4,4))\n",
    "                #     else: \n",
    "                #         skip = True\n",
    "                \n",
    "                # if skip:\n",
    "                #     break\n",
    "\n",
    "                if include_default and default_evaluator is not None:\n",
    "                    default_evaluator.add_predictions(\n",
    "                        detections=output[\"detections\"], target=targets\n",
    "                    )\n",
    "\n",
    "                # NOTE:\n",
    "                # Annotations are loaded in yxyx format and they are scaled\n",
    "                # Predicitons are in xyxy format and not scaled (original size of the image)\n",
    "                # So we need to rescale the ground truth boxes to original sizes\n",
    "                # Predicitons have a lot of low confidence scores and ground_truths have a lot of -1 values that just indicate no object\n",
    "                # We need to filter them out\n",
    "                for i in range(len(imgs)):\n",
    "                    scale = targets[\"img_scale\"][i]\n",
    "\n",
    "                    predictor_pred_mask = (\n",
    "                        pred_output[i][:, 4] >= self.confidence_threshold\n",
    "                    )\n",
    "                    if predictor_pred_mask.sum() == 0:\n",
    "                        ppm_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        ppm_scores = torch.zeros((0,), dtype=torch.float32)\n",
    "                        ppm_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        ppm_boxes = pred_output[i][predictor_pred_mask, :4]\n",
    "                        ppm_scores = pred_output[i][predictor_pred_mask, 4]\n",
    "                        ppm_labels = pred_output[i][predictor_pred_mask, 5].long()\n",
    "\n",
    "\n",
    "                    pred_mask = (\n",
    "                        output[\"detections\"][i][:, 4] >= self.confidence_threshold\n",
    "                    )\n",
    "                    if pred_mask.sum() == 0:\n",
    "                        # No predcitions above the confidence threshold\n",
    "                        pred_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        pred_scores = torch.zeros((0,), dtype=torch.float32)\n",
    "                        pred_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        pred_boxes = output[\"detections\"][i][pred_mask, :4]\n",
    "                        pred_scores = output[\"detections\"][i][pred_mask, 4]\n",
    "                        pred_labels = output[\"detections\"][i][pred_mask, 5].long()\n",
    "\n",
    "                    gt_mask = targets[\"cls\"][i] != -1\n",
    "                    if gt_mask.sum() == 0:\n",
    "                        # No ground truth boxes\n",
    "                        gt_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        gt_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        gt_boxes_yxyx_raw = targets[\"bbox\"][i][gt_mask]\n",
    "                        gt_boxes_xyxy = torch.zeros_like(gt_boxes_yxyx_raw)\n",
    "                        gt_boxes_xyxy[:, 0] = gt_boxes_yxyx_raw[:, 1]\n",
    "                        gt_boxes_xyxy[:, 1] = gt_boxes_yxyx_raw[:, 0]\n",
    "                        gt_boxes_xyxy[:, 2] = gt_boxes_yxyx_raw[:, 3]\n",
    "                        gt_boxes_xyxy[:, 3] = gt_boxes_yxyx_raw[:, 2]\n",
    "                        gt_boxes = gt_boxes_xyxy * scale\n",
    "                        gt_labels = targets[\"cls\"][i][gt_mask].long()\n",
    "\n",
    "                    evaluator.update(\n",
    "                        preds=[\n",
    "                            {\n",
    "                                \"boxes\": pred_boxes.cpu(),\n",
    "                                \"scores\": pred_scores.cpu(),\n",
    "                                \"labels\": pred_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                        target=[\n",
    "                            {\n",
    "                                \"boxes\": gt_boxes.cpu(),\n",
    "                                \"labels\": gt_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "                    pred_evaluator.update(\n",
    "                        preds=[\n",
    "                            {\n",
    "                                \"boxes\": ppm_boxes.cpu(),\n",
    "                                \"scores\": ppm_scores.cpu(),\n",
    "                                \"labels\": ppm_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                        target=[\n",
    "                            {\n",
    "                                \"boxes\": gt_boxes.cpu(),\n",
    "                                \"labels\": gt_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                    )                    \n",
    "\n",
    "        ppi_results = pred_evaluator.compute()\n",
    "        logger.info(f\"Post-processed Prediction Evaluation Results: {ppi_results}\")\n",
    "        print(f\"Post-processed Prediction Evaluation Results: {ppi_results}\")\n",
    "\n",
    "        results = evaluator.compute()\n",
    "        metrics = {\n",
    "            \"map_50\": results[\"map_50\"].item(),\n",
    "            \"map_50_95\": results[\"map\"].item(),\n",
    "        }\n",
    "        if include_default and default_evaluator is not None:\n",
    "            default_results = default_evaluator.evaluate()\n",
    "            metrics.update(\n",
    "                {\n",
    "                    \"default_map_50_95\": default_results[0],\n",
    "                    \"default_map_50\": default_results[1],\n",
    "                }\n",
    "            )\n",
    "\n",
    "        return metrics\n",
    "\n",
    "\n",
    "val_datasets : List[Tuple[str|Path, str|Path]] = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"val\",\n",
    "        # DATA_DIR / \"car_dd_testing\" / \"instances_val.json\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_val_curated.json\",\n",
    "    )\n",
    "]\n",
    "loader = create_clean_loader(\n",
    "    val_datasets, shuffle=False, transforms=None, batch_size=8 # type: ignore\n",
    ")\n",
    "\n",
    "adapter = EfficientDetAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    **{\n",
    "        \"img_size\": 384,\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 26,\n",
    "        \"optimizer\": \"momentum\",\n",
    "        \"lr\": 0.003459928723120903,\n",
    "        \"weight_decay\": 0.0001302610542371722,\n",
    "    },\n",
    "    weights=\"/home/bachelor/ml-carbucks/results/ensemble_demos/trial_4_EfficientDetAdaptermodel.pth\",\n",
    ").setup()\n",
    "\n",
    "\n",
    "logger.info(\"THIS IS PRINTING HOW THE IMAGE AND LABELS LOOK DURING EVALUAITON\")\n",
    "res = adapter.evaluate(val_datasets)\n",
    "print(res)\n",
    "\n",
    "\n",
    "def adapter_predict_demo():    \n",
    "    skip_cnt = 1\n",
    "    for custom_batch in loader:\n",
    "        if skip_cnt >0:\n",
    "            skip_cnt -=1\n",
    "            continue\n",
    "        else:\n",
    "            break\n",
    "    batch_img_nparray, batch_targets = custom_batch # type: ignore\n",
    "    pres = adapter.predict(batch_img_nparray)\n",
    "    print(pres)\n",
    "    print(batch_targets)\n",
    "\n",
    "# adapter_predict_demo()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68eb2529",
   "metadata": {},
   "source": [
    "# THIS was a verification that metrics from predictions are correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c72c00ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (664, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (666, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (563, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 668, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (668, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (654, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 750, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (664, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (669, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (668, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (563, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 563, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (673, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (663, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 750, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (563, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (702, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (748, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (685, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (666, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (659, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (668, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (484, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (787, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (683, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (751, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (666, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (563, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (1000, 667, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (672, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (750, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (662, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "PREPROCESSING: image shape: (667, 1000, 3) -> batch shape: torch.Size([3, 384, 384])\n",
      "{'map': tensor(0.1673), 'map_50': tensor(0.3597), 'map_75': tensor(0.1367), 'map_small': tensor(0.), 'map_medium': tensor(0.0196), 'map_large': tensor(0.2007), 'mar_1': tensor(0.1774), 'mar_10': tensor(0.2749), 'mar_100': tensor(0.2777), 'mar_small': tensor(0.), 'mar_medium': tensor(0.0551), 'mar_large': tensor(0.3496), 'map_per_class': tensor(-1.), 'mar_100_per_class': tensor(-1.), 'classes': tensor([1, 2, 3], dtype=torch.int32)}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# skip_cnt = 1\n",
    "# for custom_batch in loader:\n",
    "#     if skip_cnt >0:\n",
    "#         skip_cnt -=1\n",
    "#         continue\n",
    "#     else:\n",
    "#         break\n",
    "# batch_img_nparray, batch_targets = custom_batch # type: ignore\n",
    "# print(batch_img_nparray[0].shape)\n",
    "# logger.info(\"THIS IS PRINTING HOW THE IMAGE AND LABELS LOOK DURING PREDICTION\")\n",
    "# pred = adapter.predict(batch_img_nparray)\n",
    "\n",
    "def display_batch_pred(pred, batch_img_nparray, batch_targets):\n",
    "    for i in range(len(pred)):\n",
    "        ppp(\n",
    "            torch.from_numpy(batch_img_nparray[i]).permute(2, 0, 1),\n",
    "            bboxes=batch_targets[i][\"boxes\"],\n",
    "            coords=\"xyxy\",\n",
    "            color=\"green\",\n",
    "            figsize=(4, 4),\n",
    "        )\n",
    "        ppp(\n",
    "            torch.from_numpy(batch_img_nparray[i]).permute(2, 0, 1),\n",
    "            bboxes=pred[i][\"boxes\"],\n",
    "            coords=\"xyxy\",\n",
    "            color=\"red\",\n",
    "            figsize=(4, 4),\n",
    "        )\n",
    "\n",
    "# display_batch_pred(pred, batch_img_nparray, batch_targets)\n",
    "\n",
    "def run_prediction_evaluation():\n",
    "    evaluator = MeanAveragePrecision()\n",
    "    for custom_batch in loader:\n",
    "        batch_img_nparray, batch_targets = custom_batch  # type: ignore\n",
    "        pred = adapter.predict(batch_img_nparray)\n",
    "\n",
    "        evaluator.update(\n",
    "            preds=[\n",
    "                {\n",
    "                    \"boxes\": pred[i][\"boxes\"].cpu(),\n",
    "                    \"scores\": pred[i][\"scores\"].cpu(),\n",
    "                    \"labels\": pred[i][\"labels\"].cpu().long(),\n",
    "                }\n",
    "                for i in range(len(pred))\n",
    "            ],\n",
    "            target=[\n",
    "                {\n",
    "                    \"boxes\": batch_targets[i][\"boxes\"].cpu(),\n",
    "                    \"labels\": batch_targets[i][\"labels\"].cpu().long(),\n",
    "                }\n",
    "                for i in range(len(batch_targets))\n",
    "            ],\n",
    "        )\n",
    "\n",
    "    results = evaluator.compute()\n",
    "    \n",
    "    return results\n",
    "\n",
    "res = run_prediction_evaluation()\n",
    "print(res)\n",
    "\n",
    "prev_code = \"\"\"\n",
    "# logger.info(\"THIS IS PRINTING HOW THE IMAGE AND LABELS LOOK POSTPROCESSED\")\n",
    "# ppp(\n",
    "#     torch.from_numpy(batch_img_nparray[iii]).permute(2, 0, 1),\n",
    "#     bboxes=pred[iii][\"boxes\"],\n",
    "#     coords=\"xyxy\",\n",
    "#     figsize=(4, 4),\n",
    "# )\n",
    "# loader_v2 = create_clean_loader(\n",
    "#     val_datasets, shuffle=False, transforms=simple_transform(), batch_size=8 # type: ignore\n",
    "# )\n",
    "# skip_cnt2 = 1\n",
    "# for custom_batch2 in loader_v2:\n",
    "#     if skip_cnt2 >0:\n",
    "#         skip_cnt2 -=1\n",
    "#         continue\n",
    "#     else:\n",
    "#         break\n",
    "# img2, targets2 = custom_batch2  # type: ignore\n",
    "# ppp(\n",
    "#     img2[0],\n",
    "#     bboxes= [],\n",
    "#     coords=\"xyxy\",\n",
    "#     figsize=(4,4),\n",
    "# )\n",
    "# print(img2[0].shape)\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
