{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0feb700b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.adapters import YoloUltralyticsAdapter, RtdetrUltralyticsAdapter, EfficientDetAdapter, FasterRcnnAdapter\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "logger = setup_logger(__name__)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d28f0a",
   "metadata": {},
   "source": [
    "## Ensemble saving debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d3f9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle as pkl\n",
    "\n",
    "from ml_carbucks import OPTUNA_DIR, PRODUCTS_DIR\n",
    "from ml_carbucks.adapters.EnsembleModel import EnsembleModel\n",
    "from ml_carbucks.optmization.TrialParamWrapper import TrialParamWrapper\n",
    "\n",
    "\n",
    "adapters = [\n",
    "        YoloUltralyticsAdapter(\n",
    "            checkpoint=PRODUCTS_DIR / \"best_pickled_YoloUltralyticsAdapter_model.pkl\"\n",
    "        ),\n",
    "        RtdetrUltralyticsAdapter(\n",
    "            checkpoint=PRODUCTS_DIR / \"best_pickled_RtdetrUltralyticsAdapter_model.pkl\"\n",
    "        ),\n",
    "        FasterRcnnAdapter(\n",
    "            checkpoint=PRODUCTS_DIR / \"best_pickled_FasterRcnnAdapter_model.pkl\"\n",
    "        ),\n",
    "        EfficientDetAdapter(\n",
    "            checkpoint=PRODUCTS_DIR / \"best_pickled_EfficientDetAdapter_model.pkl\"\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "trial_params = {\n",
    "    \"fusion_strategy\": \"nms\",\n",
    "    \"fusion_conf_threshold\": 0.1007298895150254,\n",
    "    \"fusion_iou_threshold\": 0.3961959035309548,\n",
    "    \"fusion_max_detections\": 5,\n",
    "    \"fusion_norm_method\": \"quantile\",\n",
    "    \"fusion_trust_factor_0\": 2.9826045621475172,\n",
    "    \"fusion_trust_factor_1\": 2.9578690014499065,\n",
    "    \"fusion_trust_factor_2\": 2.277096902306202,\n",
    "    \"fusion_trust_factor_3\": 2.8984911425609545\n",
    "}\n",
    "\n",
    "ensemble_adapters = [adapter for adapter in adapters]\n",
    "ensemble_params = TrialParamWrapper.convert_ensemble_params_to_model_format(\n",
    "    trial_params, ensemble_size=len(ensemble_adapters)\n",
    ") \n",
    "obj = pkl.load(open(OPTUNA_DIR / \"ensemble\" / \"prestep_a4605cd505acf557.pkl\", \"rb\"))\n",
    "(\n",
    "    adapters_predictions,\n",
    "    ground_truths,\n",
    "    distributions,\n",
    "    adapters_crossval_metrics,\n",
    "    adapters_dataset_metrics,\n",
    ") = obj\n",
    "\n",
    "\n",
    "ensemble_model = EnsembleModel(\n",
    "    **ensemble_params,\n",
    "    adapters=ensemble_adapters,\n",
    "    distributions=distributions,\n",
    ")\n",
    "\n",
    "\n",
    "ensemble_model.save(PRODUCTS_DIR / \"ensemble\" , suffix=\"debugging\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c99926",
   "metadata": {},
   "source": [
    "## Ensemble pickled model evalaution vs trial model pickled evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bfe2e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "emodel = EfficientDetAdapter(checkpoint=PRODUCTS_DIR / \"ensemble\" / \"adapter_3_modeldebugging.pkl\")\n",
    "eres = emodel.evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "logger.info(eres[\"map_50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b28cbe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emodel2 = EfficientDetAdapter(checkpoint=PRODUCTS_DIR / \"best_pickled_EfficientDetAdapter_model.pkl\")\n",
    "eres2 = emodel2.evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "logger.info(eres2[\"map_50\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181a5c0f",
   "metadata": {},
   "source": [
    "## Ensemble pickled evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b404e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_ensemble = EnsembleModel(checkpoint=PRODUCTS_DIR / \"ensemble\" / \"ensemble_modeldebugging.pkl\")\n",
    "\n",
    "lres = loaded_ensemble.evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "\n",
    "logger.info(lres[\"map_50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2f1fbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.utils.postprocessing import plot_pr_curves_with_ap50\n",
    "plot_pr_curves_with_ap50(\n",
    "    lres,\n",
    "    save=PRODUCTS_DIR / \"ensemble\" / \"debugging_pr_curves.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f16a6f13",
   "metadata": {},
   "source": [
    "## Ensemble optimizatin results debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd860181",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.adapters import EnsembleModel\n",
    "from ml_carbucks import OPTUNA_DIR\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "\n",
    "opt_ensemble = EnsembleModel(checkpoint=OPTUNA_DIR / \"ensemble\" / \"20251201_114723_demo_combined_explorative\" / \"ensemble_model20251201_114723_demo_combined_explorative.pkl\")\n",
    "opt_ensemble.set_params({\"verbose\": True})\n",
    "ores = opt_ensemble.evaluate(DatasetsPathManager.CARBUCKS_VAL_STANDARD)\n",
    "\n",
    "logger.info(ores[\"map_50\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c34523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import numpy as np\n",
    "from ml_carbucks.utils.inference import plot_img_pred, plot_img_pred_subplots\n",
    "\n",
    "from ml_carbucks import DATA_DIR\n",
    "\n",
    "img_path = DATA_DIR / \"final_carbucks\"/ \"standard\"/ \"images\"/ \"val\" / \"bd3f80e0-c3d0-4e2c-bc9e-0d5de1e50fae.jpg\"\n",
    "\n",
    "# just laod image aIMagefile\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "\n",
    "image_base64 = base64.b64encode(open(img_path, \"rb\").read()).decode('utf-8')\n",
    "\n",
    "img_np = np.array(img)\n",
    "\n",
    "pred1 = opt_ensemble.predict([img_np])\n",
    "pred2 = opt_ensemble.predict_from_base64([image_base64])\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import torch \n",
    "\n",
    "logger.info(pred1[0][\"boxes\"])\n",
    "logger.info(img_np.shape)\n",
    "plot_img_pred_subplots(\n",
    "    [torch.tensor(img_np).permute(2,0,1)],\n",
    "    [pred1[0][\"boxes\"]],\n",
    "    labels_list=[pred1[0][\"labels\"]],\n",
    "    scores_list=[pred1[0][\"scores\"]],\n",
    "    coords=\"xyxy\",\n",
    "    figsize=(5,5)\n",
    ")\n",
    "\n",
    "logger.info(pred1[0][\"labels\"])\n",
    "plt.imshow(img_np)\n",
    "for box in pred1[0][\"boxes\"]:\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    rect = plt.Rectangle((x1, y1), width, height, fill=False, color=\"red\", linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "plt.show()\n",
    "\n",
    "logger.info(pred2[0][\"labels\"])\n",
    "plt.imshow(img_np)\n",
    "for box in pred2[0][\"boxes\"]:\n",
    "    x1, y1, x2, y2 = box\n",
    "    width = x2 - x1\n",
    "    height = y2 - y1\n",
    "    rect = plt.Rectangle((x1, y1), width, height, fill=False, color=\"red\", linewidth=2)\n",
    "    plt.gca().add_patch(rect)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b93c134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.adapters import EnsembleModel\n",
    "from ml_carbucks import OPTUNA_DIR, DATA_DIR\n",
    "from ml_carbucks.utils.DatasetsPathManager import DatasetsPathManager\n",
    "from PIL import Image\n",
    "from ml_carbucks.utils.inference import plot_img_pred_subplots\n",
    "import numpy as np\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "opt_ensemble = EnsembleModel(checkpoint=OPTUNA_DIR / \"ensemble\" / \"20251201_114723_demo_combined_explorative\" / \"ensemble_model20251201_114723_demo_combined_explorative.pkl\")\n",
    "opt_ensemble.set_params({\"verbose\": True})\n",
    "img_path = DATA_DIR / \"final_carbucks\"/ \"standard\"/ \"images\"/ \"val\" / \"bd3f80e0-c3d0-4e2c-bc9e-0d5de1e50fae.jpg\"\n",
    "\n",
    "# just laod image aIMagefile\n",
    "\n",
    "img = Image.open(img_path)\n",
    "\n",
    "import base64\n",
    "from io import BytesIO\n",
    "import torch \n",
    "\n",
    "\n",
    "img_np = np.array(img)\n",
    "\n",
    "pred1 = opt_ensemble.adapters[1].predict([img_np])\n",
    "\n",
    "\n",
    "logger.info(pred1[0][\"boxes\"])\n",
    "logger.info(img_np.shape)\n",
    "plot_img_pred_subplots(\n",
    "    [torch.tensor(img_np).permute(2,0,1)],\n",
    "    [pred1[0][\"boxes\"]],\n",
    "    labels_list=[pred1[0][\"labels\"]],\n",
    "    scores_list=[pred1[0][\"scores\"]],\n",
    "    coords=\"xyxy\",\n",
    "    figsize=(5,5)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a553ea3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.adapters import RtdetrUltralyticsAdapter\n",
    "from ml_carbucks import DATA_DIR\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "from ml_carbucks.utils.inference import plot_img_pred_subplots\n",
    "import torch\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "model = RtdetrUltralyticsAdapter(checkpoint=\"/home/damian/Desktop/Projects/Bachelor/ml/optuna/ensemble/20251201_114723_demo_combined_explorative/adapter_1_model20251201_114723_demo_combined_explorative.pkl\")\n",
    "img_path = DATA_DIR / \"final_carbucks\"/ \"standard\"/ \"images\"/ \"val\" / \"bd3f80e0-c3d0-4e2c-bc9e-0d5de1e50fae.jpg\"\n",
    "\n",
    "\n",
    "img = Image.open(img_path)\n",
    "img_np = np.array(img)\n",
    "\n",
    "pred = model.predict([img_np])\n",
    "logger.info(pred[0])\n",
    "\n",
    "plot_img_pred_subplots(\n",
    "    [torch.tensor(img_np).permute(2,0,1)],\n",
    "    [pred[0][\"boxes\"]],\n",
    "    labels_list=[pred[0][\"labels\"]],\n",
    "    scores_list=[pred[0][\"scores\"]],\n",
    "    coords=\"xyxy\",\n",
    "    figsize=(5,5)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
