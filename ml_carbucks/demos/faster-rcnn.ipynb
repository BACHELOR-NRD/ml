{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0854cd66",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Any, cast\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.datasets import CocoDetection\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor, fasterrcnn_resnet50_fpn, fasterrcnn_resnet50_fpn_v2, FasterRCNN_ResNet50_FPN_Weights, FasterRCNN_ResNet50_FPN_V2_Weights\n",
    "import torchvision.transforms.functional as F\n",
    "from ml_carbucks import DATA_CAR_DD_DIR\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "IMG_SIZE = 320\n",
    "BATCH_SIZE = 16\n",
    "NUM_CLASSES = 4  # background + 3 object classes\n",
    "# --- Dataset must return ---\n",
    "# img: tensor [3,H,W]\n",
    "# target: dict with:\n",
    "#   boxes (FloatTensor [N,4]), labels (Int64Tensor [N])\n",
    "#   optional: image_id, area, iscrowd\n",
    "\n",
    "\n",
    "\n",
    "class COCODetectionWrapper(Dataset):\n",
    "    def __init__(self, img_folder, ann_file, transforms=None):\n",
    "        self.dataset = CocoDetection(img_folder, ann_file)\n",
    "        self.transforms = transforms\n",
    "\n",
    "        # Map COCO category IDs (non-sequential) -> continuous label IDs\n",
    "        self.cat_id_to_label = {\n",
    "            cat[\"id\"]: idx + 1  # +1 because 0 = background\n",
    "            for idx, cat in enumerate(self.dataset.coco.cats.values())\n",
    "        }\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img, anns = self.dataset[idx]\n",
    "        img = np.array(img, dtype=np.uint8)  # needed for Albumentations\n",
    "\n",
    "        if len(anns) == 0:\n",
    "            boxes_coco = np.zeros((0, 4), dtype=np.float32)\n",
    "            labels = np.zeros((0,), dtype=np.int64)\n",
    "        else:\n",
    "            boxes_coco = np.array([ann[\"bbox\"] for ann in anns], dtype=np.float32)\n",
    "            boxes_coco = np.clip(boxes_coco, a_min=0, a_max=None)  # ensure non-negative\n",
    "            labels = np.array([self.cat_id_to_label[ann[\"category_id\"]] for ann in anns], dtype=np.int64)\n",
    "\n",
    "        if self.transforms:\n",
    "            sample = self.transforms(image=img, bboxes=boxes_coco.tolist(), labels=labels.tolist())\n",
    "            img = sample[\"image\"]\n",
    "            boxes_coco = np.array(sample[\"bboxes\"], dtype=np.float32)\n",
    "            labels = np.array(sample[\"labels\"], dtype=np.int64)\n",
    "\n",
    "        # Convert COCO to VOC format\n",
    "        if boxes_coco.shape[0] > 0:\n",
    "            boxes_voc = boxes_coco.copy()\n",
    "            boxes_voc[:, 2] += boxes_voc[:, 0]  # x + w → x2\n",
    "            boxes_voc[:, 3] += boxes_voc[:, 1]  # y + h → y2\n",
    "        else:\n",
    "            boxes_voc = np.zeros((0, 4), dtype=np.float32)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.from_numpy(boxes_voc),\n",
    "            \"labels\": torch.from_numpy(labels),\n",
    "            \"image_id\": torch.tensor(idx),\n",
    "            \"area\": torch.from_numpy((boxes_voc[:, 2] - boxes_voc[:, 0]) * (boxes_voc[:, 3] - boxes_voc[:, 1])),\n",
    "            \"iscrowd\": torch.zeros((boxes_voc.shape[0],), dtype=torch.int64),\n",
    "        }\n",
    "\n",
    "        return img, target\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "\n",
    "def create_transforms(is_training: bool) -> A.Compose:\n",
    "    \n",
    "    arr = []\n",
    "    arr.extend([\n",
    "        A.LongestMaxSize(max_size=IMG_SIZE),\n",
    "        A.PadIfNeeded(\n",
    "            min_height=IMG_SIZE,\n",
    "            min_width=IMG_SIZE,\n",
    "            border_mode=0,  # constant padding\n",
    "            fill=(0, 0, 0),  # black\n",
    "        ),\n",
    "    ])\n",
    "    if is_training:\n",
    "        arr.append(A.HorizontalFlip(p=0.5))\n",
    "    \n",
    "    arr.extend([\n",
    "        A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "    custom_transform = A.Compose(\n",
    "        cast(\n",
    "            Any,\n",
    "            arr,\n",
    "        ),\n",
    "        bbox_params=A.BboxParams(\n",
    "            format=\"coco\",  # we now correctly pass COCO-format boxes in/out\n",
    "            label_fields=[\"labels\"],\n",
    "            min_visibility=0.3,\n",
    "        ),\n",
    "    )\n",
    "\n",
    "    return custom_transform\n",
    "# --- Dataset ---\n",
    "train_dataset = COCODetectionWrapper(\n",
    "    img_folder=DATA_CAR_DD_DIR / \"images\" / \"train\",\n",
    "    ann_file=DATA_CAR_DD_DIR / \"instances_train.json\",\n",
    "    transforms=create_transforms(is_training=True),\n",
    ")\n",
    "val_dataset = COCODetectionWrapper(\n",
    "    img_folder=DATA_CAR_DD_DIR / \"images\" / \"val\",\n",
    "    ann_file=DATA_CAR_DD_DIR / \"instances_val.json\",\n",
    "    transforms=create_transforms(is_training=False),\n",
    ")\n",
    "\n",
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,          # 2–8 is typical, memory permitting\n",
    "    shuffle=True,\n",
    "    num_workers=BATCH_SIZE // 2,         # adjust based on your CPU\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn, # crucial\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=BATCH_SIZE // 2,\n",
    "    pin_memory=True,\n",
    "    collate_fn=collate_fn,\n",
    ")\n",
    "\n",
    "\n",
    "logger = setup_logger(\"faster_rcnn\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4c6e9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks.utils.inference import plot_img_pred as ppp\n",
    "imgs, targets = next(iter(train_loader))\n",
    "\n",
    "iii = 5\n",
    "print(imgs[iii], targets[iii])\n",
    "\n",
    "ppp(imgs[iii], targets[iii]['boxes'], coords=\"xyxy\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa9179e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "# clean gpu memory \n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = fasterrcnn_resnet50_fpn(weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT)\n",
    "# model = fasterrcnn_resnet50_fpn_v2(weights=FasterRCNN_ResNet50_FPN_V2_Weights.DEFAULT)\n",
    "in_features = model.roi_heads.box_predictor.cls_score.in_features # type: ignore\n",
    "model.roi_heads.box_predictor = FastRCNNPredictor(in_features, NUM_CLASSES)\n",
    "model = model.to(device)\n",
    "\n",
    "# --- Optimizer ---\n",
    "backbone_params = []\n",
    "head_params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if \"backbone\" in name:\n",
    "        backbone_params.append(param)\n",
    "    else:\n",
    "        head_params.append(param)\n",
    "optimizer = torch.optim.AdamW(\n",
    "    [\n",
    "        {\"params\": backbone_params, \"lr\": 5e-5, \"weight_decay\": 1e-4},\n",
    "        {\"params\": head_params, \"lr\": 1e-3, \"weight_decay\": 1e-4},\n",
    "    ]\n",
    ")\n",
    "\n",
    "EPOCHS = 200\n",
    "# --- Scheduler (optional) ---\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS)\n",
    "\n",
    "\n",
    "# --- Training loop ---\n",
    "num_epochs = EPOCHS\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for imgs, targets in tqdm(train_loader):\n",
    "\n",
    "        imgs = list(img.to(device) for img in imgs)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(imgs, targets)\n",
    "        loss = sum(loss for loss in loss_dict.values())\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # type: ignore\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() # type: ignore\n",
    "\n",
    "    if scheduler:\n",
    "        scheduler.step()\n",
    "\n",
    "    # --- Validation on training data (resized) ---\n",
    "    model.eval()\n",
    "    metric = MeanAveragePrecision()\n",
    "    with torch.no_grad():\n",
    "        for imgs, targets in tqdm(val_loader):\n",
    "            imgs = list(img.to(device) for img in imgs)\n",
    "            outputs = model(imgs)\n",
    "\n",
    "            # Prepare targets in expected dict format\n",
    "            targets_cpu = [{k: v.cpu() for k, v in t.items()} for t in targets]\n",
    "            outputs_cpu = [{k: v.cpu() for k, v in t.items()} for t in outputs]\n",
    "            metric.update(outputs_cpu, targets_cpu)\n",
    "\n",
    "    val_res = metric.compute()\n",
    "    metric.reset()\n",
    "    logger.info(f\"Epoch {epoch+1}/{num_epochs} | Loss: {total_loss:.4f} | val_map: {val_res['map'].item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e014f890",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# # --- Model ---\n",
    "# num_classes = 4  # background + 3 classes\n",
    "# model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "\n",
    "# # Replace the head (classifier)\n",
    "# in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "# model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model.to(device)\n",
    "\n",
    "\n",
    "# # --- Training Setup ---\n",
    "# params = [p for p in model.parameters() if p.requires_grad]\n",
    "# optimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "# lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "# num_epochs = 20\n",
    "# for epoch in range(num_epochs):\n",
    "#     model.train()\n",
    "#     for imgs, targets in train_loader:\n",
    "#         imgs = [img.to(device) for img in imgs]\n",
    "#         targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "#         loss_dict = model(imgs, targets)\n",
    "#         losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "#         optimizer.zero_grad()\n",
    "#         losses.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#     lr_scheduler.step()\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, loss: {losses.item():.4f}\")\n",
    "\n",
    "\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     for imgs, _ in val_loader:\n",
    "#         imgs = [img.to(device) for img in imgs]\n",
    "#         preds = model(imgs)  # list of dicts with boxes, labels, scores\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
