{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b10e13db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ml_carbucks import DATA_DIR\n",
    "\n",
    "train_img_dir = DATA_DIR /\"car_dd_testing\"/ \"images\" / \"train\"\n",
    "train_ann_file = DATA_DIR /\"car_dd_testing\"/ \"instances_train_curated.json\"\n",
    "\n",
    "val_img_dir = DATA_DIR /\"car_dd_testing\"/ \"images\" / \"val\"\n",
    "val_ann_file = DATA_DIR /\"car_dd_testing\"/ \"instances_val_curated.json\"\n",
    "\n",
    "classes = ['scratch', 'dent', 'crack']\n",
    "\n",
    "img_size = 320\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "509c2de7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:53:45 | Starting training...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:53:45 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:53:45 | COCO to YOLO conversion completed in 0.09 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:53:45 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/car_dd_testing/instances_train_curated.yaml\n",
      "New https://pypi.org/project/ultralytics/8.3.223 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/car_dd_testing/instances_train_curated.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.001, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.9, mosaic=1.0, multi_scale=False, name=train34, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/train34, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0001, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,312,793 parameters, 25,312,777 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 7722.2¬±1066.8 MB/s, size: 724.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd_testing/labels/train.cache... 2184 images, 632 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 8.7Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4449.6¬±2497.7 MB/s, size: 724.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd_testing/labels/train.cache... 2184 images, 632 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 5.6Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/train34/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.001' and 'momentum=0.9' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0001), 173 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train34\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/1      3.06G      2.055      2.808      1.894         57        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 176/176 7.6it/s 23.0s0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 88/88 6.0it/s 14.7s0.4s\n",
      "                   all       2816       5017     0.0965      0.143     0.0539     0.0191\n",
      "\n",
      "1 epochs completed in 0.011 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train34/weights/last.pt, 51.1MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train34/weights/best.pt, 51.1MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/train34/weights/best.pt...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 88/88 12.5it/s 7.0s0.2s\n",
      "                   all       2816       5017      0.097      0.141     0.0538     0.0191\n",
      "               scratch       1507       2560     0.0776      0.245     0.0644     0.0217\n",
      "                  dent       1242       1806      0.166      0.158     0.0889     0.0336\n",
      "                 crack        434        651      0.047       0.02    0.00823    0.00204\n",
      "Speed: 0.0ms preprocess, 1.1ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train34\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:54:40 | Starting evaluation...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:54:40 | Converting COCO annotations to YOLO format...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:54:40 | COCO to YOLO conversion completed in 0.02 seconds\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 20:54:40 | YOLO dataset YAML created at: /home/bachelor/ml-carbucks/data/car_dd_testing/instances_val_curated.yaml\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8020.6¬±540.5 MB/s, size: 677.2 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd_testing/labels/val... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 2.8Kit/s 0.3s.2s\n",
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/bachelor/ml-carbucks/data/car_dd_testing/labels/val.cache\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 16.2it/s 3.2s0.1s\n",
      "                   all        810       1406     0.0994      0.146     0.0518     0.0167\n",
      "               scratch        431        728     0.0705      0.239     0.0562     0.0165\n",
      "                  dent        352        501       0.16      0.176     0.0914     0.0316\n",
      "                 crack        122        177     0.0676     0.0226    0.00787    0.00198\n",
      "Speed: 0.1ms preprocess, 2.3ms inference, 0.0ms loss, 0.6ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val4\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': 0.05180518800687622, 'map_50_95': 0.016697555318842403}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.UltralyticsAdapter import YoloUltralyticsAdapter\n",
    "\n",
    "y_model = YoloUltralyticsAdapter(\n",
    "    classes=classes,\n",
    "    img_size=img_size,\n",
    "    epochs=epochs,\n",
    ")\n",
    "y_model.setup()\n",
    "y_model.fit(\n",
    "    img_dir=train_img_dir,\n",
    "    ann_file=train_ann_file,\n",
    ")\n",
    "y_res = y_model.evaluate(\n",
    "    img_dir=val_img_dir,\n",
    "    ann_file=val_ann_file,\n",
    ")\n",
    "y_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2f1ca5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 20:59:41 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 20:59:41 | Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [01:04<00:00,  2.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 21:00:45 | Starting evaluation...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': 0.06331790238618851, 'map_50_95': 0.019417647272348404}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.FasterRcnnAdapter import FasterRcnnAdapter\n",
    "\n",
    "f_model = FasterRcnnAdapter(\n",
    "    classes=classes,\n",
    "    img_size=img_size,\n",
    "    epochs=epochs,\n",
    ")\n",
    "f_model.setup()\n",
    "f_model.fit(\n",
    "    img_dir=train_img_dir,\n",
    "    ann_file=train_ann_file,\n",
    ")\n",
    "f_res = f_model.evaluate(\n",
    "    img_dir=val_img_dir,\n",
    "    ann_file=val_ann_file,\n",
    ")\n",
    "f_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9916df07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unexpected keys (bn2.num_batches_tracked, bn2.bias, bn2.running_mean, bn2.running_var, bn2.weight, classifier.bias, classifier.weight, conv_head.weight) found while loading pretrained weights. This may be expected if model is being adapted.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:10:40 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:10:40 | Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:21<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:11:02 | Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:21<00:00,  8.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:11:23 | Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:21<00:00,  8.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:11:44 | Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:21<00:00,  8.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:12:06 | Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 176/176 [00:21<00:00,  8.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.25s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.89s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.25s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.009\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.041\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.001\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.011\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.032\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.098\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.144\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.004\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.165\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': np.float64(0.04102801696408651),\n",
       " 'map_50_95': np.float64(0.009380011380935282)}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter\n",
    "\n",
    "e_model = EfficientDetAdapter(\n",
    "    classes=classes,\n",
    "    img_size=img_size,\n",
    "    epochs=epochs,\n",
    ")\n",
    "e_model.setup()\n",
    "e_model.fit(\n",
    "    img_dir=train_img_dir,\n",
    "    ann_file=train_ann_file,\n",
    ")\n",
    "e_res = e_model.evaluate(\n",
    "    img_dir=val_img_dir,\n",
    "    ann_file=val_ann_file,\n",
    ")\n",
    "e_res"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
