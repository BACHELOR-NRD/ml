{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "656d8c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Any, Dict, List, Tuple\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from tqdm import tqdm\n",
    "\n",
    "from ml_carbucks import DATA_DIR\n",
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter\n",
    "from ml_carbucks.adapters.FasterRcnnAdapter import FasterRcnnAdapter\n",
    "from ml_carbucks.adapters.UltralyticsAdapter import RtdetrUltralyticsAdapter, YoloUltralyticsAdapter\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "from ml_carbucks.utils.preprocessing import create_clean_loader\n",
    "from ml_carbucks.utils.postprocessing import process_evaluation_results\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import BaseDetectionAdapter\n",
    "\n",
    "logger = setup_logger(\"adapter_eval_vs_predict\")\n",
    "\n",
    "\n",
    "classes=[\"scratch\", \"dent\", \"crack\"]\n",
    "\n",
    "adapters=[\n",
    "    YoloUltralyticsAdapter(\n",
    "        classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "        **{\n",
    "            \"img_size\": 384,\n",
    "            \"batch_size\": 32,\n",
    "            \"epochs\": 27,\n",
    "            \"lr\": 0.0015465639515144544,\n",
    "            \"momentum\": 0.3628781599889685,\n",
    "            \"weight_decay\": 0.0013127041660177367,\n",
    "            \"optimizer\": \"NAdam\",\n",
    "            \"verbose\": False,\n",
    "        },\n",
    "        weights=\"/home/bachelor/ml-carbucks/results/ensemble_demos/trial_4_YoloUltralyticsAdaptermodel.pt\",\n",
    "    ),\n",
    "    RtdetrUltralyticsAdapter(\n",
    "        classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "        **{\n",
    "            \"img_size\": 384,\n",
    "            \"batch_size\": 16,\n",
    "            \"epochs\": 10,\n",
    "            \"lr\": 0.0001141043015859849,\n",
    "            \"momentum\": 0.424704619626319,\n",
    "            \"weight_decay\": 0.00012292547851740234,\n",
    "            \"optimizer\": \"AdamW\",\n",
    "        },\n",
    "        weights=\"/home/bachelor/ml-carbucks/results/ensemble_demos/trial_4_RtdetrUltralyticsAdaptermodel.pt\",\n",
    "    ),\n",
    "    FasterRcnnAdapter(\n",
    "        classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "        **{\n",
    "            \"img_size\": 384,\n",
    "            \"batch_size\": 8,\n",
    "            \"epochs\": 30,\n",
    "            \"lr_backbone\": 2.6373762637681257e-05,\n",
    "            \"lr_head\": 0.0011244046084737927,\n",
    "            \"weight_decay_backbone\": 0.000796017512818448,\n",
    "            \"weight_decay_head\": 0.0005747409908715994,\n",
    "        },\n",
    "        weights=\"/home/bachelor/ml-carbucks/results/ensemble_demos/FasterRcnnAdaptermodel.pth\",\n",
    "    ),\n",
    "    EfficientDetAdapter(\n",
    "        classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "        **{\n",
    "            \"img_size\": 384,\n",
    "            \"batch_size\": 8,\n",
    "            \"epochs\": 26,\n",
    "            \"optimizer\": \"momentum\",\n",
    "            \"lr\": 0.003459928723120903,\n",
    "            \"weight_decay\": 0.0001302610542371722,\n",
    "        },\n",
    "        weights=\"/home/bachelor/ml-carbucks/results/ensemble_demos/trial_4_EfficientDetAdaptermodel.pth\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "train_datasets = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"train\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_train_curated.json\",\n",
    "    )\n",
    "]\n",
    "\n",
    "val_datasets: List[Tuple[str | Path, str | Path]] = [\n",
    "    (\n",
    "        DATA_DIR / \"car_dd_testing\" / \"images\" / \"val\",\n",
    "        DATA_DIR / \"car_dd_testing\" / \"instances_val_curated.json\",\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e6e9fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO adapter_eval_vs_predict 15:15:53 | Collecting adapter predictions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 102/102 [00:55<00:00,  1.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO adapter_eval_vs_predict 15:16:48 | Adapter predictions collected.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class EnsembleModel:\n",
    "    classes: List[str]\n",
    "    adapters: List[BaseDetectionAdapter]\n",
    "\n",
    "    def setup(self) -> \"EnsembleModel\":\n",
    "        for adapter in self.adapters:\n",
    "            adapter.setup()\n",
    "        return self\n",
    "\n",
    "    def evaluate_adapters_by_evaluation_from_dataset(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> List[dict]:\n",
    "        metrics = []\n",
    "        for adapter in self.adapters:\n",
    "            adapter_metrics = adapter.evaluate(datasets)\n",
    "            metrics.append(adapter_metrics)\n",
    "        return metrics\n",
    "\n",
    "    def evaluate_adapters_by_predict_from_dataset(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> List[dict]:\n",
    "\n",
    "        metrics = [MeanAveragePrecision() for _ in self.adapters]\n",
    "        loader = create_clean_loader(\n",
    "            datasets, shuffle=False, transforms=None, batch_size=8\n",
    "        )\n",
    "        results = []\n",
    "        for adapter_idx, adapter in enumerate(self.adapters):\n",
    "            logger.info(f\"Evaluating adapter: {adapter.__class__.__name__}\")\n",
    "            for images, targets in tqdm(loader):\n",
    "                predictions = adapter.predict(images)\n",
    "\n",
    "                metrics[adapter_idx].update(predictions, targets)  # type: ignore\n",
    "\n",
    "            metric = metrics[adapter_idx].compute()\n",
    "            results.append(metric)\n",
    "\n",
    "        final_results = [process_evaluation_results(metric) for metric in results]\n",
    "        return final_results\n",
    "\n",
    "    def evaluate(self, datasets: List[Tuple[str | Path, str | Path]]):\n",
    "    # -> Dict[str, Any]:\n",
    "        loader = create_clean_loader(\n",
    "            datasets, shuffle=False, transforms=None, batch_size=8\n",
    "        )\n",
    "\n",
    "        adapters_predictions = {\n",
    "            adapter.__class__.__name__: [] for adapter in self.adapters\n",
    "        }\n",
    "\n",
    "        ground_truths = []\n",
    "\n",
    "        logger.info(\"Collecting adapter predictions...\")    \n",
    "        for images, targets in tqdm(loader):\n",
    "\n",
    "            predictions = [adapter.predict(images) for adapter in self.adapters]\n",
    "            for adapter_name, preds in zip(adapters_predictions.keys(), predictions, strict=True):\n",
    "                adapters_predictions[adapter_name].extend(preds)\n",
    "\n",
    "            ground_truths.extend({\n",
    "                \"boxes\": target[\"boxes\"],\n",
    "                \"labels\": target[\"labels\"],\n",
    "            } for target in targets)\n",
    "\n",
    "        logger.info(\"Adapter predictions collected.\")\n",
    "\n",
    "        return adapters_predictions, ground_truths\n",
    "\n",
    "ensemble_model = EnsembleModel(\n",
    "    classes=classes,\n",
    "    adapters=adapters,\n",
    ").setup()\n",
    "\n",
    "adap_preds, gts = ensemble_model.evaluate(val_datasets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "487dbf1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "810\n",
      "810\n",
      "{'map_50': 0.30941420793533325, 'map_50_95': 0.14240901172161102, 'map_75': 0.11717460304498672, 'classes': [1, 2, 3]}\n"
     ]
    }
   ],
   "source": [
    "from copy import deepcopy\n",
    "from typing import Literal, Optional\n",
    "\n",
    "import torch\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import ADAPTER_PREDICTION\n",
    "from ml_carbucks.utils.ensemble import merge_single_image, normalize_scores\n",
    "\n",
    "\n",
    "def fuse_adapters_predictions(\n",
    "    adapters_predictions: dict[str, list[ADAPTER_PREDICTION]],\n",
    "    strategy: Literal[\"wbf\", \"nms\"] = \"wbf\",\n",
    "    normalize: Optional[Literal[\"minmax\", \"zscore\"]] = None,\n",
    "    trust: Optional[list[float]] = None,\n",
    "    iou_thresh: float = 0.5,\n",
    "    score_thresh: float = 0.001,\n",
    ") -> list[ADAPTER_PREDICTION]:\n",
    "    \"\"\"\n",
    "    Fuse per-image predictions from multiple adapters into a single list of ADAPTER_PREDICTIONs.\n",
    "    \"\"\"\n",
    "\n",
    "    # Sanity check\n",
    "    adapter_names = list(adapters_predictions.keys())\n",
    "    num_images = len(next(iter(adapters_predictions.values())))\n",
    "\n",
    "    # optional: ensure all adapters predict the same number of images\n",
    "    for name, preds in adapters_predictions.items():\n",
    "        assert len(preds) == num_images, f\"{name} has {len(preds)} images, expected {num_images}\"\n",
    "\n",
    "    # Optional trust normalization\n",
    "\n",
    "    list_of_tensors_per_adapter_org = [\n",
    "        [\n",
    "            torch.cat([\n",
    "                p[\"boxes\"],\n",
    "                p[\"scores\"].unsqueeze(1),\n",
    "                p[\"labels\"].unsqueeze(1).float()\n",
    "            ], dim=1)\n",
    "            if len(p[\"boxes\"]) > 0 else torch.empty((0, 6))\n",
    "            for p in preds_per_adapter\n",
    "        ] for preds_per_adapter in list(adapters_predictions.values())\n",
    "    ]    \n",
    "\n",
    "    if normalize is not None:\n",
    "        list_of_tensors_per_adapter_pro = normalize_scores(\n",
    "            list_of_tensors_per_adapter_org,\n",
    "            method=normalize,\n",
    "            trust=trust\n",
    "        )\n",
    "    else:\n",
    "        list_of_tensors_per_adapter_pro = list_of_tensors_per_adapter_org\n",
    "    \n",
    "\n",
    "    fused_predictions: list[ADAPTER_PREDICTION] = []\n",
    "\n",
    "    filtered_list_of_tensors_per_adapter = [\n",
    "        [\n",
    "            preds_for_image[preds_for_image[:, 4] >= score_thresh]\n",
    "            for preds_for_image in preds_per_adapter\n",
    "        ]\n",
    "        for preds_per_adapter in list_of_tensors_per_adapter_pro\n",
    "    ]\n",
    "\n",
    "    combined_list_of_tensors = [\n",
    "        torch.cat([\n",
    "            filtered_list_of_tensors_per_adapter[adapter_i][img_idx]\n",
    "            for adapter_i in range(len(adapter_names))\n",
    "        ])\n",
    "        for img_idx in range(num_images)\n",
    "    ]\n",
    "\n",
    "    # for img_idx in range(num_images):\n",
    "    #     per_adapter_preds = []\n",
    "\n",
    "    #     for adapter_i, adapter_name in enumerate(adapter_names):\n",
    "    #         preds_for_image = list_of_tensors_per_adapter_pro[adapter_i][img_idx]\n",
    "\n",
    "    #         if preds_for_image.numel() == 0:\n",
    "    #             continue\n",
    "\n",
    "    #         per_adapter_preds.append(preds_for_image)\n",
    "\n",
    "    #     if len(per_adapter_preds) == 0:\n",
    "    #         fused_predictions.append({\n",
    "    #             \"boxes\": torch.empty((0, 4)),\n",
    "    #             \"scores\": torch.empty((0,)),\n",
    "    #             \"labels\": torch.empty((0,), dtype=torch.long)\n",
    "    #         })\n",
    "    #         continue\n",
    "\n",
    "    #     # merge using the chosen strategy\n",
    "    #     merged = merge_single_image(\n",
    "    #         preds_list=per_adapter_preds,\n",
    "    #         strategy=strategy,\n",
    "    #         iou_thresh=iou_thresh,\n",
    "    #         score_thresh=score_thresh,\n",
    "    #     )\n",
    "\n",
    "    #     if merged.numel() == 0:\n",
    "    #         fused_predictions.append({\n",
    "    #             \"boxes\": torch.empty((0, 4)),\n",
    "    #             \"scores\": torch.empty((0,)),\n",
    "    #             \"labels\": torch.empty((0,), dtype=torch.long)\n",
    "    #         })\n",
    "    #         continue\n",
    "\n",
    "    #     fused_predictions.append({\n",
    "    #         \"boxes\": merged[:, :4],\n",
    "    #         \"scores\": merged[:, 4],\n",
    "    #         \"labels\": merged[:, 5].long(),\n",
    "    #     })\n",
    "\n",
    "    for combined_preds in combined_list_of_tensors:\n",
    "  \n",
    "        if combined_preds.numel() == 0:\n",
    "            fused_predictions.append({\n",
    "                \"boxes\": torch.empty((0, 4)),\n",
    "                \"scores\": torch.empty((0,)),\n",
    "                \"labels\": torch.empty((0,), dtype=torch.long)\n",
    "            })\n",
    "            continue\n",
    "        \n",
    "        fused_predictions.append(\n",
    "            {\n",
    "                \"boxes\": combined_preds[:, :4],\n",
    "                \"scores\": combined_preds[:, 4],\n",
    "                \"labels\": combined_preds[:, 5].long(),\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    return fused_predictions\n",
    "\n",
    "final_preds = fuse_adapters_predictions(\n",
    "    adapters_predictions=deepcopy(adap_preds),\n",
    "    strategy=\"nms\",\n",
    "    normalize=None,\n",
    "    # normalize=None,\n",
    "    trust=[1.0 for _ in adapters],\n",
    "    iou_thresh=0.5,\n",
    "    score_thresh=0.1,\n",
    ")\n",
    "\n",
    "print(len(final_preds))\n",
    "print(len(gts))\n",
    "\n",
    "metric = MeanAveragePrecision()\n",
    "metric.update(final_preds, gts)\n",
    "\n",
    "res = process_evaluation_results(metric.compute())\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
