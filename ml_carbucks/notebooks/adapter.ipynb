{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a99a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:40:06 | Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.223 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train3, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/train3, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,312,793 parameters, 25,312,777 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 7135.8¬±789.2 MB/s, size: 785.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/train.cache... 2184 images, 632 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 9.3Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4113.2¬±2594.3 MB/s, size: 789.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 1.3Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/train3/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 167 weight(decay=0.0), 174 weight(decay=0.0005), 173 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train3\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K        1/1      10.5G      1.926      3.051      2.017         57        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 176/176 2.4it/s 1:140.4sss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26/26 2.0it/s 13.0s0.5s\n",
      "                   all        810       1406       0.17     0.0986     0.0287    0.00939\n",
      "\n",
      "1 epochs completed in 0.025 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train3/weights/last.pt, 51.2MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train3/weights/best.pt, 51.2MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/train3/weights/best.pt...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26/26 4.4it/s 5.9s0.3s\n",
      "                   all        810       1406       0.17     0.0991     0.0287    0.00934\n",
      "Speed: 0.1ms preprocess, 5.2ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train3\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:41:47 | Starting evaluation...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8882.3¬±893.7 MB/s, size: 764.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 4.0Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 5.4it/s 9.5s0.2s\n",
      "                   all        810       1406      0.164     0.0993     0.0286    0.00945\n",
      "               scratch        431        728     0.0862     0.0975     0.0322     0.0102\n",
      "                  dent        352        501     0.0564      0.189     0.0356     0.0108\n",
      "                 crack        122        177      0.349     0.0113     0.0181    0.00738\n",
      "Speed: 0.3ms preprocess, 9.1ms inference, 0.0ms loss, 1.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val6\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': 0.028637339185035953, 'map_50_95': 0.00944795263800419}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ml_carbucks.adapters.UltralyticsAdapter import YoloUltralyticsAdapter\n",
    "\n",
    "ymodel = YoloUltralyticsAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"data_yaml\": \"/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml\",\n",
    "        \"weights\": \"yolo11l.pt\",\n",
    "    },\n",
    "    hparams={\n",
    "        \"imgsz\": 640,\n",
    "        \"epochs\": 1,\n",
    "        \"batch\": 16,\n",
    "        \"lr0\": 0.005,\n",
    "        \"momentum\": 0.937,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"patience\": 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "ymodel.setup()\n",
    "ymodel.fit()\n",
    "yres = ymodel.evaluate()\n",
    "yres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf0a3dc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/rtdetr-l.pt to 'rtdetr-l.pt': 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 63.4MB 71.0MB/s 0.9s0.9s<0.0s\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:44:34 | Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.223 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=1, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.005, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=rtdetr-l.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train4, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=50, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/train4, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "WARNING ‚ö†Ô∏è no model scale passed. Assuming scale='l'.\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1     25248  ultralytics.nn.modules.block.HGStem          [3, 32, 48]                   \n",
      "  1                  -1  6    155072  ultralytics.nn.modules.block.HGBlock         [48, 48, 128, 3, 6]           \n",
      "  2                  -1  1      1408  ultralytics.nn.modules.conv.DWConv           [128, 128, 3, 2, 1, False]    \n",
      "  3                  -1  6    839296  ultralytics.nn.modules.block.HGBlock         [128, 96, 512, 3, 6]          \n",
      "  4                  -1  1      5632  ultralytics.nn.modules.conv.DWConv           [512, 512, 3, 2, 1, False]    \n",
      "  5                  -1  6   1695360  ultralytics.nn.modules.block.HGBlock         [512, 192, 1024, 5, 6, True, False]\n",
      "  6                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  7                  -1  6   2055808  ultralytics.nn.modules.block.HGBlock         [1024, 192, 1024, 5, 6, True, True]\n",
      "  8                  -1  1     11264  ultralytics.nn.modules.conv.DWConv           [1024, 1024, 3, 2, 1, False]  \n",
      "  9                  -1  6   6708480  ultralytics.nn.modules.block.HGBlock         [1024, 384, 2048, 5, 6, True, False]\n",
      " 10                  -1  1    524800  ultralytics.nn.modules.conv.Conv             [2048, 256, 1, 1, None, 1, 1, False]\n",
      " 11                  -1  1    789760  ultralytics.nn.modules.transformer.AIFI      [256, 1024, 8]                \n",
      " 12                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14                   7  1    262656  ultralytics.nn.modules.conv.Conv             [1024, 256, 1, 1, None, 1, 1, False]\n",
      " 15            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 17                  -1  1     66048  ultralytics.nn.modules.conv.Conv             [256, 256, 1, 1]              \n",
      " 18                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 19                   3  1    131584  ultralytics.nn.modules.conv.Conv             [512, 256, 1, 1, None, 1, 1, False]\n",
      " 20            [-2, -1]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 22                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 23            [-1, 17]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 24                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 25                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 26            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 27                  -1  3   2232320  ultralytics.nn.modules.block.RepC3           [512, 256, 3]                 \n",
      " 28        [21, 24, 27]  1   7308017  ultralytics.nn.modules.head.RTDETRDecoder    [3, [256, 256, 256]]          \n",
      "rt-detr-l summary: 457 layers, 32,812,241 parameters, 32,812,241 gradients, 108.0 GFLOPs\n",
      "\n",
      "Transferred 926/941 items from pretrained weights\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8012.2¬±1164.7 MB/s, size: 785.8 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/train.cache... 2184 images, 632 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 12.4Mit/s 0.0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 4243.1¬±3054.3 MB/s, size: 789.4 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 1.2Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/train4/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.005' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.001429, momentum=0.9) with parameter groups 143 weight(decay=0.0), 206 weight(decay=0.0005), 226 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train4\u001b[0m\n",
      "Starting training for 1 epochs...\n",
      "\n",
      "      Epoch    GPU_mem  giou_loss   cls_loss    l1_loss  Instances       Size\n",
      "\u001b[K        1/1      12.3G      1.674      24.88     0.9267         30        640: 0% ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ 0/176  3.3s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/torch/autograd/graph.py:829: UserWarning: grid_sampler_2d_backward_cuda does not have a deterministic implementation, but you set 'torch.use_deterministic_algorithms(True, warn_only=True)'. You can file an issue at https://github.com/pytorch/pytorch/issues to help us prioritize adding deterministic support for this operation. (Triggered internally at /pytorch/aten/src/ATen/Context.cpp:93.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[K        1/1      12.4G      0.816      3.171     0.5823         44        640: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 176/176 1.5it/s 1:570.6ss\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26/26 2.7it/s 9.6s0.3s\n",
      "                   all        810       1406      0.296      0.224      0.146     0.0749\n",
      "\n",
      "1 epochs completed in 0.036 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train4/weights/last.pt, 66.2MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/train4/weights/best.pt, 66.2MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/train4/weights/best.pt...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 26/26 3.4it/s 7.7s0.3s\n",
      "                   all        810       1406      0.293      0.228      0.145     0.0744\n",
      "Speed: 0.1ms preprocess, 8.5ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/train4\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:46:59 | Starting evaluation...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "rt-detr-l summary: 302 layers, 31,989,905 parameters, 0 gradients, 103.4 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8308.4¬±1158.5 MB/s, size: 764.5 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 2.8Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 3.6it/s 14.4s0.3s\n",
      "                   all        810       1406       0.29      0.224      0.145     0.0747\n",
      "               scratch        431        728      0.246       0.29      0.176     0.0956\n",
      "                  dent        352        501       0.28      0.296      0.184     0.0852\n",
      "                 crack        122        177      0.344     0.0858     0.0743     0.0433\n",
      "Speed: 0.4ms preprocess, 16.2ms inference, 0.0ms loss, 0.1ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val7\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': 0.1445780692611414, 'map_50_95': 0.07469711419874187}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ml_carbucks.adapters.UltralyticsAdapter import RtdetrUltralyticsAdapter\n",
    "\n",
    "rmodel = RtdetrUltralyticsAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"data_yaml\": \"/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml\",\n",
    "        \"weights\": \"rtdetr-l.pt\",\n",
    "    },\n",
    "    hparams={\n",
    "        \"imgsz\": 640,\n",
    "        \"epochs\": 1,\n",
    "        \"batch\": 16,\n",
    "        \"lr0\": 0.005,\n",
    "        \"momentum\": 0.937,\n",
    "        \"weight_decay\": 0.0005,\n",
    "        \"patience\": 50,\n",
    "    }\n",
    ")\n",
    "\n",
    "rmodel.setup()\n",
    "rmodel.fit()\n",
    "rres = rmodel.evaluate()\n",
    "rres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "48a83d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 21:48:25 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:48:25 | COCO category IDs contain 0, which is reserved for background.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:48:25 | Make sure that it is properly handled in your dataset.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:48:25 | Incrementing all category IDs by 1.\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 21:48:25 | Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [02:43<00:00,  2.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 21:51:08 | Starting evaluation...\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:51:08 | COCO category IDs contain 0, which is reserved for background.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:51:08 | Make sure that it is properly handled in your dataset.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 21:51:08 | Incrementing all category IDs by 1.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': 0.15963320434093475, 'map_50_95': 0.048279281705617905}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.FasterRcnnAdapter import FasterRcnnAdapter\n",
    "from ml_carbucks import DATA_CAR_DD_DIR\n",
    "\n",
    "fmodel = FasterRcnnAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"train_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"train\",\n",
    "        \"train_ann_file\": DATA_CAR_DD_DIR / \"instances_train.json\",\n",
    "        \"val_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"val\",\n",
    "        \"val_ann_file\": DATA_CAR_DD_DIR / \"instances_val.json\",\n",
    "    },\n",
    "    hparams={\n",
    "        \"img_size\": 640,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 8,\n",
    "        \"lr_backbone\": 5e-5,\n",
    "        \"lr_head\": 5e-4,\n",
    "        \"weight_decay_backbone\": 1e-5,\n",
    "        \"weight_decay_head\": 1e-4,\n",
    "    }\n",
    ")\n",
    "\n",
    "fmodel.setup()\n",
    "fmodel.fit()\n",
    "fmodel.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "58cc7b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:54:09 | Starting training...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 21:54:09 | Epoch 1/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:48<00:00,  7.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "Loading and preparing results...\n",
      "DONE (t=0.26s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.88s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.27s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.047\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.126\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.026\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.008\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.053\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.083\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.225\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.283\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.118\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.324\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': np.float64(0.12649547157334218),\n",
       " 'map_50_95': np.float64(0.04742573591216726)}"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter\n",
    "from ml_carbucks import DATA_CAR_DD_DIR\n",
    "\n",
    "emodel = EfficientDetAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"version\": \"efficientdet_d0\",\n",
    "        \"train_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"train\",\n",
    "        \"train_ann_file\": DATA_CAR_DD_DIR / \"instances_train_curated.json\",\n",
    "        \"val_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"val\",\n",
    "        \"val_ann_file\": DATA_CAR_DD_DIR / \"instances_val_curated.json\",\n",
    "    },\n",
    "    hparams={\n",
    "        \"img_size\": 512,\n",
    "        \"epochs\": 1,\n",
    "        \"batch_size\": 8,\n",
    "        \"opt\": \"momentum\",\n",
    "        \"lr\": 8e-3,\n",
    "        \"weight_decay\": 1e-4,\n",
    "    }\n",
    ")\n",
    "\n",
    "emodel.setup()\n",
    "emodel.fit()\n",
    "emodel.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
