{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a99a91f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:55:54 | Loading Ultralytics model...\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:55:54 | Created new YOLO model: yolo11l.pt\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:55:54 | Starting training...\n",
      "New https://pypi.org/project/ultralytics/8.3.222 available üòÉ Update with 'pip install -U ultralytics'\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=8, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=10, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=320, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.00029631881419241645, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11l.pt, momentum=0.38243835004885135, mosaic=1.0, multi_scale=False, name=ultralytics_run9, nbs=64, nms=False, opset=None, optimize=False, optimizer=AdamW, overlap_mask=True, patience=31, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=False, save_conf=False, save_crop=False, save_dir=/home/bachelor/ml-carbucks/runs/detect/ultralytics_run9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=42, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=False, verbose=False, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=9.16499123351809e-05, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      1856  ultralytics.nn.modules.conv.Conv             [3, 64, 3, 2]                 \n",
      "  1                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  2                  -1  2    173824  ultralytics.nn.modules.block.C3k2            [128, 256, 2, True, 0.25]     \n",
      "  3                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      "  4                  -1  2    691712  ultralytics.nn.modules.block.C3k2            [256, 512, 2, True, 0.25]     \n",
      "  5                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  6                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  7                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      "  8                  -1  2   2234368  ultralytics.nn.modules.block.C3k2            [512, 512, 2, True]           \n",
      "  9                  -1  1    656896  ultralytics.nn.modules.block.SPPF            [512, 512, 5]                 \n",
      " 10                  -1  2   1455616  ultralytics.nn.modules.block.C2PSA           [512, 512, 2]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  2    756736  ultralytics.nn.modules.block.C3k2            [1024, 256, 2, True]          \n",
      " 17                  -1  1    590336  ultralytics.nn.modules.conv.Conv             [256, 256, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  2   2365440  ultralytics.nn.modules.block.C3k2            [768, 512, 2, True]           \n",
      " 20                  -1  1   2360320  ultralytics.nn.modules.conv.Conv             [512, 512, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  2   2496512  ultralytics.nn.modules.block.C3k2            [1024, 512, 2, True]          \n",
      " 23        [16, 19, 22]  1   1413337  ultralytics.nn.modules.head.Detect           [3, [256, 512, 512]]          \n",
      "YOLO11l summary: 357 layers, 25,312,793 parameters, 25,312,777 gradients, 87.3 GFLOPs\n",
      "\n",
      "Transferred 1009/1015 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8478.3¬±1393.9 MB/s, size: 724.0 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/train.cache... 2184 images, 632 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2816/2816 9.4Mit/s 0.0s0s\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 5326.7¬±3543.4 MB/s, size: 765.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 1.7Mit/s 0.0s0s\n",
      "Plotting labels to /home/bachelor/ml-carbucks/runs/detect/ultralytics_run9/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00029631881419241645, momentum=0.38243835004885135) with parameter groups 167 weight(decay=0.0), 174 weight(decay=9.16499123351809e-05), 173 bias(decay=0.0)\n",
      "Image sizes 320 train, 320 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/ultralytics_run9\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "Closing dataloader mosaic\n",
      "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01, method='weighted_average', num_output_channels=3), CLAHE(p=0.01, clip_limit=(1.0, 4.0), tile_grid_size=(8, 8))\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       1/10      1.95G      2.159      3.044      1.993         13        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 13.1it/s 26.8s0.1ss\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       2/10      1.95G      2.041      2.633      1.919         12        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.5it/s 21.3s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       3/10      1.95G      1.943      2.487      1.861         20        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.9it/s 20.9s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       4/10      1.95G      1.857      2.341      1.794          8        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.9it/s 20.8s0.1ss\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       5/10      1.95G      1.772      2.153      1.723         23        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.9it/s 20.8s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       6/10      1.95G      1.701      2.013      1.663         18        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.9it/s 20.8s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       7/10      1.95G      1.634      1.891      1.606         15        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.8it/s 20.9s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       8/10      1.95G      1.559      1.777      1.563         12        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.7it/s 21.0s0.1ss\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K       9/10      1.95G      1.498       1.66      1.521         28        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.8it/s 21.0s<0.1s\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n",
      "\u001b[K      10/10      1.95G      1.429      1.542      1.483          6        320: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 352/352 16.8it/s 21.0s<0.1s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 5.6it/s 9.2s0.5ss\n",
      "                   all        810       1406      0.481      0.386      0.393      0.215\n",
      "\n",
      "10 epochs completed in 0.063 hours.\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/ultralytics_run9/weights/last.pt, 51.1MB\n",
      "Optimizer stripped from /home/bachelor/ml-carbucks/runs/detect/ultralytics_run9/weights/best.pt, 51.1MB\n",
      "\n",
      "Validating /home/bachelor/ml-carbucks/runs/detect/ultralytics_run9/weights/best.pt...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 27.8it/s 1.8s0.1s\n",
      "                   all        810       1406      0.481      0.385      0.393      0.215\n",
      "Speed: 0.0ms preprocess, 1.1ms inference, 0.0ms loss, 0.4ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/ultralytics_run9\u001b[0m\n",
      "INFO ml_carbucks.adapters.UltralyticsAdapter 21:59:50 | Starting evaluation...\n",
      "Ultralytics 8.3.217 üöÄ Python-3.12.3 torch-2.8.0+cu128 CUDA:0 (NVIDIA RTX 4000 SFF Ada Generation, 20154MiB)\n",
      "YOLO11l summary (fused): 190 layers, 25,281,625 parameters, 0 gradients, 86.6 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access ‚úÖ (ping: 0.0¬±0.0 ms, read: 8543.5¬±1594.5 MB/s, size: 751.1 KB)\n",
      "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /home/bachelor/ml-carbucks/data/car_dd/labels/val.cache... 633 images, 177 backgrounds, 0 corrupt: 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 810/810 3.0Mit/s 0.0s0s\n",
      "\u001b[K                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100% ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 51/51 17.7it/s 2.9s0.1s\n",
      "                   all        810       1406       0.48      0.384      0.393      0.214\n",
      "               scratch        431        728      0.604      0.394      0.459       0.25\n",
      "                  dent        352        501      0.567      0.531      0.538      0.301\n",
      "                 crack        122        177       0.27      0.226      0.182     0.0926\n",
      "Speed: 0.1ms preprocess, 2.3ms inference, 0.0ms loss, 0.3ms postprocess per image\n",
      "Results saved to \u001b[1m/home/bachelor/ml-carbucks/runs/detect/val4\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from ml_carbucks.adapters.UltralyticsAdapter import UltralyticsAdapter\n",
    "\n",
    "umodel = UltralyticsAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"model_type\": \"yolo\",\n",
    "        \"model_version\": \"yolo11l.pt\"\n",
    "    },\n",
    "    hparams={\n",
    "        \"imgsz\": 320,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"epochs\": 10,\n",
    "        \"batch\": 8,\n",
    "        \"lr0\": 0.00029631881419241645,\n",
    "        \"momentum\": 0.38243835004885135,\n",
    "        \"weight_decay\": 9.16499123351809e-05,\n",
    "        \"patience\": 31,\n",
    "    },\n",
    "    datasets={\n",
    "        \"data_yaml\": \"/home/bachelor/ml-carbucks/data/car_dd/dataset.yaml\"\n",
    "    }\n",
    ")\n",
    "\n",
    "umodel.fit()\n",
    "res = umodel.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48a83d04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Loading Faster R-CNN model...\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Loading datasets...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | COCO category IDs contain 0, which is reserved for background.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Make sure that it is properly handled in your dataset.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Incrementing all category IDs by 1.\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | COCO category IDs contain 0, which is reserved for background.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Make sure that it is properly handled in your dataset.\n",
      "WARNING ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Incrementing all category IDs by 1.\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Starting training...\n",
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:21:48 | Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bachelor/ml-carbucks/.venv/lib/python3.12/site-packages/albumentations/core/validation.py:114: UserWarning: ShiftScaleRotate is a special case of Affine transform. Please use Affine transform instead.\n",
      "  original_init(self, **validated_kwargs)\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [03:59<00:00,  1.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:25:47 | Epoch 2/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [04:02<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:29:49 | Epoch 3/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [04:02<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:33:52 | Epoch 4/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [04:02<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:37:54 | Epoch 5/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [04:02<00:00,  1.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.FasterRcnnAdapter 22:41:57 | Starting evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'map_50_95'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m      4\u001b[39m fmodel = FasterRcnnAdapter(\n\u001b[32m      5\u001b[39m     classes=[\u001b[33m\"\u001b[39m\u001b[33mscratch\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdent\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcrack\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m      6\u001b[39m     hparams={\n\u001b[32m   (...)\u001b[39m\u001b[32m     16\u001b[39m     }\n\u001b[32m     17\u001b[39m )\n\u001b[32m     19\u001b[39m fmodel.fit()\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[43mfmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/home/bachelor/ml-carbucks/ml_carbucks/adapters/FasterRcnnAdapter.py:174\u001b[39m, in \u001b[36mFasterRcnnAdapter.evaluate\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    168\u001b[39m         metric.update(outputs_cpu, targets_cpu)\n\u001b[32m    170\u001b[39m results = metric.compute()\n\u001b[32m    172\u001b[39m metrics = {\n\u001b[32m    173\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmap_50\u001b[39m\u001b[33m\"\u001b[39m: results[\u001b[33m\"\u001b[39m\u001b[33mmap_50\u001b[39m\u001b[33m\"\u001b[39m].item(),\n\u001b[32m--> \u001b[39m\u001b[32m174\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmap_50_95\u001b[39m\u001b[33m\"\u001b[39m: \u001b[43mresults\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmap_50_95\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m.item(),\n\u001b[32m    175\u001b[39m }\n\u001b[32m    177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m metrics\n",
      "\u001b[31mKeyError\u001b[39m: 'map_50_95'"
     ]
    }
   ],
   "source": [
    "from ml_carbucks.adapters.FasterRcnnAdapter import FasterRcnnAdapter\n",
    "from ml_carbucks import DATA_CAR_DD_DIR\n",
    "\n",
    "fmodel = FasterRcnnAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    hparams={\n",
    "        \"batch_size\": 8,\n",
    "        \"img_size\": 320,\n",
    "        \"epochs\": 5\n",
    "    },\n",
    "    datasets={\n",
    "        \"train_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"train\",\n",
    "        \"train_ann_file\": DATA_CAR_DD_DIR / \"instances_train.json\",\n",
    "        \"val_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"val\",\n",
    "        \"val_ann_file\": DATA_CAR_DD_DIR / \"instances_val.json\",\n",
    "    }\n",
    ")\n",
    "\n",
    "fmodel.fit()\n",
    "fmodel.evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58cc7b4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:01:00 | Loading EfficientDet model...\n",
      "loading annotations into memory...\n",
      "Done (t=0.01s)\n",
      "creating index...\n",
      "index created!\n",
      "loading annotations into memory...\n",
      "Done (t=0.00s)\n",
      "creating index...\n",
      "index created!\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:01:00 | Starting training...\n",
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:01:00 | Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:49<00:00,  7.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:01:50 | Epoch 2/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  7.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:02:40 | Epoch 3/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:03:30 | Epoch 4/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:04:21 | Epoch 5/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:05:12 | Epoch 6/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:06:02 | Epoch 7/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:06:53 | Epoch 8/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:07:44 | Epoch 9/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO ml_carbucks.adapters.EfficientDetAdapter 22:08:34 | Epoch 10/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 352/352 [00:50<00:00,  6.95it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and preparing results...\n",
      "DONE (t=0.31s)\n",
      "creating index...\n",
      "index created!\n",
      "Running per image evaluation...\n",
      "Evaluate annotation type *bbox*\n",
      "DONE (t=0.91s).\n",
      "Accumulating evaluation results...\n",
      "DONE (t=0.26s).\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.167\n",
      " Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.366\n",
      " Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.137\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.000\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.033\n",
      " Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.201\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.187\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.345\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.400\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = 0.005\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = 0.247\n",
      " Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'map_50': np.float64(0.3658111423431102),\n",
       " 'map_50_95': np.float64(0.16692027192806946)}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ml_carbucks.adapters.EfficientDetAdapter import EfficientDetAdapter\n",
    "from ml_carbucks import DATA_CAR_DD_DIR\n",
    "\n",
    "emodel = EfficientDetAdapter(\n",
    "    classes=[\"scratch\", \"dent\", \"crack\"],\n",
    "    metadata={\n",
    "        \"model_version\": \"tf_efficientdet_d0\"\n",
    "    },\n",
    "    hparams={\n",
    "        \"batch_size\": 8,\n",
    "        \"epochs\": 10,\n",
    "        \"lr\": 0.009,\n",
    "        \"opt\": \"momentum\",\n",
    "        \"weight_decay\": 1e-5,\n",
    "    },\n",
    "    datasets={\n",
    "        \"train_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"train\",\n",
    "        \"train_ann_file\": DATA_CAR_DD_DIR / \"instances_train_curated.json\",\n",
    "        \"val_img_dir\": DATA_CAR_DD_DIR / \"images\" / \"val\",\n",
    "        \"val_ann_file\": DATA_CAR_DD_DIR / \"instances_val_curated.json\",\n",
    "    }\n",
    ")\n",
    "\n",
    "emodel.fit()\n",
    "emodel.evaluate()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml-carbucks-py3.12 (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
