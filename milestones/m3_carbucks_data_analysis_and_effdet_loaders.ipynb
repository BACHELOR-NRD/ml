{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b48705d",
   "metadata": {},
   "source": [
    "## Imports and setups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce4a588",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "from pprint import pprint as print\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Literal, Optional, Tuple\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from torchmetrics.detection.mean_ap import MeanAveragePrecision\n",
    "from effdet import create_model, create_loader\n",
    "from effdet.data import resolve_input_config, resolve_fill_color\n",
    "from effdet.bench import DetBenchPredict  # noqa F401\n",
    "from effdet.data.transforms import ResizePad, ImageToNumpy, Compose\n",
    "from timm.optim._optim_factory import create_optimizer_v2\n",
    "\n",
    "from ml_carbucks.utils.preprocessing import create_clean_loader, create_transforms, preprocess_images\n",
    "from ml_carbucks.utils.inference import plot_img_pred_subplots as psp\n",
    "from ml_carbucks import DATA_DIR, RESULTS_DIR\n",
    "from ml_carbucks.utils.postprocessing import (\n",
    "    postprocess_prediction_nms,\n",
    "    postprocess_evaluation_results,\n",
    ")\n",
    "from ml_carbucks.utils.result_saver import ResultSaver\n",
    "from ml_carbucks.adapters.BaseDetectionAdapter import (\n",
    "    ADAPTER_METRICS,\n",
    "    BaseDetectionAdapter,\n",
    "    ADAPTER_PREDICTION,\n",
    ")\n",
    "from ml_carbucks.patches.effdet import (\n",
    "    CocoStatsEvaluator,\n",
    "    ConcatDetectionDataset,\n",
    "    create_dataset_custom,\n",
    ")\n",
    "from ml_carbucks.utils.conversions import convert_yolo_to_coco\n",
    "from ml_carbucks.utils.logger import setup_logger\n",
    "\n",
    "\n",
    "logger = setup_logger(__name__)\n",
    "\n",
    "\n",
    "DATASET_TUPLE_TYPE = tuple[str | Path, str | Path]\n",
    "\n",
    "classes = [\"scratch\", \"dent\", \"crack\"]\n",
    "\n",
    "cardd_dataset_train: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"car_dd\" / \"images\" / \"train\",\n",
    "    DATA_DIR / \"car_dd\" / \"annotations\" /\"instances_train_curated.json\",\n",
    ")\n",
    "cardd_dataset_val: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"car_dd\" / \"images\" / \"val\",\n",
    "    DATA_DIR / \"car_dd\" / \"annotations\" /\"instances_val_curated.json\",\n",
    ")\n",
    "\n",
    "carbucks_raw_dataset_train: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks\" / \"images\" / \"train\",\n",
    "    DATA_DIR / \"carbucks\" / \"instances_train_curated.json\",\n",
    ")\n",
    "carbucks_raw_dataset_val: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks\" / \"images\" / \"val\",\n",
    "    DATA_DIR / \"carbucks\" / \"instances_val_curated.json\",\n",
    ")\n",
    "carbucks_clean_dataset_train: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks_cleaned\" / \"images\" / \"train\",\n",
    "    DATA_DIR / \"carbucks_cleaned\" / \"instances_train_curated.json\",\n",
    ")\n",
    "carbucks_clean_dataset_val: DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks_cleaned\" / \"images\" / \"val\",\n",
    "    DATA_DIR / \"carbucks_cleaned\" / \"instances_val_curated.json\",\n",
    ")\n",
    "carbucks_balanced_dataset_train : DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks_balanced\" / \"images\" / \"train\",\n",
    "    DATA_DIR / \"carbucks_balanced\" / \"instances_train_curated.json\",\n",
    ")\n",
    "carbucks_balanced_dataset_val : DATASET_TUPLE_TYPE = (\n",
    "    DATA_DIR / \"carbucks_balanced\" / \"images\" / \"val\",\n",
    "    DATA_DIR / \"carbucks_balanced\" / \"instances_val_curated.json\",\n",
    ")\n",
    "\n",
    "mvp = {}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccee12e8",
   "metadata": {},
   "source": [
    "# Carbucks intial exploration "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3ab2bf",
   "metadata": {},
   "source": [
    "## Displaying of carbucks dataset (sample) with its annotations\n",
    "At the time of compute, the EXIF orientation tag was already fixed and re-saved.\n",
    "The issue was that some images had EXIF orientation tag that made them appear rotated (correctly) in some viewers. And thus annotations seemed okay. However, when reading images with libraries that do not respect EXIF orientation tag (like OpenCV), the images appeared rotated incorrectly with respect to annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e3684f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_sample_batches(datasets: list[DATASET_TUPLE_TYPE], cnt: int | None = None, skip: int | None = None, batch_size: int | None = None, exif_aware: bool = False):\n",
    "    loader = create_clean_loader(\n",
    "        datasets=datasets,\n",
    "        batch_size=batch_size if batch_size is not None else random.randint(4,8),\n",
    "        shuffle=False,\n",
    "        transforms=None,\n",
    "        exif_aware=exif_aware\n",
    "    )\n",
    "    if cnt is None:\n",
    "        cnt = random.randint(3,8)\n",
    "    \n",
    "    if skip is None:\n",
    "        skip = random.randint(0,20)\n",
    "\n",
    "    print(f\"Showing {cnt} batches after skipping {skip} batches from loader with batch size {loader.batch_size}\")\n",
    "    for i, (imgs, targets) in enumerate(loader):\n",
    "        if i < skip:\n",
    "            continue\n",
    "        if i >= cnt + skip:\n",
    "            break\n",
    "\n",
    "        print(f\"Image batch {i}\")\n",
    "        img_tensors = [torch.from_numpy(img).permute(2, 0, 1) for img in imgs]\n",
    "        psp(\n",
    "            img_list=img_tensors,\n",
    "            bboxes_list=[t[\"boxes\"] for t in targets],\n",
    "            labels_list=[t[\"labels\"] for t in targets],\n",
    "            coords=\"xyxy\",\n",
    "            figsize=(10, 15)\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8c6aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_sample_batches(datasets=[carbucks_raw_dataset_train], cnt=1, skip=6, batch_size=10, exif_aware=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbde337d",
   "metadata": {},
   "source": [
    "## Make analysis on datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f49b5e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_counter(images_dir_root: str | Path, splits: list = [\"train\", \"val\"], normalize: bool = False):\n",
    "    counter = {\"all\": {}, \"img_counts\": {}}\n",
    "    for split in splits:\n",
    "        # load each file and count how many classes there are in each split and what is their distribution\n",
    "        split_dir = os.path.join(images_dir_root, split)\n",
    "        counter[split] = {}\n",
    "        for root, dirs, files in os.walk(split_dir):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".jpg\"):\n",
    "                    continue\n",
    "\n",
    "                file_path = os.path.join(root, file)\n",
    "                label_path = file_path.replace(\"images\", \"labels\").replace(\".jpg\", \".txt\")\n",
    "\n",
    "                counter[\"img_counts\"][split] = counter[\"img_counts\"].get(split, 0) + 1\n",
    "                counter[\"img_counts\"]['all'] = counter[\"img_counts\"].get('all', 0) + 1\n",
    "                if not os.path.exists(label_path):\n",
    "                    counter['all']['no_label'] = counter['all'].get(\"no_label\", 0) + 1\n",
    "                    counter[split][\"no_label\"] = counter[split].get(\"no_label\", 0) + 1\n",
    "                    continue\n",
    "\n",
    "                with open(label_path, \"r\") as f:\n",
    "                    if len(f.read().strip()) == 0:\n",
    "                        counter['all']['no_label'] = counter['all'].get(\"no_label\", 0) + 1\n",
    "                        counter[split][\"no_label\"] = counter[split].get(\"no_label\", 0) + 1\n",
    "                        continue\n",
    "                    f.seek(0)\n",
    "                    for line in f:\n",
    "                        class_id = line.strip().split()[0]\n",
    "                        counter[split][class_id] = counter[split].get(class_id, 0) + 1\n",
    "                        counter['all'][class_id] = counter['all'].get(class_id, 0) + 1\n",
    "\n",
    "    if normalize:\n",
    "        counter_normalized = deepcopy(counter)\n",
    "        for split in splits + ['all']:\n",
    "            total = sum(counter_normalized[split].values())\n",
    "            for class_id in counter_normalized[split]:\n",
    "                counter_normalized[split][class_id] = round(counter_normalized[split][class_id] / total * 100, 4) \n",
    "\n",
    "            counter_normalized[split] = dict(sorted(counter_normalized[split].items(), key=lambda item: (item[0] != \"no_label\", int(item[0]) if item[0] != \"no_label\" else -1)))\n",
    "        return counter_normalized\n",
    "    else:\n",
    "        for split in splits + ['all']:\n",
    "            counter[split] = dict(sorted(counter[split].items(), key=lambda item: (item[0] != \"no_label\", int(item[0]) if item[0] != \"no_label\" else -1)))\n",
    "    return counter\n",
    "\n",
    "def visualize_counter(counter:dict, split: str = 'all', counter_name: str = \"\"):\n",
    "    plt.bar(\n",
    "        x=list(counter[split].keys()),\n",
    "        height=[v for v in counter[split].values()]\n",
    "    )\n",
    "    plt.xlabel(\"Class ID\")\n",
    "    plt.ylabel(\"Proportion\")\n",
    "    # write actual numbers on top of bars\n",
    "    for i, v in enumerate(counter[split].values()):\n",
    "        plt.text(i, v + 0.5, str(v), ha='center')\n",
    "    plt.title(f\"Class Distribution in '{split}' Split for {counter_name} Dataset\")\n",
    "    plt.show()\n",
    "\n",
    "def display_dataset_analysis(images_dir_root: str | Path, splits: list = [\"train\", \"val\"], counter_name: str = \"\", normalize: bool = False, visualize_splits: list = ['all']):\n",
    "    counter = create_counter(images_dir_root, splits, normalize)\n",
    "    print(f\"Dataset Analysis for {counter_name} Dataset:\")\n",
    "    print(counter)\n",
    "    for split in visualize_splits:\n",
    "        visualize_counter(counter, split, counter_name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9562b6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_dataset_analysis(carbucks_raw_dataset_train[0].parent, counter_name=\"Carbucks Dataset\") # type: ignore\n",
    "display_dataset_analysis(cardd_dataset_train[0].parent, counter_name=\"CardDD Dataset\") # type: ignore\n",
    "display_dataset_analysis(carbucks_raw_dataset_train[0].parent, counter_name=\"Carbucks Normalized Dataset\", normalize=True, visualize_splits=[\"all\"]) # type: ignore\n",
    "display_dataset_analysis(cardd_dataset_train[0].parent, counter_name=\"CardDD Normalized Dataset\", normalize=True, visualize_splits=[\"all\"]) # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59fec94e",
   "metadata": {},
   "source": [
    "## Dataset manipulation : removing no_label images and balancing\n",
    "\n",
    "Those **functions** might be potentially **destructive**, so make sure you know how to use them **Before** applying them to your dataset.\n",
    "\n",
    "It will not delete those files but **move them** to an **\"empty\" folder** within the dataset directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7d66c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_empty_labels(dataset_dir: str | Path, splits: list):\n",
    "    print(f\"Cleaning up empty labels in dataset at: {dataset_dir}\")\n",
    "    for split in splits:\n",
    "        for root, dirs, files in os.walk(Path(dataset_dir) / \"images\" / split):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".jpg\"):\n",
    "                    continue\n",
    "                \n",
    "                img_file_path = os.path.join(root, file)\n",
    "                label_file_path = img_file_path.replace(\".jpg\", \".txt\").replace(\"images\", \"labels\")\n",
    "                img_name = file\n",
    "                label_name = img_name.replace(\".jpg\", \".txt\")\n",
    "\n",
    "                if not os.path.exists(label_file_path):\n",
    "                    print(f\"Found image with no corresponding label file: {img_file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"empty\", split), exist_ok=True)\n",
    "                    # move image file\n",
    "                    new_img_path = os.path.join(dataset_dir, \"images\", \"empty\", split, file)\n",
    "                    os.rename(img_file_path, new_img_path)\n",
    "                    continue\n",
    "\n",
    "                with open(label_file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                if len(lines) == 0:\n",
    "                    print(f\"Found empty label file: {label_file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"empty\", split), exist_ok=True)\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"labels\", \"empty\", split), exist_ok=True)\n",
    "                    # move label file\n",
    "                    new_label_path = os.path.join(dataset_dir, \"labels\", \"empty\", split, label_name)\n",
    "                    os.rename(label_file_path, new_label_path)\n",
    "                    # move image file\n",
    "                    new_img_path = os.path.join(dataset_dir, \"images\", \"empty\", split, img_name)\n",
    "                    os.rename(img_file_path, new_img_path)\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=dataset_dir,\n",
    "        splits=splits,\n",
    "    )\n",
    "                    \n",
    "\n",
    "def balance_dataset(dataset_dir: str | Path, splits: list, remove_class_probabilities: dict[str, float] | None = None):\n",
    "    for split in splits:\n",
    "        files_moved_cnt = {class_id: 0 for class_id in remove_class_probabilities.keys()} if remove_class_probabilities else {}\n",
    "        for root, dirs, files in os.walk(Path(dataset_dir) / \"labels\" / split):\n",
    "            for file in files:\n",
    "                if not file.endswith(\".txt\"):\n",
    "                    continue\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, \"r\") as f:\n",
    "                    lines = f.readlines()\n",
    "                \n",
    "                class_labels = set()\n",
    "                for line in lines:\n",
    "                    class_id = line.strip().split()[0]\n",
    "                    class_labels.add(class_id)\n",
    "\n",
    "                # we want to move the files only that have pure one class labels, not mixed\n",
    "                if len(class_labels) != 1:\n",
    "                    continue\n",
    "\n",
    "                class_id = class_labels.pop()\n",
    "                move_file = False\n",
    "                if remove_class_probabilities and class_id in remove_class_probabilities:\n",
    "                    prob = remove_class_probabilities[class_id]\n",
    "                    if random.random() <= prob:\n",
    "                        move_file = True\n",
    "\n",
    "                if move_file:\n",
    "                    files_moved_cnt[class_id] += 1\n",
    "                    print(f\"Moving pure class {class_id} label file: {file_path}\")\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"images\", \"balancing\", split), exist_ok=True)\n",
    "                    os.makedirs(os.path.join(dataset_dir, \"labels\", \"balancing\", split), exist_ok=True)\n",
    "                    # move label file\n",
    "                    new_label_path = os.path.join(dataset_dir, \"labels\", \"balancing\", split, file)\n",
    "                    os.rename(file_path, new_label_path)\n",
    "                    # move image file\n",
    "                    img_file = file.replace(\".txt\", \".jpg\")\n",
    "                    img_path = os.path.join(dataset_dir, \"images\", split, img_file)\n",
    "                    if os.path.exists(img_path):\n",
    "                        new_img_path = os.path.join(dataset_dir, \"images\", \"balancing\", split, img_file)\n",
    "                        os.rename(img_path, new_img_path)\n",
    "                    else:\n",
    "                        print(f\"⚠️ Corresponding image file not found for label: {file_path}\")\n",
    "        if files_moved_cnt:\n",
    "            print(f\"Moved files for {split}: {files_moved_cnt}\")\n",
    "\n",
    "\n",
    "    convert_yolo_to_coco(\n",
    "        base_dir=dataset_dir,\n",
    "        splits=splits,\n",
    "    )\n",
    "       \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d64dbb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: this function will remove \"no annotations files\" and move them to \"empty\" folder within the dataset\n",
    "clean_up_empty_labels(carbucks_clean_dataset_train[0].parent.parent, splits=[\"train\", \"val\"]) # type: ignore       \n",
    "\n",
    "display_dataset_analysis(carbucks_clean_dataset_train[0].parent, counter_name=\"Carbucks Cleaned Dataset\", normalize=True) # type: ignore\n",
    "\n",
    "\n",
    "# NOTE: settings 0.0 will not remove any files, adjust as needed and move them to \"balancing\" folder\n",
    "balance_dataset(\n",
    "    dataset_dir=carbucks_balanced_dataset_train[0].parent.parent, # type: ignore\n",
    "    splits=[], # <- adjust splits as needed (\"train\", \"val\" or both)\n",
    "    remove_class_probabilities={\n",
    "        \"0\": 0.0, # <- this can be adjusted to remove some, it removes the % of pure class images\n",
    "        \"1\": 0.0, # <-\n",
    "        \"2\": 0.0, # <-\n",
    "    }\n",
    ")\n",
    "\n",
    "display_dataset_analysis(carbucks_balanced_dataset_train[0].parent, counter_name=\"Carbucks Balanced Dataset\", normalize=True, visualize_splits=[\"train\", \"val\"]) # type: ignore\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3720f3e7",
   "metadata": {},
   "source": [
    "# Carbucks secondary exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1470724d",
   "metadata": {},
   "source": [
    "## Effdet with in-built loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fcc97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EfficientDetAdapterInbuildLoader(BaseDetectionAdapter):\n",
    "\n",
    "    weights: str | Path = \"\"\n",
    "    backbone: str = \"tf_efficientdet_d0\"\n",
    "\n",
    "    optimizer: str = \"momentum\"\n",
    "    lr: float = 8e-3\n",
    "    weight_decay: float = 9e-6\n",
    "    confidence_threshold: float = 0.15\n",
    "    training_augmentations: bool = True\n",
    "\n",
    "    def save(self, dir: Path | str, prefix: str = \"\", suffix: str = \"\") -> Path:\n",
    "        save_path = Path(dir) / f\"{prefix}model{suffix}.pth\"\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.model.model.state_dict(), save_path)\n",
    "        return save_path\n",
    "\n",
    "    def _preprocess_images(\n",
    "        self, images: List[np.ndarray]\n",
    "    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        fill_color = resolve_fill_color(\n",
    "            input_config[\"fill_color\"], input_config[\"mean\"]\n",
    "        )\n",
    "\n",
    "        mean = (\n",
    "            torch.tensor(input_config[\"mean\"], device=self.device).view(3, 1, 1) * 255\n",
    "        )\n",
    "        std = torch.tensor(input_config[\"std\"], device=self.device).view(3, 1, 1) * 255\n",
    "\n",
    "        transform = Compose(\n",
    "            [\n",
    "                ResizePad(\n",
    "                    target_size=self.img_size,\n",
    "                    interpolation=input_config[\"interpolation\"],\n",
    "                    fill_color=fill_color,\n",
    "                ),\n",
    "                ImageToNumpy(),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        batch, scales = [], []\n",
    "\n",
    "        for img in images:\n",
    "\n",
    "            img_np, anno = transform(Image.fromarray(img), {})\n",
    "            scales.append(anno.get(\"img_scale\", 1.0))\n",
    "\n",
    "            img_norm = (\n",
    "                torch.from_numpy(img_np)\n",
    "                .to(self.device, non_blocking=True)\n",
    "                .float()\n",
    "                .sub_(mean)\n",
    "                .div_(std)\n",
    "            )\n",
    "            batch.append(img_norm)\n",
    "\n",
    "        batch_tensor = torch.stack(batch, dim=0)  # B,C,H,W\n",
    "        scales_tensor = torch.tensor(scales, dtype=torch.float32, device=self.device)\n",
    "        batch_original_sizes = torch.tensor(\n",
    "            [[img.shape[1], img.shape[0]] for img in images],\n",
    "            dtype=torch.float32,\n",
    "            device=self.device,\n",
    "        )\n",
    "\n",
    "        return batch_tensor, scales_tensor, batch_original_sizes\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        images: List[np.ndarray],\n",
    "        conf_threshold: float = 0.15,\n",
    "        iou_threshold: float = 1.0,\n",
    "        max_detections: int = 10,\n",
    "    ) -> List[ADAPTER_PREDICTION]:\n",
    "        \"\"\"\n",
    "        The issue is that predicitons are weird but the results of the evaluation\n",
    "        are good. So either the evaluation is wrong or the prediction extraction is wrong.\n",
    "        \"\"\"\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()\n",
    "        predictions: List[ADAPTER_PREDICTION] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            batch_tensor, batch_scales, batch_original_sizes = self._preprocess_images(\n",
    "                images\n",
    "            )\n",
    "\n",
    "            # NOTE: Passing the img_info dict will allow to get the predictions in original image scale\n",
    "            img_info_dict = {\n",
    "                \"img_scale\": batch_scales,\n",
    "                \"img_size\": batch_original_sizes,\n",
    "            }\n",
    "\n",
    "            outputs = predictor(batch_tensor, img_info=img_info_dict)\n",
    "\n",
    "            for i, pred in enumerate(outputs):\n",
    "                boxes = pred[:, :4]\n",
    "                scores = pred[:, 4]\n",
    "                labels_idx = pred[:, 5]\n",
    "\n",
    "                prediction = postprocess_prediction_nms(\n",
    "                    boxes,\n",
    "                    scores,\n",
    "                    labels_idx,\n",
    "                    conf_threshold,\n",
    "                    iou_threshold,\n",
    "                    max_detections,\n",
    "                )\n",
    "\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def setup(self) -> \"EfficientDetAdapterInbuildLoader\":\n",
    "        img_size = self.img_size\n",
    "\n",
    "        backbone = self.backbone\n",
    "        weights = self.weights\n",
    "\n",
    "        extra_args = dict(image_size=(img_size, img_size))\n",
    "        self.model = create_model(\n",
    "            model_name=backbone,\n",
    "            bench_task=\"train\",\n",
    "            num_classes=len(self.classes),\n",
    "            pretrained=weights == \"\",\n",
    "            checkpoint_path=str(weights),\n",
    "            # NOTE: we set it to True because we are using custom Mean Average Precision and it is easier that way\n",
    "            # custom anchor labeler would be good idea if the boxes had unusual sizes and aspect ratios -> worth remembering for future\n",
    "            bench_labeler=True,\n",
    "            checkpoint_ema=False,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> \"EfficientDetAdapterInbuildLoader\":\n",
    "        logger.info(\"Starting training...\")\n",
    "        self.model.train()\n",
    "\n",
    "        epochs = self.epochs\n",
    "        opt = self.optimizer\n",
    "        lr = self.lr\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        train_loader = self._create_loader(datasets, is_training=True)\n",
    "\n",
    "        parser_max_label = train_loader.dataset.parsers[0].max_label  # type: ignore\n",
    "        config_num_classes = self.model.config.num_classes\n",
    "\n",
    "        if parser_max_label != config_num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Number of classes in dataset ({parser_max_label}) does not match \"\n",
    "                f\"model config ({config_num_classes}).\"\n",
    "                f\"Please verify that the dataset is curated (classes IDs start from 1)\"\n",
    "            )\n",
    "\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=opt,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            _ = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _create_loader(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]], is_training: bool\n",
    "    ):\n",
    "        batch_size = self.batch_size\n",
    "\n",
    "        all_datasets = []\n",
    "        for img_dir, ann_file in datasets:\n",
    "            dataset = create_dataset_custom(\n",
    "                img_dir=img_dir,\n",
    "                ann_file=ann_file,\n",
    "                has_labels=True,\n",
    "            )\n",
    "            all_datasets.append(dataset)\n",
    "\n",
    "        concat_dataset = ConcatDetectionDataset(all_datasets)\n",
    "\n",
    "        if is_training and (not self.training_augmentations):\n",
    "            logger.warning(\n",
    "                \"Data augmentations are disabled. This may worsen model performance. It should only be used for debugging purposes.\"\n",
    "            )\n",
    "\n",
    "        input_config = resolve_input_config(self.get_params(), self.model.config)\n",
    "        loader = create_loader(\n",
    "            concat_dataset,\n",
    "            input_size=input_config[\"input_size\"],\n",
    "            batch_size=batch_size,\n",
    "            is_training=is_training and self.training_augmentations,\n",
    "            use_prefetcher=True,\n",
    "            interpolation=input_config[\"interpolation\"],\n",
    "            fill_color=input_config[\"fill_color\"],\n",
    "            mean=input_config[\"mean\"],\n",
    "            std=input_config[\"std\"],\n",
    "            num_workers=4,\n",
    "            distributed=False,\n",
    "            pin_mem=False,\n",
    "            anchor_labeler=None,\n",
    "            transform_fn=None,\n",
    "            collate_fn=None,\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def train_epoch(\n",
    "        self, optimizer: torch.optim.Optimizer, loader: DataLoader\n",
    "    ) -> float:\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            output = self.model(imgs, targets)\n",
    "            loss = output[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def debug(\n",
    "        self,\n",
    "        train_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        val_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        results_path: str | Path,\n",
    "        results_name: str,\n",
    "        visualize: Literal[\"every\", \"last\", \"none\"] = \"none\",\n",
    "    ) -> ADAPTER_METRICS:\n",
    "        logger.info(\"Debugging training and evaluation loops...\")\n",
    "\n",
    "        epochs = self.epochs\n",
    "        train_loader = self._create_loader(train_datasets, is_training=True)\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=self.optimizer,\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        saver = ResultSaver(\n",
    "            path=results_path,\n",
    "            name=results_name,\n",
    "        )\n",
    "        val_metrics: Optional[ADAPTER_METRICS] = None\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            total_loss = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "            val_metrics = self.evaluate(val_datasets)\n",
    "            saver.save(\n",
    "                epoch=epoch,\n",
    "                loss=total_loss,\n",
    "                val_map=val_metrics[\"map_50_95\"],\n",
    "                val_map_50=val_metrics[\"map_50\"],\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Debug Epoch {epoch}/{epochs} - Loss: {total_loss}, Val MAP: {val_metrics['map_50_95']}\"\n",
    "            )\n",
    "\n",
    "            show = False\n",
    "            if visualize == \"every\":\n",
    "                show = True\n",
    "            elif visualize == \"last\" and epoch == epochs:\n",
    "                show = True\n",
    "            saver.plot(show=show)\n",
    "\n",
    "        if val_metrics is None:\n",
    "            raise RuntimeError(\"Validation metrics were not computed during debugging.\")\n",
    "        return val_metrics\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        datasets: List[Tuple[str | Path, str | Path]],\n",
    "        include_default: bool = False,\n",
    "    ) -> ADAPTER_METRICS:\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loader = self._create_loader(datasets, is_training=False)\n",
    "\n",
    "        default_evaluator = None\n",
    "        if include_default:\n",
    "            default_evaluator = CocoStatsEvaluator(dataset=val_loader.dataset)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(extended_summary=False, class_metrics=False)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                output = self.model(imgs, targets)\n",
    "                loss = output[\"loss\"]\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if include_default and default_evaluator is not None:\n",
    "                    default_evaluator.add_predictions(\n",
    "                        detections=output[\"detections\"], target=targets\n",
    "                    )\n",
    "\n",
    "                # NOTE:\n",
    "                # Annotations are loaded in yxyx format and they are scaled\n",
    "                # Predicitons are in xyxy format and not scaled (original size of the image)\n",
    "                # So we need to rescale the ground truth boxes to original sizes\n",
    "                # Predicitons have a lot of low confidence scores and ground_truths have a lot of -1 values that just indicate no object\n",
    "                # We need to filter them out\n",
    "                for i in range(len(imgs)):\n",
    "                    scale = targets[\"img_scale\"][i]\n",
    "\n",
    "                    pred_mask = (\n",
    "                        output[\"detections\"][i][:, 4] >= self.confidence_threshold\n",
    "                    )\n",
    "                    if pred_mask.sum() == 0:\n",
    "                        # No predcitions above the confidence threshold\n",
    "                        pred_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        pred_scores = torch.zeros((0,), dtype=torch.float32)\n",
    "                        pred_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        pred_boxes = output[\"detections\"][i][pred_mask, :4]\n",
    "                        pred_scores = output[\"detections\"][i][pred_mask, 4]\n",
    "                        pred_labels = output[\"detections\"][i][pred_mask, 5].long()\n",
    "\n",
    "                    gt_mask = targets[\"cls\"][i] != -1\n",
    "                    if gt_mask.sum() == 0:\n",
    "                        # No ground truth boxes\n",
    "                        gt_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        gt_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        gt_boxes_yxyx_raw = targets[\"bbox\"][i][gt_mask]\n",
    "                        gt_boxes_xyxy = torch.zeros_like(gt_boxes_yxyx_raw)\n",
    "                        gt_boxes_xyxy[:, 0] = gt_boxes_yxyx_raw[:, 1]\n",
    "                        gt_boxes_xyxy[:, 1] = gt_boxes_yxyx_raw[:, 0]\n",
    "                        gt_boxes_xyxy[:, 2] = gt_boxes_yxyx_raw[:, 3]\n",
    "                        gt_boxes_xyxy[:, 3] = gt_boxes_yxyx_raw[:, 2]\n",
    "                        gt_boxes = gt_boxes_xyxy * scale\n",
    "                        gt_labels = targets[\"cls\"][i][gt_mask].long()\n",
    "\n",
    "                    evaluator.update(\n",
    "                        preds=[\n",
    "                            {\n",
    "                                \"boxes\": pred_boxes.cpu(),\n",
    "                                \"scores\": pred_scores.cpu(),\n",
    "                                \"labels\": pred_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                        target=[\n",
    "                            {\n",
    "                                \"boxes\": gt_boxes.cpu(),\n",
    "                                \"labels\": gt_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "        results = evaluator.compute()\n",
    "        metrics = postprocess_evaluation_results(results)\n",
    "\n",
    "        if include_default and default_evaluator is not None:\n",
    "            default_results = default_evaluator.evaluate()\n",
    "            metrics[\"default_map_50_95\"] = default_results[0]  # type: ignore\n",
    "            metrics[\"default_map_50\"] = default_results[1]  # type: ignore\n",
    "\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f4cd9",
   "metadata": {},
   "source": [
    "## Effdet with custom loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a2d8b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class EfficientDetAdapterCustomLoader(BaseDetectionAdapter):\n",
    "\n",
    "    weights: str | Path = \"\"\n",
    "    backbone: str = \"tf_efficientdet_d0\"\n",
    "\n",
    "    optimizer: str = \"momentum\"\n",
    "    lr: float = 8e-3\n",
    "    weight_decay: float = 9e-6\n",
    "    confidence_threshold: float = 0.15\n",
    "    training_augmentations: bool = True\n",
    "\n",
    "    def save(self, dir: Path | str, prefix: str = \"\", suffix: str = \"\") -> Path:\n",
    "        save_path = Path(dir) / f\"{prefix}model{suffix}.pth\"\n",
    "        save_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(self.model.model.state_dict(), save_path)\n",
    "        return save_path\n",
    "\n",
    "\n",
    "    def predict(\n",
    "        self,\n",
    "        images: List[np.ndarray],\n",
    "        conf_threshold: float = 0.15,\n",
    "        iou_threshold: float = 1.0,\n",
    "        max_detections: int = 10,\n",
    "    ) -> List[ADAPTER_PREDICTION]:\n",
    "        \"\"\"\n",
    "        The issue is that predicitons are weird but the results of the evaluation\n",
    "        are good. So either the evaluation is wrong or the prediction extraction is wrong.\n",
    "        \"\"\"\n",
    "        predictor = DetBenchPredict(deepcopy(self.model.model))\n",
    "        predictor.to(self.device)\n",
    "        predictor.eval()\n",
    "        predictions: List[ADAPTER_PREDICTION] = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            preprocessed_images, scales, original_sizes = preprocess_images(\n",
    "                images, img_size=self.img_size\n",
    "            )\n",
    "\n",
    "            batch_img_tensor = torch.stack(preprocessed_images).to(self.device)\n",
    "            batch_scales = torch.tensor(scales, dtype=torch.float32, device=self.device)\n",
    "            batch_original_sizes = torch.tensor(original_sizes, dtype=torch.float32, device=self.device)\n",
    "\n",
    "            # NOTE: Passing the img_info dict will allow to get the predictions in original image scale\n",
    "            img_info_dict = {\n",
    "                \"img_scale\": batch_scales,\n",
    "                \"img_size\": batch_original_sizes,\n",
    "            }\n",
    "\n",
    "            outputs = predictor(batch_img_tensor, img_info=img_info_dict)\n",
    "\n",
    "            for i, pred in enumerate(outputs):\n",
    "                boxes = pred[:, :4]\n",
    "                scores = pred[:, 4]\n",
    "                labels_idx = pred[:, 5]\n",
    "\n",
    "                prediction = postprocess_prediction_nms(\n",
    "                    boxes,\n",
    "                    scores,\n",
    "                    labels_idx,\n",
    "                    conf_threshold,\n",
    "                    iou_threshold,\n",
    "                    max_detections,\n",
    "                )\n",
    "\n",
    "                predictions.append(prediction)\n",
    "\n",
    "        return predictions\n",
    "\n",
    "    def setup(self) -> \"EfficientDetAdapterCustomLoader\":\n",
    "        img_size = self.img_size\n",
    "\n",
    "        backbone = self.backbone\n",
    "        weights = self.weights\n",
    "\n",
    "        extra_args = dict(image_size=(img_size, img_size))\n",
    "        self.model = create_model(\n",
    "            model_name=backbone,\n",
    "            bench_task=\"train\",\n",
    "            num_classes=len(self.classes),\n",
    "            pretrained=weights == \"\",\n",
    "            checkpoint_path=str(weights),\n",
    "            # NOTE: we set it to True because we are using custom Mean Average Precision and it is easier that way\n",
    "            # custom anchor labeler would be good idea if the boxes had unusual sizes and aspect ratios -> worth remembering for future\n",
    "            bench_labeler=True,\n",
    "            checkpoint_ema=False,\n",
    "            **extra_args,\n",
    "        )\n",
    "\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def fit(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]]\n",
    "    ) -> \"EfficientDetAdapterCustomLoader\":\n",
    "        logger.info(\"Starting training...\")\n",
    "        self.model.train()\n",
    "\n",
    "        epochs = self.epochs\n",
    "        opt = self.optimizer\n",
    "        lr = self.lr\n",
    "        weight_decay = self.weight_decay\n",
    "\n",
    "        train_loader = self._create_loader(datasets, is_training=True)\n",
    "\n",
    "        parser_max_label = train_loader.dataset.parsers[0].max_label  # type: ignore\n",
    "        config_num_classes = self.model.config.num_classes\n",
    "\n",
    "        if parser_max_label != config_num_classes:\n",
    "            raise ValueError(\n",
    "                f\"Number of classes in dataset ({parser_max_label}) does not match \"\n",
    "                f\"model config ({config_num_classes}).\"\n",
    "                f\"Please verify that the dataset is curated (classes IDs start from 1)\"\n",
    "            )\n",
    "\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=opt,\n",
    "            lr=lr,\n",
    "            weight_decay=weight_decay,\n",
    "        )\n",
    "\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            _ = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _create_loader(\n",
    "        self, datasets: List[Tuple[str | Path, str | Path]], is_training: bool\n",
    "    ):\n",
    "        batch_size = self.batch_size\n",
    "        img_size = self.img_size\n",
    "        loader = create_clean_loader(\n",
    "            datasets,\n",
    "            shuffle=is_training and self.training_augmentations,\n",
    "            batch_size=batch_size,\n",
    "            transforms=create_transforms(\n",
    "                is_training=is_training and self.training_augmentations,\n",
    "                img_size=img_size,\n",
    "            ),\n",
    "            format=\"yxyx\",\n",
    "            placeholders=100,\n",
    "        )\n",
    "\n",
    "        return loader\n",
    "\n",
    "    def _convert_batch(\n",
    "        self, imgs: List[torch.Tensor], targets: List[dict]\n",
    "    ) -> Tuple[torch.Tensor, dict]:\n",
    "        imgs = list(img.to(self.device) for img in imgs)\n",
    "        targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "        imgs_tensor = torch.stack(imgs)\n",
    "        targets_dict = {\n",
    "            \"bbox\": torch.stack([t[\"boxes\"] for t in targets]),\n",
    "            \"cls\": torch.stack([t[\"labels\"] for t in targets]),\n",
    "            \"img_size\": torch.stack(\n",
    "                [t[\"img_size\"] for t in targets]),\n",
    "            \"img_scale\": torch.stack(\n",
    "                [t[\"img_scale\"] for t in targets]),\n",
    "        }\n",
    "        return imgs_tensor, targets_dict\n",
    "\n",
    "    def train_epoch(\n",
    "        self, optimizer: torch.optim.Optimizer, loader: DataLoader\n",
    "    ) -> float:\n",
    "        self.model.train()\n",
    "\n",
    "        total_loss = 0.0\n",
    "        for imgs, targets in tqdm(loader):\n",
    "            \n",
    "            cimgs, ctargets = self._convert_batch(imgs, targets)\n",
    "            output = self.model(cimgs, ctargets)\n",
    "            loss = output[\"loss\"]\n",
    "            total_loss += loss.item()\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        return total_loss\n",
    "\n",
    "    def debug(\n",
    "        self,\n",
    "        train_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        val_datasets: List[Tuple[str | Path, str | Path]],\n",
    "        results_path: str | Path,\n",
    "        results_name: str,\n",
    "        visualize: Literal[\"every\", \"last\", \"none\"] = \"none\",\n",
    "    ) -> ADAPTER_METRICS:\n",
    "        logger.info(\"Debugging training and evaluation loops...\")\n",
    "\n",
    "        epochs = self.epochs\n",
    "        train_loader = self._create_loader(train_datasets, is_training=True)\n",
    "        optimizer = create_optimizer_v2(\n",
    "            self.model,\n",
    "            opt=self.optimizer,\n",
    "            lr=self.lr,\n",
    "            weight_decay=self.weight_decay,\n",
    "        )\n",
    "        saver = ResultSaver(\n",
    "            path=results_path,\n",
    "            name=results_name,\n",
    "        )\n",
    "        val_metrics: Optional[ADAPTER_METRICS] = None\n",
    "        for epoch in range(1, epochs + 1):\n",
    "            logger.info(f\"Epoch {epoch}/{epochs}\")\n",
    "\n",
    "            total_loss = self.train_epoch(optimizer, train_loader)  # type: ignore\n",
    "            val_metrics = self.evaluate(val_datasets)\n",
    "            saver.save(\n",
    "                epoch=epoch,\n",
    "                loss=total_loss,\n",
    "                val_map=val_metrics[\"map_50_95\"],\n",
    "                val_map_50=val_metrics[\"map_50\"],\n",
    "            )\n",
    "            logger.info(\n",
    "                f\"Debug Epoch {epoch}/{epochs} - Loss: {total_loss}, Val MAP: {val_metrics['map_50_95']}\"\n",
    "            )\n",
    "\n",
    "            show = False\n",
    "            if visualize == \"every\":\n",
    "                show = True\n",
    "            elif visualize == \"last\" and epoch == epochs:\n",
    "                show = True\n",
    "            saver.plot(show=show)\n",
    "\n",
    "        if val_metrics is None:\n",
    "            raise RuntimeError(\"Validation metrics were not computed during debugging.\")\n",
    "        return val_metrics\n",
    "\n",
    "    def evaluate(\n",
    "        self,\n",
    "        datasets: List[Tuple[str | Path, str | Path]],\n",
    "        include_default: bool = False,\n",
    "    ) -> ADAPTER_METRICS:\n",
    "        self.model.eval()\n",
    "\n",
    "        val_loader = self._create_loader(datasets, is_training=False)\n",
    "\n",
    "        default_evaluator = None\n",
    "        if include_default:\n",
    "            default_evaluator = CocoStatsEvaluator(dataset=val_loader.dataset)\n",
    "\n",
    "        evaluator = MeanAveragePrecision(extended_summary=False, class_metrics=False)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for imgs, targets in val_loader:\n",
    "                cimgs, ctargets = self._convert_batch(imgs, targets)\n",
    "                output = self.model(cimgs, ctargets)\n",
    "                loss = output[\"loss\"]\n",
    "                total_loss += loss.item()\n",
    "\n",
    "                if include_default and default_evaluator is not None:\n",
    "                    default_evaluator.add_predictions(\n",
    "                        detections=output[\"detections\"], target=ctargets\n",
    "                    )\n",
    "\n",
    "                # NOTE:\n",
    "                # Annotations are loaded in yxyx format and they are scaled\n",
    "                # Predicitons are in xyxy format and not scaled (original size of the image)\n",
    "                # So we need to rescale the ground truth boxes to original sizes\n",
    "                # Predicitons have a lot of low confidence scores and ground_truths have a lot of -1 values that just indicate no object\n",
    "                # We need to filter them out\n",
    "                for i in range(len(cimgs)):\n",
    "                    scale = ctargets[\"img_scale\"][i]\n",
    "\n",
    "                    pred_mask = (\n",
    "                        output[\"detections\"][i][:, 4] >= self.confidence_threshold\n",
    "                    )\n",
    "                    if pred_mask.sum() == 0:\n",
    "                        # No predcitions above the confidence threshold\n",
    "                        pred_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        pred_scores = torch.zeros((0,), dtype=torch.float32)\n",
    "                        pred_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        pred_boxes = output[\"detections\"][i][pred_mask, :4]\n",
    "                        pred_scores = output[\"detections\"][i][pred_mask, 4]\n",
    "                        pred_labels = output[\"detections\"][i][pred_mask, 5].long()\n",
    "\n",
    "                    gt_mask = ctargets[\"cls\"][i] != -1\n",
    "                    if gt_mask.sum() == 0:\n",
    "                        # No ground truth boxes\n",
    "                        gt_boxes = torch.zeros((0, 4), dtype=torch.float32)\n",
    "                        gt_labels = torch.zeros((0,), dtype=torch.int64)\n",
    "                    else:\n",
    "                        gt_boxes_yxyx_raw = ctargets[\"bbox\"][i][gt_mask]\n",
    "                        gt_boxes_xyxy = torch.zeros_like(gt_boxes_yxyx_raw)\n",
    "                        gt_boxes_xyxy[:, 0] = gt_boxes_yxyx_raw[:, 1]\n",
    "                        gt_boxes_xyxy[:, 1] = gt_boxes_yxyx_raw[:, 0]\n",
    "                        gt_boxes_xyxy[:, 2] = gt_boxes_yxyx_raw[:, 3]\n",
    "                        gt_boxes_xyxy[:, 3] = gt_boxes_yxyx_raw[:, 2]\n",
    "                        gt_boxes = gt_boxes_xyxy * scale\n",
    "                        gt_labels = ctargets[\"cls\"][i][gt_mask].long()\n",
    "\n",
    "                    evaluator.update(\n",
    "                        preds=[\n",
    "                            {\n",
    "                                \"boxes\": pred_boxes.cpu(),\n",
    "                                \"scores\": pred_scores.cpu(),\n",
    "                                \"labels\": pred_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                        target=[\n",
    "                            {\n",
    "                                \"boxes\": gt_boxes.cpu(),\n",
    "                                \"labels\": gt_labels.cpu(),\n",
    "                            }\n",
    "                        ],\n",
    "                    )\n",
    "\n",
    "        results = evaluator.compute()\n",
    "        metrics = postprocess_evaluation_results(results)\n",
    "\n",
    "        if include_default and default_evaluator is not None:\n",
    "            default_results = default_evaluator.evaluate()\n",
    "            metrics[\"default_map_50_95\"] = default_results[0]  # type: ignore\n",
    "            metrics[\"default_map_50\"] = default_results[1]  # type: ignore\n",
    "\n",
    "        return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76d013c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "il_emodel = EfficientDetAdapterInbuildLoader(classes=classes, img_size=512, epochs=30).setup()\n",
    "cl_emodel = EfficientDetAdapterCustomLoader(classes=classes, img_size=512, epochs=30).setup()\n",
    "\n",
    "results_path = RESULTS_DIR / \"debug\"/ \"effdet_loading_comparison\"\n",
    "\n",
    "ilres = il_emodel.debug(train_datasets=[carbucks_raw_dataset_train], val_datasets=[carbucks_raw_dataset_val], results_path=results_path, results_name=\"inbuild_loader30\", visualize=\"last\") # type: ignore\n",
    "clres = cl_emodel.debug(train_datasets=[carbucks_raw_dataset_train], val_datasets=[carbucks_raw_dataset_val], results_path=results_path, results_name=\"custom_loader30\", visualize=\"last\") # type: ignore\n",
    "\n",
    "print(f\"EfficientDetAdapterInbuildLoader results: {ilres}\")\n",
    "print(f\"EfficientDetAdapterCustomLoader results: {clres}\")\n",
    "\n",
    "mvp[\"effdet_loaders30\"] = {\n",
    "    \"inbuild_loader30\": ilres[\"map_50_95\"],\n",
    "    \"custom_loader30\": clres[\"map_50_95\"],\n",
    "}\n",
    "\n",
    "# NOTE: cell outputs has been cleared to save space - refer to results/summary section at the bottom of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aaa3818",
   "metadata": {},
   "source": [
    "# Evaluation Demo for results and discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55056ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_experiment(\n",
    "    train_dataset: List[Tuple[str | Path, str | Path]],\n",
    "    val_dataset: List[Tuple[str | Path, str | Path]],\n",
    "    name: str,\n",
    ") -> ADAPTER_METRICS:\n",
    "\n",
    "    print(f\"Setting up experiment: {name}\")\n",
    "    adapter = EfficientDetAdapterCustomLoader(\n",
    "        classes=[\"scratch\", \"dent\", \"crack\"], img_size=512, epochs=10\n",
    "    ).setup()\n",
    "\n",
    "\n",
    "    res = adapter.debug(\n",
    "        train_dataset, # type: ignore\n",
    "        val_dataset, # type: ignore\n",
    "        results_path=RESULTS_DIR / \"debug\" / \"effdet_datasets_comparison\",\n",
    "        results_name=name,\n",
    "        visualize=\"last\",\n",
    "    )\n",
    "\n",
    "    print(f\"Experiment {name} results: {res}\")\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc30b5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# car_dd only\n",
    "exp1 = setup_experiment(\n",
    "    [cardd_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"1cardd-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# carbucks_train_raw\n",
    "exp2 = setup_experiment(\n",
    "    [carbucks_raw_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"2carbucks_raw-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# carbucks_train_clean\n",
    "exp3 = setup_experiment(\n",
    "    [carbucks_clean_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"3carbucks_clean-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# carbucks_train_balanced\n",
    "exp3half = setup_experiment(\n",
    "    [carbucks_balanced_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"3.5carbucks_balanced-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# car_dd + carbucks_raw\n",
    "exp4 = setup_experiment(\n",
    "    [cardd_dataset_train, carbucks_raw_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"4car_dd_plus_carbucks_raw-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# car_dd + carbucks_clean\n",
    "exp5 = setup_experiment(\n",
    "    [cardd_dataset_train, carbucks_clean_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"5car_dd_plus_carbucks_clean-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "# car_dd + carbucks_balanced\n",
    "exp6 = setup_experiment(\n",
    "    [cardd_dataset_train, carbucks_balanced_dataset_train],\n",
    "    [carbucks_raw_dataset_val],\n",
    "    name=\"6car_dd_plus_carbucks_balanced-carbucks_raw_val\",\n",
    ")\n",
    "\n",
    "\n",
    "mvp[\"effdet_datasets\"] = {\n",
    "    \"1cardd-carbucks_raw_val\": exp1[\"map_50_95\"],\n",
    "    \"2carbucks_raw-carbucks_raw_val\": exp2[\"map_50_95\"],\n",
    "    \"3carbucks_clean-carbucks_raw_val\": exp3[\"map_50_95\"],\n",
    "    \"4car_dd_plus_carbucks_raw-carbucks_raw_val\": exp4[\"map_50_95\"],\n",
    "    \"5car_dd_plus_carbucks_clean-carbucks_raw_val\": exp5[\"map_50_95\"],\n",
    "    \"6car_dd_plus_carbucks_balanced-carbucks_raw_val\": exp6[\"map_50_95\"],\n",
    "}\n",
    "\n",
    "# NOTE: cell outputs has been cleared to save space - refer to results/summary section at the bottom of the file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21c363b",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b400f15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: here are the results of the most valuable experiments\n",
    "\n",
    "print(\"Summary of mvps:\")\n",
    "for key, value in mvp.items():\n",
    "    print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb74064a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This milestone notebook covers the data analysis of Carbucks private dataset and EfficientDet data loaders comparison. The notebook includes visualizations of class distributions, dataset normalization, and performance comparisons between in-built and custom data loaders for EfficientDet as well as evaluation results of different datasets combinations.\n",
    "\n",
    "## Experiments\n",
    "\n",
    "### Data loading for EfficientDet\n",
    "One thing about the EfficientDet implementation was that it provided an in-built data loader that was rather stubborn to work with and did provide only narrow augmentation options. Therefore, experiments were conducted to compare the performance of the in-built data loader with a custom data loader implemented (that is also used in Faster R-CNN adapter). This was to see if the custom data loader could provide better performance or flexibility compared to the in-built one.\n",
    "\n",
    "| Loader Type            | mAP@50–95  | epochs |\n",
    "|------------------------|-----------:|-------:|\n",
    "|  in-built loader       | 0.0327     |  10    |\n",
    "|  custom loader         | 0.0329     |  10    |\n",
    "|  in-built loader       | 0.0392     |  20    |\n",
    "|  custom loader         | 0.0402     |  20    |\n",
    "|  in-built loader       | 0.0463     |  30    |\n",
    "|  custom loader         | 0.0343     |  30    |\n",
    "\n",
    "\n",
    "Notes:\n",
    "- custom loader has more and stronger augmentations compared to the in-built one\n",
    "- in this case, the in-built loader performed better than the custom one, which was unexpected:\n",
    "    - possible reasons could be that the custom augmentations were too strong and made learning harder\n",
    "    - perhaps custom loader would benefit from hyperparameter tuning to better suit the augmentations used\n",
    "    - more experiments would be needed to draw definitive conclusions\n",
    "    \n",
    "    \n",
    "    \n",
    "### Dataset Analysis\n",
    "\n",
    "#### Corrupted Images and EXIF Orientation Issue\n",
    "In those experiments we have made analysis on the Carbucks dataset. This was because after we started working with it, we noticed the drop in the mAP@50-95 metric when compared to the CardDD dataset. Therefore, we wanted to understand the class distribution and other characteristics of the Carbucks dataset to identify potential issues that could be affecting model performance.\n",
    "\n",
    "Firstly, we have discovered that Carbucks images were often either partially corrupt or had EXIF orientation tags that impacted the correct display of images. EXIF orientation tags make the images appear rotated correctly in some viewers, but when read with libraries that do not respect these tags (like OpenCV), the images appeared rotated incorrectly with respect to annotations. This was leading to misalignment between images and their annotations (raw unrotated images did not match the rotated annotations). We have fixed this by reading each image, correcting its orientation based on the EXIF tag, and re-saving.\n",
    "\n",
    "#### Distribution Analysis\n",
    "We have also analyzed the class distribution of the Carbucks dataset. The carbucks data showed to be more imbalanced compared to the CardDD dataset, with certain classes being underrepresented. This imbalance could potentially lead to biased model performance, where the model performs well on majority classes but poorly on minority classes.\n",
    "\n",
    "The results summed all the singular annotations across all the datasets' splits and included no-annotation images as a separate category to give a full picture of the dataset composition. \n",
    "\n",
    "| Dataset Version | Scratch |  Dent  | Crack  | No Annotations | Total Annotations* | Total Images | Avg Annotations per Image |\n",
    "|-----------------|---------|--------|--------|----------------|--------------------|--------------|--------------------------:|\n",
    "| Car-DD          | 45.46%  | 31.90% | 11.45% | 11.19%         | 7232               | 3626         | 2.00                      |\n",
    "| Carbucks Raw    | 60.67%  | 17.79% | 8.92%  | 12.62%         | 4643               | 2915         | 1.59                      |\n",
    "\n",
    "\\* total annotations include \"empty\"/\"no annotation\" images\n",
    "\n",
    "\n",
    "#### Datasets Combinations\n",
    "We also tried to understand what impacts the performance dataset-wise. We have experimented with different combinations of datasets (Carbucks raw, Carbucks cleaned, Carbucks balanced, CardDD) to see how they affect the model's performance. \n",
    "\n",
    "The idea was to see if the dataset is the limiting factor in achieving better performance, and if combining datasets or cleaning/balancing them could lead to improvements.\n",
    "\n",
    "All those experiments have been conducted without cross-validation due to time constraints, but they provide a good starting point for understanding the impact of dataset characteristics on model performance. \n",
    "\n",
    "The same model configuration was trained and evaluated against different dataset configurations to isolate the effect of the dataset itself.\n",
    "\n",
    "| Training Dataset                            | Evaluation Dataset     | mAP@50–95  |\n",
    "|---------------------------------------------|------------------------|-----------:|\n",
    "|  car_dd_train                               |  carbucks_raw_val      | 0.0266     |\n",
    "|  carbucks_raw_train                         |  carbucks_raw_val      | `0.0380`   |\n",
    "|  carbucks_cleaned_train                     |  carbucks_raw_val      | 0.0378     |\n",
    "|  carbucks_balanced_train                    |  carbucks_raw_val      | 0.0244     |\n",
    "|  car_dd_train `+` carbucks_raw_train        |  carbucks_raw_val      | 0.0417     |\n",
    "|  car_dd_train `+` carbucks_cleaned_train    |  carbucks_raw_val      | `0.0544`   |\n",
    "|  car_dd_train `+` carbucks_balanced_train   |  carbucks_raw_val      | `0.0409`   |\n",
    "\n",
    "\n",
    "Notes: \n",
    "- car_dd is a public dataset with images of cars' damages.\n",
    "- carbucks_* is a private dataset with images of cars' damages:\n",
    "    - suffix \"raw\" refers to dataset with images that had no annotation labels\n",
    "    - suffix \"cleaned\" refers to dataset without images that had no annotation labels\n",
    "    - suffix \"balanced\" refers to \"cleaned\" carbucks dataset that was manually balanced to have more equal distribution of classes\n",
    "- the `worst` result of combined datasets is `better` than the `best` results of singular dataset by `7.63%` (relative)\n",
    "- the `best` result of combined datasets is `better` than the `best` results of singular dataset by `43.16%` (relative)\n",
    "\n",
    "\n",
    "\n",
    "## Discussions and results\n",
    "- Carbucks dataset had issues with corrupted images and EXIF orientation tags, which were fixed.\n",
    "- Carbucks dataset was more imbalanced compared to CardDD dataset, which could lead to biased model performance.\n",
    "- Custom data loader for EfficientDet did not outperform the in-built loader, possibly due to strong augmentations or lack of hyperparameter tuning.\n",
    "- All experiments yielded poor results (mAP@50–95 < 0.2 - arbitrary threshold found during exploration and visualization).\n",
    "- They have been run against the same evaluation dataset, so there can be some bias in the results (no cross-validation due to time constraints).\n",
    "- Combining datasets (Carbucks + CardDD) generally led to better performance compared to using single datasets. This can indicate that the model benefits from the increased diversity and quantity of training data. This means that Carbucks dataset can be difficult to learn from and we may need more data or better quality data to achieve good performance.\n",
    "- Cleaning and balancing the Carbucks dataset led to slight improvements in performance, indicating that addressing class imbalance can be beneficial.\n",
    "\n",
    "## Conclusions\n",
    "It is hard to pinpoint the exact cause of the poor performance, but it is likely a combination of factors including dataset quality, complexity, and model capacity. \n",
    "\n",
    "The most prominent factor seems to be the dataset quality and quantity, as combining datasets yielded better results that stood out significantly.\n",
    "\n",
    "It would be recommended to label/annotate more data and compare the results again.\n",
    "\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
